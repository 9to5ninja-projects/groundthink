{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f94657",
   "metadata": {},
   "source": [
    "# Task 0.0.3: GPT-1 Baseline (Colab-Ready)\n",
    "\n",
    "**Purpose**: Establish baseline GPT-1 performance for fair comparison with RWKV-6 and Mamba-2  \n",
    "**Phase**: V0.5 Phase 0 - Base Model Characterization  \n",
    "**Status**: ‚úÖ COMPLETE (2026-01-12)  \n",
    "**Documentation**: See [BASE_MODEL_CHARACTERIZATION.md](../BASE_MODEL_CHARACTERIZATION.md), [HANDOFF.md](../HANDOFF.md)\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "| Metric | Value | Note |\n",
    "|--------|-------|------|\n",
    "| **Characterization** | **AMPLIFIER (782x)** | Extreme amplification |\n",
    "| Variance | 0.02 ‚Üí 16.7 | 782x total over 8 layers |\n",
    "| Final loss | 6.77 | 50 steps with BlinkDL init |\n",
    "| Max prob | 0.058 | Healthy (no saturation) |\n",
    "| Entropy | 70.0% | Of random (9.68) |\n",
    "| Logits range | [-4.5, +4.5] | Well-bounded |\n",
    "\n",
    "**Key Insight:** GPT-1 is an extreme amplifier (782x) compared to RWKV-6 (5.5x) and Mamba-2 (2.0x). BlinkDL init keeps it stable by starting from tiny variance (0.02).\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- **GPT-1 style**: Decoder-only transformer with causal attention\n",
    "- **Scale**: 4.37M params (8 layers √ó 144 hidden) to match RWKV-6 and Mamba-2\n",
    "- **Activation**: GELU (same as other baselines)\n",
    "- **Init**: BlinkDL-style (architecture-agnostic, proven on all three models)\n",
    "\n",
    "## Execution\n",
    "\n",
    "1. **VS Code + Colab** (RECOMMENDED): Select Kernel ‚Üí Connect to Google Colab\n",
    "2. **Local WSL**: Should work (pure PyTorch, no CUDA dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283529f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Running on Google Colab\n",
      "‚úì Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Colab setup\n",
    "import os\n",
    "\n",
    "try:\n",
    "    IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úì Running on Google Colab\")\n",
    "    if not os.path.exists('groundthink'):\n",
    "        !git clone https://github.com/9to5ninja-projects/groundthink.git\n",
    "    else:\n",
    "        !cd groundthink && git pull --quiet\n",
    "    os.chdir('groundthink')\n",
    "    !pip install -q tokenizers\n",
    "    print(\"‚úì Dependencies installed\")\n",
    "else:\n",
    "    print(\"Running locally (WSL/Linux)\")\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1800075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before imports] Memory: 147 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Memory monitoring\n",
    "import resource\n",
    "import gc\n",
    "\n",
    "def mem_mb():\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "def mem_check(label):\n",
    "    gc.collect()\n",
    "    print(f\"[{label}] Memory: {mem_mb():.0f} MB\")\n",
    "\n",
    "mem_check(\"Before imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a008d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After torch import] Memory: 280 MB\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "mem_check(\"After torch import\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a37cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: 16000 vocab\n",
      "[After tokenizer] Memory: 297 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load tokenizer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.exists('groundthink') and os.getcwd().endswith('content'):\n",
    "    os.chdir('groundthink')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from data.tokenizer import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer('data/tokenizer_wikitext.json')\n",
    "print(f\"Loaded tokenizer: {tokenizer.vocab_size} vocab\")\n",
    "mem_check(\"After tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57df4817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming from HuggingFace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20,000 items, 5.9MB\n",
      "  30,000 items, 8.8MB\n",
      "  60,000 items, 17.7MB\n",
      "  70,000 items, 20.6MB\n",
      "  80,000 items, 23.5MB\n",
      "  90,000 items, 26.5MB\n",
      "  100,000 items, 29.4MB\n",
      "  120,000 items, 35.4MB\n",
      "  130,000 items, 38.4MB\n",
      "  140,000 items, 41.3MB\n",
      "  160,000 items, 47.2MB\n",
      "  170,000 items, 50.1MB\n",
      "‚úì Tokenized: 11,926,606 tokens\n",
      "[After tokenization] Memory: 1219 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load data (same as Task 0.0.1/0.0.2)\n",
    "TOKEN_FILE = 'data/wikitext103_tokens.bin'\n",
    "\n",
    "if os.path.exists(TOKEN_FILE):\n",
    "    import numpy as np\n",
    "    tokens = torch.from_numpy(np.fromfile(TOKEN_FILE, dtype=np.int32)).long()\n",
    "    print(f\"‚úì Loaded {len(tokens):,} tokens from cache\")\n",
    "else:\n",
    "    print(\"Streaming from HuggingFace...\")\n",
    "    if IN_COLAB:\n",
    "        !pip install -q datasets\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\", streaming=True)\n",
    "    all_tokens = []\n",
    "    char_count = 0\n",
    "    max_chars = 50 * 1024 * 1024\n",
    "    \n",
    "    for i, item in enumerate(ds):\n",
    "        text = item['text']\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        char_count += len(text)\n",
    "        all_tokens.extend(tokenizer.encode(text))\n",
    "        if i % 10000 == 0 and i > 0:\n",
    "            print(f\"  {i:,} items, {char_count/1e6:.1f}MB\")\n",
    "        if char_count >= max_chars:\n",
    "            break\n",
    "    \n",
    "    tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "    print(f\"‚úì Tokenized: {len(tokens):,} tokens\")\n",
    "    del all_tokens\n",
    "    gc.collect()\n",
    "\n",
    "mem_check(\"After tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67acfee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 186,353 sequences of length 64\n",
      "[After dataset setup] Memory: 1219 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Dataset setup\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 64\n",
    "\n",
    "n_tokens = (len(tokens) // (BATCH_SIZE * SEQ_LEN)) * (BATCH_SIZE * SEQ_LEN)\n",
    "tokens = tokens[:n_tokens]\n",
    "print(f\"Dataset: {n_tokens // SEQ_LEN:,} sequences of length {SEQ_LEN}\")\n",
    "mem_check(\"After dataset setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6638e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CausalSelfAttention defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: GPT-1 Model Definition\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Standard causal multi-head attention (GPT-style).\"\"\"\n",
    "    def __init__(self, d_model, n_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "        self.register_buffer('mask', mask.view(1, 1, max_seq_len, max_seq_len))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scale = 1.0 / math.sqrt(self.d_head)\n",
    "        attn = (q @ k.transpose(-2, -1)) * scale\n",
    "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "print(\"‚úì CausalSelfAttention defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59224a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-1 Model: 4,370,832 parameters (4.37M)\n",
      "[After model creation] Memory: 1219 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: GPT-1 Block and Model\n",
    "\n",
    "class GPT1Block(nn.Module):\n",
    "    \"\"\"GPT-1 transformer block: attention + FFN with residuals.\"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = RMSNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads)\n",
    "        self.ln2 = RMSNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model, bias=False),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT1Model(nn.Module):\n",
    "    \"\"\"GPT-1 decoder-only transformer.\"\"\"\n",
    "    def __init__(self, vocab_size, d_model=144, n_layers=8, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(512, d_model)  # max 512 positions\n",
    "        self.blocks = nn.ModuleList([GPT1Block(d_model, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_out = RMSNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.head.weight = self.embed.weight  # Tie weights\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
    "        x = self.embed(x) + self.pos_embed(pos)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.head(self.ln_out(x))\n",
    "\n",
    "# Create model\n",
    "model = GPT1Model(tokenizer.vocab_size, d_model=144, n_layers=8, n_heads=4)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"GPT-1 Model: {params:,} parameters ({params/1e6:.2f}M)\")\n",
    "mem_check(\"After model creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53f1871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings: uniform(-1e-4, 1e-4)\n",
      "‚úì Zeroed out_proj in all 8 blocks\n",
      "‚úì BlinkDL init applied\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Apply BlinkDL initialization (architecture-agnostic)\n",
    "\n",
    "def apply_blinkdl_init(model):\n",
    "    \"\"\"Apply proven BlinkDL init pattern.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Small embedding init\n",
    "        nn.init.uniform_(model.embed.weight, -1e-4, 1e-4)\n",
    "        nn.init.uniform_(model.pos_embed.weight, -1e-4, 1e-4)\n",
    "        print(\"‚úì Embeddings: uniform(-1e-4, 1e-4)\")\n",
    "        \n",
    "        # Zero output projections in each block\n",
    "        for i, block in enumerate(model.blocks):\n",
    "            nn.init.zeros_(block.attn.out_proj.weight)\n",
    "            nn.init.zeros_(block.ffn[2].weight)  # FFN output\n",
    "        print(f\"‚úì Zeroed out_proj in all {len(model.blocks)} blocks\")\n",
    "\n",
    "apply_blinkdl_init(model)\n",
    "print(\"‚úì BlinkDL init applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e658d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10/50: loss=9.18, 0.9s\n",
      "Step 20/50: loss=8.75, 1.7s\n",
      "Step 30/50: loss=7.30, 2.5s\n",
      "Step 40/50: loss=6.72, 3.3s\n",
      "Step 50/50: loss=6.77, 4.3s\n",
      "\n",
      "‚úì Training complete: 9.68 ‚Üí 6.77\n",
      "[After training] Memory: 1219 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Training loop (50 steps)\n",
    "import time\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "NUM_STEPS = 50\n",
    "LOG_EVERY = 10\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    idx = (step * SEQ_LEN) % (len(tokens) - SEQ_LEN - 1)\n",
    "    x = tokens[idx:idx + SEQ_LEN].unsqueeze(0)\n",
    "    y = tokens[idx + 1:idx + SEQ_LEN + 1].unsqueeze(0)\n",
    "    \n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (step + 1) % LOG_EVERY == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Step {step+1}/{NUM_STEPS}: loss={losses[-1]:.2f}, {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\n‚úì Training complete: {losses[0]:.2f} ‚Üí {losses[-1]:.2f}\")\n",
    "mem_check(\"After training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4379864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Output Diagnostic ===\n",
      "Logits shape: torch.Size([32, 64, 16000])\n",
      "Logits range: [-4.46, 4.43]\n",
      "Max prob: 0.0583\n",
      "Entropy: 6.77 / 9.68 (70.0%)\n",
      "‚úì Model outputs healthy\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Diagnostic - output health check\n",
    "\n",
    "print(\"=== Model Output Diagnostic ===\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_x = tokens[:32*64].view(32, 64)\n",
    "    sample_logits = model(sample_x)\n",
    "\n",
    "print(f\"Logits shape: {sample_logits.shape}\")\n",
    "print(f\"Logits range: [{sample_logits.min().item():.2f}, {sample_logits.max().item():.2f}]\")\n",
    "\n",
    "probs = F.softmax(sample_logits, dim=-1)\n",
    "max_prob = probs.max().item()\n",
    "entropy = -(probs * torch.log(probs + 1e-10)).sum(-1).mean().item()\n",
    "random_entropy = math.log(tokenizer.vocab_size)\n",
    "\n",
    "print(f\"Max prob: {max_prob:.4f}\")\n",
    "print(f\"Entropy: {entropy:.2f} / {random_entropy:.2f} ({100*entropy/random_entropy:.1f}%)\")\n",
    "\n",
    "if max_prob > 0.99:\n",
    "    print(\"‚ö†Ô∏è Softmax saturating\")\n",
    "elif entropy < 2.0:\n",
    "    print(\"‚ö†Ô∏è Low entropy - model overconfident\")\n",
    "else:\n",
    "    print(\"‚úì Model outputs healthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba631abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Layer-wise Variance Analysis ===\n",
      "\n",
      "Layer-wise statistics:\n",
      "--------------------------------------------------\n",
      "embed       : std=0.0214\n",
      "layer_0     : std=1.0030\n",
      "layer_1     : std=3.6235\n",
      "layer_2     : std=6.7759\n",
      "layer_3     : std=9.4000\n",
      "layer_4     : std=11.5465\n",
      "layer_5     : std=13.4393\n",
      "layer_6     : std=15.1841\n",
      "layer_7     : std=16.7061\n",
      "--------------------------------------------------\n",
      "Variance evolution: 0.02 ‚Üí 16.71 (781.90x)\n",
      "\n",
      "üéØ CHARACTERIZATION: **AMPLIFIER**\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Layer-wise variance analysis\n",
    "\n",
    "print(\"=== Layer-wise Variance Analysis ===\")\n",
    "\n",
    "layer_outputs = {}\n",
    "\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "hooks = []\n",
    "for i, block in enumerate(model.blocks):\n",
    "    h = block.register_forward_hook(make_hook(f'layer_{i}'))\n",
    "    hooks.append(h)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_x = tokens[:64].unsqueeze(0)\n",
    "    pos = torch.arange(64).unsqueeze(0)\n",
    "    embed_out = model.embed(sample_x) + model.pos_embed(pos)\n",
    "    layer_outputs['embed'] = embed_out\n",
    "    _ = model(sample_x)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "print(\"\\nLayer-wise statistics:\")\n",
    "print(\"-\" * 50)\n",
    "variances = []\n",
    "for name in ['embed'] + [f'layer_{i}' for i in range(len(model.blocks))]:\n",
    "    out = layer_outputs[name]\n",
    "    var = out.std().item()\n",
    "    variances.append(var)\n",
    "    print(f\"{name:12s}: std={var:.4f}\")\n",
    "\n",
    "ratio = variances[-1] / variances[0] if variances[0] > 0 else 0\n",
    "print(\"-\" * 50)\n",
    "print(f\"Variance evolution: {variances[0]:.2f} ‚Üí {variances[-1]:.2f} ({ratio:.2f}x)\")\n",
    "\n",
    "if ratio > 1.5:\n",
    "    char = \"AMPLIFIER\"\n",
    "elif ratio < 0.5:\n",
    "    char = \"DAMPER\"\n",
    "else:\n",
    "    char = \"NEUTRAL\"\n",
    "print(f\"\\nüéØ CHARACTERIZATION: **{char}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ebcc209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task 0.0.3 Findings ===\n",
      "{\n",
      "  \"task\": \"0.0.3\",\n",
      "  \"model\": \"GPT-1 Baseline\",\n",
      "  \"architecture\": {\n",
      "    \"type\": \"decoder-only transformer\",\n",
      "    \"layers\": 8,\n",
      "    \"hidden\": 144,\n",
      "    \"n_heads\": 4,\n",
      "    \"params\": 4370832\n",
      "  },\n",
      "  \"init\": \"BlinkDL (uniform emb, zero out_proj)\",\n",
      "  \"characterization\": \"AMPLIFIER\",\n",
      "  \"variance_evolution\": {\n",
      "    \"input\": 0.021366169676184654,\n",
      "    \"output\": 16.70612144470215,\n",
      "    \"ratio\": 781.8959456885373\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"steps\": 50,\n",
      "    \"initial_loss\": 9.680351257324219,\n",
      "    \"final_loss\": 6.774981498718262\n",
      "  },\n",
      "  \"softmax\": {\n",
      "    \"max_prob\": 0.05834708362817764,\n",
      "    \"entropy\": 6.774600505828857,\n",
      "    \"random_entropy\": 9.680344001221918\n",
      "  },\n",
      "  \"timestamp\": \"2026-01-12T20:41:32.027629\"\n",
      "}\n",
      "\n",
      "‚úì Saved to logs/gpt1_baseline_findings.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save findings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "findings = {\n",
    "    'task': '0.0.3',\n",
    "    'model': 'GPT-1 Baseline',\n",
    "    'architecture': {\n",
    "        'type': 'decoder-only transformer',\n",
    "        'layers': 8,\n",
    "        'hidden': 144,\n",
    "        'n_heads': 4,\n",
    "        'params': params,\n",
    "    },\n",
    "    'init': 'BlinkDL (uniform emb, zero out_proj)',\n",
    "    'characterization': char,\n",
    "    'variance_evolution': {\n",
    "        'input': variances[0],\n",
    "        'output': variances[-1],\n",
    "        'ratio': ratio,\n",
    "    },\n",
    "    'training': {\n",
    "        'steps': NUM_STEPS,\n",
    "        'initial_loss': losses[0],\n",
    "        'final_loss': losses[-1],\n",
    "    },\n",
    "    'softmax': {\n",
    "        'max_prob': max_prob,\n",
    "        'entropy': entropy,\n",
    "        'random_entropy': random_entropy,\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "with open('logs/gpt1_baseline_findings.json', 'w') as f:\n",
    "    json.dump(findings, f, indent=2)\n",
    "\n",
    "print(\"=== Task 0.0.3 Findings ===\")\n",
    "print(json.dumps(findings, indent=2))\n",
    "print(f\"\\n‚úì Saved to logs/gpt1_baseline_findings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58209acf",
   "metadata": {},
   "source": [
    "## Summary (Task 0.0.3 Complete - 2026-01-12)\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Metric | GPT-1 | RWKV-6 | Mamba-2 |\n",
    "|--------|-------|--------|---------|\n",
    "| Params | 4.37M | 4.3M | 4.4M |\n",
    "| **Characterization** | **AMPLIFIER (782x)** | AMPLIFIER (5.5x) | AMPLIFIER (2.0x) |\n",
    "| Variance ratio | 0.02 ‚Üí 16.7 | 1.0 ‚Üí 5.6 | 1.0 ‚Üí 2.0 |\n",
    "| Final loss (50 steps) | 6.77 | 7.9 | 6.75 |\n",
    "| Max prob | 0.058 | 0.08 | 0.05 |\n",
    "| Entropy (% of random) | 70.0% | 95% | 72.1% |\n",
    "| Logits range | [-4.5, +4.5] | [-10, +10] | [-66, +155]* |\n",
    "\n",
    "*Mamba-2 baseline (before BlinkDL init)\n",
    "\n",
    "### Key Finding: GPT-1 is an Extreme Amplifier\n",
    "\n",
    "GPT-1 amplifies variance **782x** through 8 layers vs:\n",
    "- RWKV-6: 5.5x\n",
    "- Mamba-2: 2.0x\n",
    "\n",
    "**Why doesn't it saturate?** The BlinkDL init (tiny embeddings + zero out_proj) keeps starting variance at 0.02 instead of 1.0. The amplification ratio is huge, but absolute values stay reasonable (logits [-4.5, +4.5]).\n",
    "\n",
    "### Implications for Fusion\n",
    "\n",
    "1. **All three are amplifiers** at the full-model level\n",
    "2. **GPT-1 amplifies most aggressively** (782x vs 5.5x vs 2.0x)\n",
    "3. **BlinkDL init is critical** - works across all architectures\n",
    "4. **SSM layers show different behavior:**\n",
    "   - RWKV-6 raw layer: amplifier\n",
    "   - Mamba-2 raw layer: **damper** (0.005x)\n",
    "   - GPT-1 attention: amplifier\n",
    "\n",
    "### Next Steps\n",
    "- ‚úÖ Task 0.0.3: COMPLETE\n",
    "- ‚¨ú Task 0.0.4: Comparative analysis document\n",
    "- ‚¨ú Phase 1: Implement GRU Arbiter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
