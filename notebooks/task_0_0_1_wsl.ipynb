{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8ae7e5",
   "metadata": {},
   "source": [
    "# Task 0.0.1: Modified RWKV-6 Baseline (Colab-Ready)\n",
    "\n",
    "**Purpose**: Establish baseline characteristics for RWKV-6 architecture\n",
    "\n",
    "**Execution Options**:\n",
    "1. **VS Code + Colab** (RECOMMENDED): Select Kernel → Connect to Google Colab → Free T4 GPU + 15GB RAM\n",
    "2. **Local WSL**: Not viable (full 540MB crashes at 5.3GB RAM usage)\n",
    "\n",
    "**⚠️ DEVIATIONS FROM ORIGINAL PLAN:**\n",
    "\n",
    "| Item | Original | Actual | Why |\n",
    "|------|----------|--------|-----|\n",
    "| Dataset | 540MB full | 50MB subset | 5.3GB RAM to load; Colab tokenizer too slow on full |\n",
    "| Environment | Local WSL | Colab | WSL has ~2.5GB limit |\n",
    "| Tokenization | Single-pass | 10MB chunks | Prevents OOM |\n",
    "| mamba-ssm | Installed | Skipped | Build fails on Colab; not needed for RWKV6 |\n",
    "\n",
    "**Validation**: 50MB (~5M tokens) is sufficient for baseline characterization per V4_BUILD_LOG.md compute constraints.\n",
    "\n",
    "**Strategy**: Cell-by-cell execution with memory monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66478c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Running on Google Colab\n",
      "✓ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Colab setup (run first if using Colab kernel)\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"✓ Running on Google Colab\")\n",
    "    # Clone repo if not present\n",
    "    if not os.path.exists('groundthink'):\n",
    "        !git clone https://github.com/9to5ninja-projects/groundthink.git\n",
    "    os.chdir('groundthink')\n",
    "    # Only install tokenizers (mamba-ssm not needed for RWKV6)\n",
    "    !pip install -q tokenizers\n",
    "    print(\"✓ Dependencies installed\")\n",
    "else:\n",
    "    print(\"Running locally (WSL/Linux)\")\n",
    "    os.chdir('..')  # notebooks → project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf91b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before imports] Memory: 146 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Memory monitoring utility\n",
    "import resource\n",
    "import gc\n",
    "\n",
    "def mem_mb():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "def mem_check(label):\n",
    "    gc.collect()\n",
    "    print(f\"[{label}] Memory: {mem_mb():.0f} MB\")\n",
    "\n",
    "mem_check(\"Before imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a57edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After torch import] Memory: 269 MB\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import PyTorch only\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "mem_check(\"After torch import\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71e847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: 16000 vocab\n",
      "Working directory: /content/groundthink\n",
      "[After tokenizer load] Memory: 287 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load pre-trained tokenizer (skip BPE training)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure we're in the right directory and path is set\n",
    "if os.path.exists('groundthink') and os.getcwd().endswith('content'):\n",
    "    os.chdir('groundthink')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from data.tokenizer import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer('data/tokenizer_wikitext.json')\n",
    "print(f\"Loaded tokenizer: {tokenizer.vocab_size} vocab\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "mem_check(\"After tokenizer load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310b606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset subset: 52,428,800 chars (52 MB)\n",
      "[After text load] Memory: 4924 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load data (download on Colab via HuggingFace, use local on WSL)\n",
    "import os\n",
    "\n",
    "# Re-detect environment (in case kernel restarted)\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "DATA_PATH = 'data/wikitext103/train.txt'\n",
    "\n",
    "# Download WikiText-103 if not present (Colab) - using HuggingFace\n",
    "if IN_COLAB and not os.path.exists(DATA_PATH):\n",
    "    print(\"Downloading WikiText-103 via HuggingFace...\")\n",
    "    !pip install -q datasets\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "    os.makedirs('data/wikitext103', exist_ok=True)\n",
    "    with open(DATA_PATH, 'w', encoding='utf-8') as f:\n",
    "        for item in ds:\n",
    "            f.write(item['text'] + '\\n')\n",
    "    del ds\n",
    "    gc.collect()\n",
    "    print(\"✓ Downloaded WikiText-103\")\n",
    "\n",
    "# Verify file exists\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data not found at {DATA_PATH}. Check download logs above.\")\n",
    "\n",
    "# Load subset for baseline (50MB is plenty for characterization)\n",
    "SUBSET_SIZE = 50 * 1024 * 1024  # 50MB - fast tokenization, meaningful training\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read(SUBSET_SIZE)\n",
    "print(f\"Loaded dataset subset: {len(text):,} chars ({len(text)/1e6:.0f} MB)\")\n",
    "\n",
    "mem_check(\"After text load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce5e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 52,428,800 chars in 6 chunks...\n",
      "  Chunk 0: 2,416,077 tokens so far\n",
      "Tokenized: 12,063,505 tokens\n",
      "[After tokenization] Memory: 4924 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize in chunks (full text crashes even Colab)\n",
    "CHUNK_SIZE = 10 * 1024 * 1024  # 10MB chunks\n",
    "\n",
    "print(f\"Tokenizing {len(text):,} chars in {len(text)//CHUNK_SIZE + 1} chunks...\")\n",
    "all_tokens = []\n",
    "\n",
    "for i in range(0, len(text), CHUNK_SIZE):\n",
    "    chunk = text[i:i+CHUNK_SIZE]\n",
    "    chunk_tokens = tokenizer.encode(chunk)\n",
    "    all_tokens.extend(chunk_tokens)\n",
    "    if (i // CHUNK_SIZE) % 10 == 0:\n",
    "        print(f\"  Chunk {i//CHUNK_SIZE}: {len(all_tokens):,} tokens so far\")\n",
    "    gc.collect()\n",
    "\n",
    "tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "print(f\"Tokenized: {len(tokens):,} tokens\")\n",
    "\n",
    "# Free memory\n",
    "del text, all_tokens\n",
    "gc.collect()\n",
    "mem_check(\"After tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b5b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 188,492 batches of size 1x64\n",
      "[After dataset setup] Memory: 4924 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create simple dataset\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 64\n",
    "\n",
    "# Simple batching (not full StatefulDataset to save memory)\n",
    "n_tokens = (len(tokens) // (BATCH_SIZE * SEQ_LEN)) * (BATCH_SIZE * SEQ_LEN)\n",
    "tokens = tokens[:n_tokens]\n",
    "num_batches = n_tokens // (BATCH_SIZE * SEQ_LEN)\n",
    "\n",
    "print(f\"Dataset: {num_batches:,} batches of size {BATCH_SIZE}x{SEQ_LEN}\")\n",
    "mem_check(\"After dataset setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca9a95b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mamba_ssm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-912055744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell 7: Import RWKV6 (ops package, not mamba)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRWKV6Attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRWKV6_CUDA_AVAILABLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"RWKV6 CUDA available: {RWKV6_CUDA_AVAILABLE}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmem_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After RWKV6 import\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/groundthink/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcuda_backends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRWKV6Attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMamba2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRWKV6_CUDA_AVAILABLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrwkv6_cuda_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_wkv6_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/groundthink/ops/cuda_backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Import Mamba-2 from mamba-ssm (already has CUDA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMamba2_SSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mamba_ssm'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Import RWKV6 (ops package, not mamba)\n",
    "from ops import RWKV6Attention, RWKV6_CUDA_AVAILABLE\n",
    "\n",
    "print(f\"RWKV6 CUDA available: {RWKV6_CUDA_AVAILABLE}\")\n",
    "mem_check(\"After RWKV6 import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create model\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class RWKV6Block(nn.Module):\n",
    "    def __init__(self, hidden, num_heads=4, layer_idx=0):\n",
    "        super().__init__()\n",
    "        self.ln1 = RMSNorm(hidden)\n",
    "        self.rwkv = RWKV6Attention(hidden, num_heads=num_heads, layer_idx=layer_idx)\n",
    "        self.ln2 = RMSNorm(hidden)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden * 4, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden * 4, hidden, bias=False),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.rwkv(self.ln1(x))[0]\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class RWKV6Model(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden=144, layers=8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden)\n",
    "        self.blocks = nn.ModuleList([RWKV6Block(hidden, layer_idx=i) for i in range(layers)])\n",
    "        self.ln_out = RMSNorm(hidden)\n",
    "        self.head = nn.Linear(hidden, vocab_size, bias=False)\n",
    "        self.head.weight = self.embed.weight  # Tie weights\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.head(self.ln_out(x))\n",
    "\n",
    "model = RWKV6Model(tokenizer.vocab_size, hidden=144, layers=8)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {params:,} parameters ({params/1e6:.2f}M)\")\n",
    "mem_check(\"After model creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815500f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mem_check(\"After optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Single training step test\n",
    "model.train()\n",
    "\n",
    "# Get one batch\n",
    "start = 0\n",
    "x = tokens[start:start + SEQ_LEN].unsqueeze(0)\n",
    "y = tokens[start + 1:start + SEQ_LEN + 1].unsqueeze(0)\n",
    "\n",
    "# Forward\n",
    "logits = model(x)\n",
    "loss = criterion(logits.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "\n",
    "# Backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Step 0: loss = {loss.item():.4f}\")\n",
    "mem_check(\"After 1 training step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aeea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training loop (50 steps)\n",
    "import time\n",
    "\n",
    "NUM_STEPS = 50\n",
    "LOG_EVERY = 10\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    idx = (step * SEQ_LEN) % (len(tokens) - SEQ_LEN - 1)\n",
    "    x = tokens[idx:idx + SEQ_LEN].unsqueeze(0)\n",
    "    y = tokens[idx + 1:idx + SEQ_LEN + 1].unsqueeze(0)\n",
    "    \n",
    "    logits = model(x)\n",
    "    loss = criterion(logits.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (step + 1) % LOG_EVERY == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_loss = sum(losses[-LOG_EVERY:]) / LOG_EVERY\n",
    "        print(f\"Step {step+1}/{NUM_STEPS}: loss={avg_loss:.4f}, {elapsed:.1f}s, mem={mem_mb():.0f}MB\")\n",
    "\n",
    "print(f\"\\n✓ Training complete: {NUM_STEPS} steps\")\n",
    "mem_check(\"Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Plot loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Task 0.0.1: RWKV-6 Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
