{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8ae7e5",
   "metadata": {},
   "source": [
    "# Task 0.0.1: Modified RWKV-6 Baseline (WSL-Safe)\n",
    "\n",
    "**Purpose**: Establish baseline characteristics for RWKV-6 architecture\n",
    "\n",
    "**WSL Constraints**:\n",
    "- ~2.5 GB total memory budget\n",
    "- PyTorch import: ~350 MB\n",
    "- Must avoid: BPE training on full corpus, loading 540MB text at once\n",
    "\n",
    "**Strategy**: Cell-by-cell execution with memory monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf91b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Memory monitoring utility\n",
    "import resource\n",
    "import gc\n",
    "\n",
    "def mem_mb():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "def mem_check(label):\n",
    "    gc.collect()\n",
    "    print(f\"[{label}] Memory: {mem_mb():.0f} MB\")\n",
    "\n",
    "mem_check(\"Before imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a57edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import PyTorch only\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "mem_check(\"After torch import\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load pre-trained tokenizer (skip BPE training)\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from data.tokenizer import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer('../data/tokenizer_wikitext.json')\n",
    "print(f\"Loaded tokenizer: {tokenizer.vocab_size} vocab\")\n",
    "mem_check(\"After tokenizer load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load SMALL subset of data (10MB instead of 540MB)\n",
    "# This avoids the memory crash during tokenization\n",
    "\n",
    "DATA_PATH = '../data/wikitext103/train.txt'\n",
    "SUBSET_SIZE = 10 * 1024 * 1024  # 10 MB\n",
    "\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read(SUBSET_SIZE)\n",
    "\n",
    "print(f\"Loaded {len(text):,} characters ({len(text)/1e6:.1f} MB)\")\n",
    "mem_check(\"After text load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Tokenize subset\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f\"Tokenized: {len(tokens):,} tokens\")\n",
    "\n",
    "# Free the text\n",
    "del text\n",
    "gc.collect()\n",
    "mem_check(\"After tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create simple dataset\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 64\n",
    "\n",
    "# Simple batching (not full StatefulDataset to save memory)\n",
    "n_tokens = (len(tokens) // (BATCH_SIZE * SEQ_LEN)) * (BATCH_SIZE * SEQ_LEN)\n",
    "tokens = tokens[:n_tokens]\n",
    "num_batches = n_tokens // (BATCH_SIZE * SEQ_LEN)\n",
    "\n",
    "print(f\"Dataset: {num_batches:,} batches of size {BATCH_SIZE}x{SEQ_LEN}\")\n",
    "mem_check(\"After dataset setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Import RWKV6 (ops package, not mamba)\n",
    "from ops import RWKV6Attention, RWKV6_CUDA_AVAILABLE\n",
    "\n",
    "print(f\"RWKV6 CUDA available: {RWKV6_CUDA_AVAILABLE}\")\n",
    "mem_check(\"After RWKV6 import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create model\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class RWKV6Block(nn.Module):\n",
    "    def __init__(self, hidden, num_heads=4, layer_idx=0):\n",
    "        super().__init__()\n",
    "        self.ln1 = RMSNorm(hidden)\n",
    "        self.rwkv = RWKV6Attention(hidden, num_heads=num_heads, layer_idx=layer_idx)\n",
    "        self.ln2 = RMSNorm(hidden)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden * 4, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden * 4, hidden, bias=False),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.rwkv(self.ln1(x))[0]\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class RWKV6Model(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden=144, layers=8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden)\n",
    "        self.blocks = nn.ModuleList([RWKV6Block(hidden, layer_idx=i) for i in range(layers)])\n",
    "        self.ln_out = RMSNorm(hidden)\n",
    "        self.head = nn.Linear(hidden, vocab_size, bias=False)\n",
    "        self.head.weight = self.embed.weight  # Tie weights\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.head(self.ln_out(x))\n",
    "\n",
    "model = RWKV6Model(tokenizer.vocab_size, hidden=144, layers=8)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {params:,} parameters ({params/1e6:.2f}M)\")\n",
    "mem_check(\"After model creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815500f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mem_check(\"After optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Single training step test\n",
    "model.train()\n",
    "\n",
    "# Get one batch\n",
    "start = 0\n",
    "x = tokens[start:start + SEQ_LEN].unsqueeze(0)\n",
    "y = tokens[start + 1:start + SEQ_LEN + 1].unsqueeze(0)\n",
    "\n",
    "# Forward\n",
    "logits = model(x)\n",
    "loss = criterion(logits.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "\n",
    "# Backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Step 0: loss = {loss.item():.4f}\")\n",
    "mem_check(\"After 1 training step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aeea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training loop (50 steps)\n",
    "import time\n",
    "\n",
    "NUM_STEPS = 50\n",
    "LOG_EVERY = 10\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    idx = (step * SEQ_LEN) % (len(tokens) - SEQ_LEN - 1)\n",
    "    x = tokens[idx:idx + SEQ_LEN].unsqueeze(0)\n",
    "    y = tokens[idx + 1:idx + SEQ_LEN + 1].unsqueeze(0)\n",
    "    \n",
    "    logits = model(x)\n",
    "    loss = criterion(logits.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (step + 1) % LOG_EVERY == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_loss = sum(losses[-LOG_EVERY:]) / LOG_EVERY\n",
    "        print(f\"Step {step+1}/{NUM_STEPS}: loss={avg_loss:.4f}, {elapsed:.1f}s, mem={mem_mb():.0f}MB\")\n",
    "\n",
    "print(f\"\\nâœ“ Training complete: {NUM_STEPS} steps\")\n",
    "mem_check(\"Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Plot loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Task 0.0.1: RWKV-6 Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
