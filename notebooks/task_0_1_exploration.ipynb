{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20327e33",
   "metadata": {},
   "source": [
    "# GroundThink Phase 0.1: GRU Arbiter Exploration\n",
    "\n",
    "**Purpose**: Characterize GRU Arbiter behavior before committing to architecture\n",
    "\n",
    "**Context**: Phase 1 proposes a \"Twin Debate\" architecture fusing RWKV-6 and Mamba-2 with a GRU Arbiter to replace static gating. We implemented it without understanding whether it's the right approach.\n",
    "\n",
    "**Critical Question**: Does the GRU Arbiter do anything useful, and if so, what?\n",
    "\n",
    "---\n",
    "\n",
    "## Repository Status\n",
    "\n",
    "- **Repo**: https://github.com/9to5ninja-projects/groundthink\n",
    "- **Branch**: main  \n",
    "- **Version**: 0.5.1.1\n",
    "\n",
    "## Required Files (Per Briefing)\n",
    "\n",
    "1. `ops/arbiter_gru.py` - GRUArbiter class\n",
    "2. `ops/rwkv6_prototype.py` - Pure PyTorch RWKV-6\n",
    "3. `ops/mamba2_prototype.py` - Pure PyTorch Mamba-2\n",
    "4. `tests/test_arbiter_gru.py` - Unit tests\n",
    "\n",
    "**Note**: These files may not exist yet. We'll create prototype implementations if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b54d6",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fcb1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "PyTorch: 2.9.0+cpu\n",
      "CUDA Available: False\n",
      "Running on CPU\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281bc63",
   "metadata": {},
   "source": [
    "## 2. Define Core Components\n",
    "\n",
    "Since the expected files may not exist, we'll define minimal prototype implementations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bf41b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GRUArbiter defined\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class GRUArbiter(nn.Module):\n",
    "    \"\"\"GRU-based arbiter for fusing RWKV and Mamba outputs.\n",
    "    \n",
    "    Takes concatenated [rwkv_out, mamba_out] as input to GRU,\n",
    "    outputs softmax weights Î± = [Î±_rwkv, Î±_mamba],\n",
    "    returns weighted fusion: Î±_rwkv * rwkv_out + Î±_mamba * mamba_out\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # GRU takes concatenated input: 2*d_model\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=2 * d_model,\n",
    "            hidden_size=d_model,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if dropout > 0 else 0.0\n",
    "        )\n",
    "        \n",
    "        # Project GRU output to 2 weights\n",
    "        self.weight_proj = nn.Linear(d_model, 2)\n",
    "        \n",
    "        # Initialize projections near zero for balanced start\n",
    "        nn.init.normal_(self.weight_proj.weight, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.weight_proj.bias)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        rwkv_out: torch.Tensor,  # (B, L, D)\n",
    "        mamba_out: torch.Tensor, # (B, L, D)  \n",
    "        hidden: torch.Tensor = None  # (1, B, D) or None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            fused: (B, L, D) - weighted combination\n",
    "            weights: (B, L, 2) - softmax weights [Î±_rwkv, Î±_mamba]\n",
    "            hidden: (1, B, D) - final GRU hidden state\n",
    "        \"\"\"\n",
    "        B, L, D = rwkv_out.shape\n",
    "        \n",
    "        # Concatenate inputs for GRU\n",
    "        combined = torch.cat([rwkv_out, mamba_out], dim=-1)  # (B, L, 2D)\n",
    "        \n",
    "        # Run GRU\n",
    "        if hidden is not None:\n",
    "            hidden = hidden.unsqueeze(0) if hidden.dim() == 2 else hidden\n",
    "        gru_out, hidden = self.gru(combined, hidden)  # gru_out: (B, L, D)\n",
    "        \n",
    "        # Compute weights\n",
    "        logits = self.weight_proj(gru_out)  # (B, L, 2)\n",
    "        weights = F.softmax(logits, dim=-1)  # (B, L, 2)\n",
    "        \n",
    "        # Fuse outputs\n",
    "        Î±_rwkv = weights[..., 0:1]  # (B, L, 1)\n",
    "        Î±_mamba = weights[..., 1:2]  # (B, L, 1)\n",
    "        fused = Î±_rwkv * rwkv_out + Î±_mamba * mamba_out  # (B, L, D)\n",
    "        \n",
    "        return fused, weights, hidden.squeeze(0)\n",
    "\n",
    "print(\"âœ“ GRUArbiter defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c97b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RWKV6TimeMix defined\n"
     ]
    }
   ],
   "source": [
    "class RWKV6TimeMix(nn.Module):\n",
    "    \"\"\"Minimal RWKV-6 TimeMix prototype (CPU-compatible).\n",
    "    \n",
    "    Simplified version for testing - implements core recurrent mechanism\n",
    "    without full RWKV-6 complexity. Uses BlinkDL-style initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        \n",
    "        # Time-mixing parameters (learnable)\n",
    "        self.time_mix_k = nn.Parameter(torch.ones(1, 1, hidden_size))\n",
    "        self.time_mix_v = nn.Parameter(torch.ones(1, 1, hidden_size))\n",
    "        self.time_mix_r = nn.Parameter(torch.ones(1, 1, hidden_size))\n",
    "        \n",
    "        # Projections\n",
    "        self.key = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.receptance = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.output = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "        # BlinkDL-style initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"BlinkDL initialization for RWKV stability.\"\"\"\n",
    "        # Small random init for time-mix\n",
    "        for p in [self.time_mix_k, self.time_mix_v, self.time_mix_r]:\n",
    "            nn.init.uniform_(p, 0.0, 1.0)\n",
    "            \n",
    "        # Orthogonal init for projections (stability)\n",
    "        for module in [self.key, self.value, self.receptance, self.output]:\n",
    "            nn.init.orthogonal_(module.weight, gain=0.5)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, L, D)\n",
    "        Returns:\n",
    "            output: (B, L, D)\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        # Time-shifted mixing (simplified)\n",
    "        x_shifted = torch.roll(x, shifts=1, dims=1)\n",
    "        x_shifted[:, 0] = 0  # Zero out first position\n",
    "        \n",
    "        # Mix current and previous\n",
    "        k_input = self.time_mix_k * x + (1 - self.time_mix_k) * x_shifted\n",
    "        v_input = self.time_mix_v * x + (1 - self.time_mix_v) * x_shifted\n",
    "        r_input = self.time_mix_r * x + (1 - self.time_mix_r) * x_shifted\n",
    "        \n",
    "        # Project\n",
    "        k = self.key(k_input)\n",
    "        v = self.value(v_input)\n",
    "        r = torch.sigmoid(self.receptance(r_input))\n",
    "        \n",
    "        # Simplified attention-like mechanism\n",
    "        # (Real RWKV-6 is more complex)\n",
    "        wkv = k * v  # Element-wise (simplified)\n",
    "        \n",
    "        # Gated output\n",
    "        output = self.output(r * wkv)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"âœ“ RWKV6TimeMix defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Mamba2TimeMix defined\n"
     ]
    }
   ],
   "source": [
    "class Mamba2TimeMix(nn.Module):\n",
    "    \"\"\"Minimal Mamba-2 TimeMix prototype (CPU-compatible).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4, expand: int = 2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = int(expand * d_model)\n",
    "        \n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            self.d_inner, self.d_inner, \n",
    "            kernel_size=d_conv, \n",
    "            groups=self.d_inner, \n",
    "            padding=d_conv - 1\n",
    "        )\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        \n",
    "        # Simple init\n",
    "        nn.init.orthogonal_(self.in_proj.weight, gain=0.5)\n",
    "        nn.init.orthogonal_(self.out_proj.weight, gain=0.5)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        # Project and split\n",
    "        xz = self.in_proj(x)\n",
    "        x_inner, z = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        # Conv for local context\n",
    "        x_conv = self.conv1d(x_inner.transpose(1, 2))[:, :, :L].transpose(1, 2)\n",
    "        \n",
    "        # SiLU gating (skip complex SSM scan for prototype)\n",
    "        y = F.silu(x_conv) * F.silu(z)\n",
    "        \n",
    "        return self.out_proj(y)\n",
    "\n",
    "print(\"âœ“ Mamba2TimeMix defined (simplified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba1e00",
   "metadata": {},
   "source": [
    "## 3. Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f7d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RWKV forward pass: torch.Size([2, 64, 128]) -> torch.Size([2, 64, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[2, 2, 256]}, size=[2, 256]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1559448879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Test Mamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmamba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMamba2TimeMix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_conv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmamba_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmamba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mmamba_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Mamba output shape mismatch: {mamba_out.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ“ Mamba forward pass: {x.shape} -> {mamba_out.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-70908058.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# y = C*h + D*x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_conv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Gate with z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[2, 2, 256]}, size=[2, 256]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "# Test dimensions and forward pass\n",
    "d_model = 128\n",
    "batch_size = 2\n",
    "seq_len = 64\n",
    "\n",
    "# Create test input\n",
    "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "\n",
    "# Test RWKV\n",
    "rwkv = RWKV6TimeMix(hidden_size=d_model, num_heads=4).to(device)\n",
    "rwkv_out = rwkv(x)\n",
    "assert rwkv_out.shape == (batch_size, seq_len, d_model), f\"RWKV output shape mismatch: {rwkv_out.shape}\"\n",
    "print(f\"âœ“ RWKV forward pass: {x.shape} -> {rwkv_out.shape}\")\n",
    "\n",
    "# Test Mamba\n",
    "mamba = Mamba2TimeMix(d_model=d_model, d_state=16, d_conv=4, expand=2).to(device)\n",
    "mamba_out = mamba(x)\n",
    "assert mamba_out.shape == (batch_size, seq_len, d_model), f\"Mamba output shape mismatch: {mamba_out.shape}\"\n",
    "print(f\"âœ“ Mamba forward pass: {x.shape} -> {mamba_out.shape}\")\n",
    "\n",
    "# Test Arbiter\n",
    "arbiter = GRUArbiter(d_model=d_model).to(device)\n",
    "fused, weights, hidden = arbiter(rwkv_out, mamba_out)\n",
    "assert fused.shape == (batch_size, seq_len, d_model), f\"Fused output shape mismatch: {fused.shape}\"\n",
    "assert weights.shape == (batch_size, seq_len, 2), f\"Weights shape mismatch: {weights.shape}\"\n",
    "assert hidden.shape == (batch_size, d_model), f\"Hidden shape mismatch: {hidden.shape}\"\n",
    "print(f\"âœ“ Arbiter forward pass: ({rwkv_out.shape}, {mamba_out.shape}) -> fused={fused.shape}, weights={weights.shape}\")\n",
    "\n",
    "# Check weights sum to 1\n",
    "weight_sums = weights.sum(dim=-1)\n",
    "assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5), \"Weights don't sum to 1\"\n",
    "print(f\"âœ“ Weights sum to 1.0 (mean: {weight_sums.mean():.6f})\")\n",
    "\n",
    "print(\"\\nâœ“ All sanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae171565",
   "metadata": {},
   "source": [
    "## 4. Experiment 1: Constant Input\n",
    "\n",
    "**Hypothesis**: When both inputs are identical, arbiter should output Î± â‰ˆ [0.5, 0.5] (no preference).\n",
    "\n",
    "**Test**: Feed identical tensors to arbiter, measure Î± stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 1: Constant Input\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create arbiter\n",
    "arbiter = GRUArbiter(d_model=128).to(device)\n",
    "arbiter.eval()\n",
    "\n",
    "# Identical inputs\n",
    "x_const = torch.randn(1, 64, 128, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fused, weights, hidden = arbiter(x_const, x_const)\n",
    "\n",
    "# Analyze weights\n",
    "Î±_rwkv = weights[..., 0]  # (1, 64)\n",
    "Î±_mamba = weights[..., 1]  # (1, 64)\n",
    "\n",
    "exp1_results = {\n",
    "    \"experiment\": \"constant_input\",\n",
    "    \"config\": {\n",
    "        \"d_model\": 128,\n",
    "        \"seq_len\": 64,\n",
    "        \"batch_size\": 1\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"alpha_rwkv_mean\": Î±_rwkv.mean().item(),\n",
    "        \"alpha_rwkv_std\": Î±_rwkv.std().item(),\n",
    "        \"alpha_rwkv_min\": Î±_rwkv.min().item(),\n",
    "        \"alpha_rwkv_max\": Î±_rwkv.max().item(),\n",
    "        \"alpha_per_timestep\": Î±_rwkv.squeeze().cpu().tolist(),\n",
    "        \"hidden_state_norm\": hidden.norm().item(),\n",
    "        \"observations\": [\n",
    "            f\"With identical inputs, Î±_rwkv ranges from {Î±_rwkv.min().item():.3f} to {Î±_rwkv.max().item():.3f}\",\n",
    "            f\"Mean Î±_rwkv = {Î±_rwkv.mean().item():.3f} (expected ~0.5 for balanced)\",\n",
    "            f\"Std Î±_rwkv = {Î±_rwkv.std().item():.4f} (low = stable, high = varying)\",\n",
    "            \"Expectation: Î± should be close to 0.5 with low variance\" if Î±_rwkv.std() < 0.1 else \"Warning: High variance in weights despite identical inputs\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display\n",
    "print(f\"\\nÎ±_rwkv: mean={Î±_rwkv.mean():.4f}, std={Î±_rwkv.std():.4f}, range=[{Î±_rwkv.min():.4f}, {Î±_rwkv.max():.4f}]\")\n",
    "print(f\"Î±_mamba: mean={Î±_mamba.mean():.4f}, std={Î±_mamba.std():.4f}, range=[{Î±_mamba.min():.4f}, {Î±_mamba.max():.4f}]\")\n",
    "print(f\"\\nHidden state norm: {hidden.norm():.4f}\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Î±_rwkv.squeeze().cpu(), label='Î±_rwkv', alpha=0.7)\n",
    "plt.plot(Î±_mamba.squeeze().cpu(), label='Î±_mamba', alpha=0.7)\n",
    "plt.axhline(0.5, color='black', linestyle='--', alpha=0.3, label='Expected (0.5)')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Exp 1: Î± Over Sequence (Identical Inputs)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(Î±_rwkv.squeeze().cpu().numpy(), bins=20, alpha=0.7, label='Î±_rwkv')\n",
    "plt.axvline(0.5, color='black', linestyle='--', alpha=0.5, label='Expected')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Î±_rwkv')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment 1 Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5270395",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: Synthetic Divergence\n",
    "\n",
    "**Hypothesis**: Arbiter should differentiate between amplifier-pattern (growing variance) and damper-pattern (shrinking variance).\n",
    "\n",
    "**Test**: Feed synthetic signals with opposite variance trends, measure Î± response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b302a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 2: Synthetic Divergence\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "arbiter = GRUArbiter(d_model=128).to(device)\n",
    "arbiter.eval()\n",
    "\n",
    "# Amplifier signal: variance grows over sequence (simulates RWKV behavior)\n",
    "amp_scale = torch.linspace(1.0, 3.0, 64, device=device).view(1, -1, 1)\n",
    "amp_signal = torch.randn(1, 64, 128, device=device) * amp_scale\n",
    "\n",
    "# Damper signal: variance shrinks over sequence (simulates Mamba behavior)\n",
    "damp_scale = torch.linspace(1.0, 0.3, 64, device=device).view(1, -1, 1)\n",
    "damp_signal = torch.randn(1, 64, 128, device=device) * damp_scale\n",
    "\n",
    "with torch.no_grad():\n",
    "    fused, weights, hidden = arbiter(amp_signal, damp_signal)\n",
    "\n",
    "Î±_rwkv = weights[..., 0].squeeze()\n",
    "Î±_mamba = weights[..., 1].squeeze()\n",
    "\n",
    "# Measure signal norms over time\n",
    "amp_norms = amp_signal.norm(dim=-1).squeeze()\n",
    "damp_norms = damp_signal.norm(dim=-1).squeeze()\n",
    "\n",
    "exp2_results = {\n",
    "    \"experiment\": \"synthetic_divergence\",\n",
    "    \"config\": {\n",
    "        \"d_model\": 128,\n",
    "        \"seq_len\": 64,\n",
    "        \"amp_scale_range\": [1.0, 3.0],\n",
    "        \"damp_scale_range\": [1.0, 0.3]\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"alpha_rwkv_mean\": Î±_rwkv.mean().item(),\n",
    "        \"alpha_rwkv_std\": Î±_rwkv.std().item(),\n",
    "        \"alpha_correlation_with_amp_norm\": torch.corrcoef(torch.stack([Î±_rwkv.cpu(), amp_norms.cpu()]))[0, 1].item(),\n",
    "        \"alpha_per_timestep\": Î±_rwkv.cpu().tolist(),\n",
    "        \"observations\": [\n",
    "            f\"Î±_rwkv starts at {Î±_rwkv[0]:.3f}, ends at {Î±_rwkv[-1]:.3f}\",\n",
    "            f\"Amplifier norm grows: {amp_norms[0]:.2f} -> {amp_norms[-1]:.2f}\",\n",
    "            f\"Damper norm shrinks: {damp_norms[0]:.2f} -> {damp_norms[-1]:.2f}\",\n",
    "            f\"Correlation(Î±_rwkv, amp_norm): {torch.corrcoef(torch.stack([Î±_rwkv.cpu(), amp_norms.cpu()]))[0, 1]:.3f}\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nÎ±_rwkv: mean={Î±_rwkv.mean():.4f}, std={Î±_rwkv.std():.4f}\")\n",
    "print(f\"Amplifier norm growth: {amp_norms[0]:.2f} -> {amp_norms[-1]:.2f}\")\n",
    "print(f\"Damper norm decay: {damp_norms[0]:.2f} -> {damp_norms[-1]:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Weights over time\n",
    "axes[0, 0].plot(Î±_rwkv.cpu(), label='Î±_rwkv (amp)', alpha=0.7)\n",
    "axes[0, 0].plot(Î±_mamba.cpu(), label='Î±_mamba (damp)', alpha=0.7)\n",
    "axes[0, 0].axhline(0.5, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Weight')\n",
    "axes[0, 0].set_title('Exp 2: Weights Over Sequence')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Signal norms\n",
    "axes[0, 1].plot(amp_norms.cpu(), label='Amplifier norm', alpha=0.7)\n",
    "axes[0, 1].plot(damp_norms.cpu(), label='Damper norm', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Norm')\n",
    "axes[0, 1].set_title('Signal Norm Trends')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight vs amp norm scatter\n",
    "axes[1, 0].scatter(amp_norms.cpu(), Î±_rwkv.cpu(), alpha=0.5, s=20)\n",
    "axes[1, 0].set_xlabel('Amplifier Norm')\n",
    "axes[1, 0].set_ylabel('Î±_rwkv')\n",
    "axes[1, 0].set_title('Î±_rwkv vs Amplifier Norm')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight distribution\n",
    "axes[1, 1].hist(Î±_rwkv.cpu().numpy(), bins=20, alpha=0.7, label='Î±_rwkv')\n",
    "axes[1, 1].axvline(0.5, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Weight Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Î±_rwkv')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment 2 Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42816f",
   "metadata": {},
   "source": [
    "## 6. Experiment 3-4: Real RWKV and Mamba Outputs\n",
    "\n",
    "**Hypothesis**: Arbiter responds differently to real RWKV vs real Mamba outputs.\n",
    "\n",
    "**Test**: Generate actual layer outputs, measure Î± distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENTS 3-4: Real RWKV & Mamba Outputs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create models\n",
    "rwkv = RWKV6TimeMix(hidden_size=128, num_heads=4).to(device)\n",
    "mamba = Mamba2TimeMix(d_model=128, d_state=16, d_conv=4, expand=2).to(device)\n",
    "arbiter = GRUArbiter(d_model=128).to(device)\n",
    "\n",
    "# Set to eval\n",
    "rwkv.eval()\n",
    "mamba.eval()\n",
    "arbiter.eval()\n",
    "\n",
    "# Generate input\n",
    "x = torch.randn(1, 64, 128, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Real outputs\n",
    "    rwkv_out = rwkv(x)\n",
    "    mamba_out = mamba(x)\n",
    "    \n",
    "    # Feed to arbiter\n",
    "    fused, weights, hidden = arbiter(rwkv_out, mamba_out)\n",
    "\n",
    "Î±_rwkv = weights[..., 0].squeeze()\n",
    "Î±_mamba = weights[..., 1].squeeze()\n",
    "\n",
    "# Analyze outputs\n",
    "rwkv_norm = rwkv_out.norm(dim=-1).squeeze()\n",
    "mamba_norm = mamba_out.norm(dim=-1).squeeze()\n",
    "\n",
    "exp34_results = {\n",
    "    \"experiment\": \"real_outputs\",\n",
    "    \"config\": {\n",
    "        \"d_model\": 128,\n",
    "        \"seq_len\": 64\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"rwkv_norm_mean\": rwkv_norm.mean().item(),\n",
    "        \"rwkv_norm_std\": rwkv_norm.std().item(),\n",
    "        \"mamba_norm_mean\": mamba_norm.mean().item(),\n",
    "        \"mamba_norm_std\": mamba_norm.std().item(),\n",
    "        \"alpha_rwkv_mean\": Î±_rwkv.mean().item(),\n",
    "        \"alpha_rwkv_std\": Î±_rwkv.std().item(),\n",
    "        \"alpha_per_timestep\": Î±_rwkv.cpu().tolist(),\n",
    "        \"observations\": [\n",
    "            f\"RWKV output norm: {rwkv_norm.mean():.2f} Â± {rwkv_norm.std():.2f}\",\n",
    "            f\"Mamba output norm: {mamba_norm.mean():.2f} Â± {mamba_norm.std():.2f}\",\n",
    "            f\"Î±_rwkv: {Î±_rwkv.mean():.3f} Â± {Î±_rwkv.std():.3f}\",\n",
    "            \"Arbiter prefers RWKV\" if Î±_rwkv.mean() > 0.6 else \"Arbiter prefers Mamba\" if Î±_rwkv.mean() < 0.4 else \"Arbiter is balanced\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nRWKV output norm: {rwkv_norm.mean():.4f} Â± {rwkv_norm.std():.4f}\")\n",
    "print(f\"Mamba output norm: {mamba_norm.mean():.4f} Â± {mamba_norm.std():.4f}\")\n",
    "print(f\"\\nÎ±_rwkv: {Î±_rwkv.mean():.4f} Â± {Î±_rwkv.std():.4f}\")\n",
    "print(f\"Î±_mamba: {Î±_mamba.mean():.4f} Â± {Î±_mamba.std():.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Output norms\n",
    "axes[0, 0].plot(rwkv_norm.cpu(), label='RWKV norm', alpha=0.7)\n",
    "axes[0, 0].plot(mamba_norm.cpu(), label='Mamba norm', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Norm')\n",
    "axes[0, 0].set_title('Real Output Norms')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weights\n",
    "axes[0, 1].plot(Î±_rwkv.cpu(), label='Î±_rwkv', alpha=0.7)\n",
    "axes[0, 1].plot(Î±_mamba.cpu(), label='Î±_mamba', alpha=0.7)\n",
    "axes[0, 1].axhline(0.5, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Weight')\n",
    "axes[0, 1].set_title('Arbiter Weights (Real Outputs)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: weight vs norm difference\n",
    "norm_diff = rwkv_norm - mamba_norm\n",
    "axes[1, 0].scatter(norm_diff.cpu(), Î±_rwkv.cpu(), alpha=0.5, s=20)\n",
    "axes[1, 0].axvline(0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].axhline(0.5, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].set_xlabel('RWKV Norm - Mamba Norm')\n",
    "axes[1, 0].set_ylabel('Î±_rwkv')\n",
    "axes[1, 0].set_title('Weight Response to Norm Difference')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distributions\n",
    "axes[1, 1].hist(rwkv_norm.cpu().numpy(), bins=20, alpha=0.5, label='RWKV norm')\n",
    "axes[1, 1].hist(mamba_norm.cpu().numpy(), bins=20, alpha=0.5, label='Mamba norm')\n",
    "axes[1, 1].set_xlabel('Norm Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Output Norm Distributions')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiments 3-4 Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867204a",
   "metadata": {},
   "source": [
    "## 7. Experiment 6: Hidden State Analysis\n",
    "\n",
    "**Hypothesis**: GRU hidden state carries information across timesteps.\n",
    "\n",
    "**Test**: Track hidden state norm evolution, check for growth/decay/oscillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 6: Hidden State Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create fresh arbiter\n",
    "arbiter = GRUArbiter(d_model=128).to(device)\n",
    "arbiter.eval()\n",
    "\n",
    "# Generate real outputs\n",
    "rwkv = RWKV6TimeMix(hidden_size=128, num_heads=4).to(device).eval()\n",
    "mamba = Mamba2TimeMix(d_model=128, d_state=16, d_conv=4, expand=2).to(device).eval()\n",
    "\n",
    "x = torch.randn(1, 64, 128, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    rwkv_out = rwkv(x)\n",
    "    mamba_out = mamba(x)\n",
    "\n",
    "# Process timestep-by-timestep, tracking hidden state\n",
    "hidden_norms = []\n",
    "weights_over_time = []\n",
    "hidden = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(64):\n",
    "        _, w, hidden = arbiter(\n",
    "            rwkv_out[:, t:t+1],\n",
    "            mamba_out[:, t:t+1],\n",
    "            hidden\n",
    "        )\n",
    "        hidden_norms.append(hidden.norm().item())\n",
    "        weights_over_time.append(w.squeeze().cpu())\n",
    "\n",
    "hidden_norms = np.array(hidden_norms)\n",
    "weights_over_time = torch.stack(weights_over_time)  # (64, 2)\n",
    "\n",
    "exp6_results = {\n",
    "    \"experiment\": \"hidden_state_analysis\",\n",
    "    \"config\": {\n",
    "        \"d_model\": 128,\n",
    "        \"seq_len\": 64\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"hidden_norm_initial\": hidden_norms[0],\n",
    "        \"hidden_norm_final\": hidden_norms[-1],\n",
    "        \"hidden_norm_mean\": hidden_norms.mean(),\n",
    "        \"hidden_norm_std\": hidden_norms.std(),\n",
    "        \"hidden_state_norms\": hidden_norms.tolist(),\n",
    "        \"observations\": [\n",
    "            f\"Hidden norm: {hidden_norms[0]:.3f} -> {hidden_norms[-1]:.3f}\",\n",
    "            f\"Mean: {hidden_norms.mean():.3f}, Std: {hidden_norms.std():.3f}\",\n",
    "            \"Growing\" if hidden_norms[-1] > hidden_norms[0] * 1.2 else \"Decaying\" if hidden_norms[-1] < hidden_norms[0] * 0.8 else \"Stable\",\n",
    "            \"Hidden state carries temporal information\" if hidden_norms.std() > 0.5 else \"Hidden state is relatively constant\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nHidden norm evolution: {hidden_norms[0]:.4f} -> {hidden_norms[-1]:.4f}\")\n",
    "print(f\"Mean: {hidden_norms.mean():.4f}, Std: {hidden_norms.std():.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Hidden norm over time\n",
    "axes[0].plot(hidden_norms, color='purple', alpha=0.7)\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].set_ylabel('Hidden State Norm')\n",
    "axes[0].set_title('Hidden State Evolution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weights over time (recurrent)\n",
    "axes[1].plot(weights_over_time[:, 0], label='Î±_rwkv', alpha=0.7)\n",
    "axes[1].plot(weights_over_time[:, 1], label='Î±_mamba', alpha=0.7)\n",
    "axes[1].axhline(0.5, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylabel('Weight')\n",
    "axes[1].set_title('Weights (Recurrent Processing)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: hidden norm vs weight\n",
    "axes[2].scatter(hidden_norms, weights_over_time[:, 0].numpy(), alpha=0.5, s=20)\n",
    "axes[2].set_xlabel('Hidden State Norm')\n",
    "axes[2].set_ylabel('Î±_rwkv')\n",
    "axes[2].set_title('Weight vs Hidden Norm')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment 6 Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ce980",
   "metadata": {},
   "source": [
    "## 8. Experiment 7: Trainability Test\n",
    "\n",
    "**Hypothesis**: Arbiter can learn to make useful gating decisions.\n",
    "\n",
    "**Test**: Simple supervised task - predict which signal has higher variance, train arbiter to weight it lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 7: Trainability Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create trainable arbiter\n",
    "arbiter = GRUArbiter(d_model=128).to(device)\n",
    "optimizer = torch.optim.Adam(arbiter.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_steps = 100\n",
    "losses = []\n",
    "weights_history = []\n",
    "\n",
    "print(\"\\nTraining arbiter to weight lower-variance signal higher...\")\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # Generate signals with different variances\n",
    "    variance_ratio = torch.rand(1).item() * 2 + 0.5  # Random in [0.5, 2.5]\n",
    "    \n",
    "    signal_a = torch.randn(1, 64, 128, device=device) * variance_ratio\n",
    "    signal_b = torch.randn(1, 64, 128, device=device)\n",
    "    \n",
    "    # Target: weight lower-variance signal higher\n",
    "    var_a = signal_a.var(dim=-1, keepdim=True).mean()\n",
    "    var_b = signal_b.var(dim=-1, keepdim=True).mean()\n",
    "    \n",
    "    target_weight = (var_b / (var_a + var_b)).item()  # If var_a > var_b, target < 0.5\n",
    "    \n",
    "    # Forward pass\n",
    "    fused, weights, hidden = arbiter(signal_a, signal_b)\n",
    "    predicted_weight = weights[..., 0].mean()  # Î± for signal_a\n",
    "    \n",
    "    # Loss: want predicted_weight â‰ˆ target_weight\n",
    "    loss = F.mse_loss(predicted_weight, torch.tensor(target_weight, device=device))\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    weights_history.append(predicted_weight.item())\n",
    "    \n",
    "    if (step + 1) % 20 == 0:\n",
    "        print(f\"Step {step+1}/{n_steps}, Loss: {loss.item():.6f}, Pred: {predicted_weight.item():.3f}, Target: {target_weight:.3f}\")\n",
    "\n",
    "exp7_results = {\n",
    "    \"experiment\": \"trainability_test\",\n",
    "    \"config\": {\n",
    "        \"d_model\": 128,\n",
    "        \"seq_len\": 64,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"learning_rate\": 0.001\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"initial_loss\": losses[0],\n",
    "        \"final_loss\": losses[-1],\n",
    "        \"loss_reduction\": (losses[0] - losses[-1]) / losses[0],\n",
    "        \"training_curve\": losses,\n",
    "        \"observations\": [\n",
    "            f\"Loss: {losses[0]:.6f} -> {losses[-1]:.6f}\",\n",
    "            f\"Reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\",\n",
    "            \"Arbiter is trainable\" if losses[-1] < losses[0] * 0.5 else \"Limited learning observed\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(losses, alpha=0.7)\n",
    "axes[0].set_xlabel('Training Step')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight evolution\n",
    "axes[1].plot(weights_history, alpha=0.7)\n",
    "axes[1].axhline(0.5, color='black', linestyle='--', alpha=0.3, label='Balanced')\n",
    "axes[1].set_xlabel('Training Step')\n",
    "axes[1].set_ylabel('Predicted Weight (Î±_a)')\n",
    "axes[1].set_title('Weight Evolution During Training')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment 7 Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91ec37",
   "metadata": {},
   "source": [
    "## 9. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = {\n",
    "    \"experiment_1\": exp1_results,\n",
    "    \"experiment_2\": exp2_results,\n",
    "    \"experiment_3_4\": exp34_results,\n",
    "    \"experiment_6\": exp6_results,\n",
    "    \"experiment_7\": exp7_results\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_dir = Path(\"/home/claude/logs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = output_dir / \"task_0_1_exploration_results.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_table = [\n",
    "    [\"Experiment\", \"Key Finding\", \"Conclusion\"],\n",
    "    [\"-\"*20, \"-\"*35, \"-\"*20],\n",
    "    [\n",
    "        \"1: Constant Input\",\n",
    "        f\"Î±_rwkv = {exp1_results['results']['alpha_rwkv_mean']:.3f} Â± {exp1_results['results']['alpha_rwkv_std']:.3f}\",\n",
    "        \"Balanced\" if abs(exp1_results['results']['alpha_rwkv_mean'] - 0.5) < 0.1 else \"Biased\"\n",
    "    ],\n",
    "    [\n",
    "        \"2: Synthetic Div\",\n",
    "        f\"Î± varies: {exp2_results['results']['alpha_rwkv_std']:.3f} std\",\n",
    "        \"Responds to variance\" if exp2_results['results']['alpha_rwkv_std'] > 0.05 else \"Static\"\n",
    "    ],\n",
    "    [\n",
    "        \"3-4: Real Outputs\",\n",
    "        f\"Î±_rwkv = {exp34_results['results']['alpha_rwkv_mean']:.3f}\",\n",
    "        \"Prefers RWKV\" if exp34_results['results']['alpha_rwkv_mean'] > 0.6 else \"Prefers Mamba\" if exp34_results['results']['alpha_rwkv_mean'] < 0.4 else \"Balanced\"\n",
    "    ],\n",
    "    [\n",
    "        \"6: Hidden State\",\n",
    "        f\"Norm: {exp6_results['results']['hidden_norm_initial']:.2f} -> {exp6_results['results']['hidden_norm_final']:.2f}\",\n",
    "        \"Carries info\" if exp6_results['results']['hidden_norm_std'] > 0.5 else \"Static\"\n",
    "    ],\n",
    "    [\n",
    "        \"7: Trainability\",\n",
    "        f\"Loss reduction: {exp7_results['results']['loss_reduction']*100:.1f}%\",\n",
    "        \"Trainable\" if exp7_results['results']['loss_reduction'] > 0.3 else \"Limited\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for row in summary_table:\n",
    "    print(f\"{row[0]:<25} {row[1]:<40} {row[2]:<20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CRITICAL QUESTIONS ANSWERED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "questions = [\n",
    "    (\"Does Î± vary meaningfully?\", \n",
    "     \"YES\" if exp2_results['results']['alpha_rwkv_std'] > 0.05 else \"NO\"),\n",
    "    (\"Does GRU hidden carry information?\", \n",
    "     \"YES\" if exp6_results['results']['hidden_norm_std'] > 0.5 else \"LIMITED\"),\n",
    "    (\"Can arbiter distinguish signals?\", \n",
    "     \"YES\" if exp2_results['results']['alpha_rwkv_std'] > 0.05 else \"NO\"),\n",
    "    (\"Can arbiter be trained?\", \n",
    "     \"YES\" if exp7_results['results']['loss_reduction'] > 0.3 else \"LIMITED\"),\n",
    "]\n",
    "\n",
    "for q, a in questions:\n",
    "    print(f\"  â€¢ {q:<45} {a}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATIONS FOR NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Analyze results\n",
    "if exp7_results['results']['loss_reduction'] > 0.5:\n",
    "    recommendations.append(\"âœ“ Arbiter shows strong trainability - proceed with Phase 1 implementation\")\n",
    "elif exp7_results['results']['loss_reduction'] > 0.3:\n",
    "    recommendations.append(\"âš  Arbiter shows moderate trainability - consider architecture modifications\")\n",
    "else:\n",
    "    recommendations.append(\"âœ— Arbiter shows limited trainability - explore alternatives (attention, learned gates)\")\n",
    "\n",
    "if exp2_results['results']['alpha_rwkv_std'] < 0.05:\n",
    "    recommendations.append(\"âš  Î± weights are too static - may need stronger signal differentiation\")\n",
    "\n",
    "if exp6_results['results']['hidden_norm_std'] < 0.5:\n",
    "    recommendations.append(\"âš  Hidden state not carrying much information - recurrence may be unnecessary\")\n",
    "\n",
    "if abs(exp1_results['results']['alpha_rwkv_mean'] - 0.5) > 0.2:\n",
    "    recommendations.append(\"âš  Initialization bias detected - may need better init strategy\")\n",
    "\n",
    "recommendations.append(\"ðŸ“Š Run Phase 0 characterization on both RWKV and Mamba in hybrid context\")\n",
    "recommendations.append(\"ðŸ”¬ Test with actual training data to validate gating behavior\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Exploration Complete - Ready for Architecture Decision\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
