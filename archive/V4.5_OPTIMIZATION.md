# V4.5 Optimization & Analysis Guide

**Created:** 2026-01-09  
**Status:** Active  
**Purpose:** Performance optimization and analysis methodology for RwMBA (Roomba) hybrid model

---

## üêß ENVIRONMENT: NATIVE LINUX

All optimization work happens in native Linux (Ubuntu) with full CUDA kernel support.

- Paths: `/home/m_tes/groundthink/...`
- Commands: bash/shell syntax
- GPU: RTX 4050 (6GB VRAM, sm_89)
- CUDA: 12.4 with native causal-conv1d and mamba-ssm

---

## Document Overview

**V4.5 Focus:** Optimize and analyze the existing 3.8M parameter hybrid model before scaling.

**Sections:**
1. [Baseline Metrics](#baseline-metrics) - Current performance state
2. [GPU & System Monitoring](#gpu--system-monitoring) - Real-time performance tracking
3. [Performance Optimization](#performance-optimization) - Concrete speed improvements
4. [Profiling & Bottleneck Analysis](#profiling--bottleneck-analysis) - Finding inefficiencies
5. [Controlled Experiments](#controlled-experiments) - Variable isolation methodology
6. [Analysis Metrics](#analysis-metrics) - What to measure and why
7. [Implementation Checklist](#implementation-checklist) - Actionable tasks

---

## Baseline Metrics

**Current State (from V4 Build Log):**

| Metric | Value | Source |
|--------|-------|--------|
| Model Size | 3.8M params | hybrid_v4.py |
| Training Throughput | ~33K tokens/sec | First training run |
| VRAM Usage | ~422 MB | Training with batch=8 |
| Final Loss | 1.37-1.38 | 5000 steps on shakespeare.txt |
| Perplexity | ~3.0 | Calculated from loss |
| Gradient Ratio | 0.15-0.16 | RWKV/Mamba (WARN range) |

**Hardware:**
- GPU: RTX 4050 (6GB VRAM, 115W TDP)
- Compute Capability: 8.9 (sm_89)
- CUDA Kernels: Native (causal-conv1d v1.2.0, mamba-ssm v2.2.0)

**Optimization Targets:**
- Training throughput: 33K ‚Üí 165K-330K tokens/sec (5-10x improvement)
- GPU utilization: Unknown ‚Üí 80-90%+ during compute
- Temperature: Keep <80¬∞C to avoid throttling
- Power draw: Expect 50-80W under load (max 115W)

---

## GPU & System Monitoring

### Real-Time Monitoring Tools

#### 1. nvidia-smi (Essential Baseline)

**Setup:**
```bash
# Basic monitoring (refresh every 1 second)
nvidia-smi -l 1

# Log to file for post-run analysis
nvidia-smi -l 1 > logs/gpu_monitor_$(date +%Y%m%d_%H%M%S).log &
# Save PID to stop later: echo $! > /tmp/nvidia_smi.pid
```

**What to watch:**
- **GPU Util %**: Target 80-90%+ during forward/backward passes
  - Low util (<50%) = I/O bottleneck or inefficient kernels
  - High util (>90%) = Good, compute-bound
- **Memory**: Should use <1GB for 3.8M model + batch 8
  - Leaves 5GB headroom for optimization experiments
- **Power (W)**: Expect 50-80W during training
  - Spikes >100W may indicate inefficiency
- **Temp (¬∞C)**: Keep under 80¬∞C
  - Above 80¬∞C = throttling risk
- **Processes**: Verify only your Python script is using GPU

**Stopping background monitor:**
```bash
kill $(cat /tmp/nvidia_smi.pid)
```

#### 2. nvtop (Interactive Dashboard)

**Installation:**
```bash
sudo apt install nvtop
```

**Usage:**
```bash
nvtop  # Press 'q' to quit
```

**What you get:**
- Real-time graphs for GPU util, memory, power
- SM (streaming multiprocessor) occupancy
- Per-process breakdown
- Encoder/decoder usage

**Diagnosis:**
- **High memory, low util** = Memory-bound, need better data layout
- **High util, high SM occupancy** = Optimal, compute-bound
- **Low util, low memory** = I/O bottleneck (data loading)

#### 3. powerstat (System-Wide Power)

**Installation:**
```bash
sudo apt install powerstat
```

**Usage:**
```bash
# Sample every 1 second, show CPU+GPU+system power
powerstat -R -d 1
```

**Why this matters:**
- Tracks total system power consumption
- Helps identify if CPU is bottleneck (high CPU power = data loading issue)
- Baseline for efficiency comparisons

#### 4. htop (CPU/Memory Side)

**Usage:**
```bash
htop  # Press F10 to quit
```

**What to check:**
- CPU usage by Python process
- If CPU maxed out = data loader bottleneck
- System memory usage (should have plenty free)

---

## Performance Optimization

### Quick Wins (2-3x speedup)

#### 1. Increase Batch Size

**Current:** batch_size=8  
**Target:** 8-16 (monitor VRAM with nvidia-smi)

**Implementation:**
```python
# In train_v4.py CONFIG
'batch_size': 16,  # Test with nvidia-smi to ensure <5GB used
```

**Expected gain:** 1.5-2x (better GPU utilization)

**Validation:**
```bash
# During training, check memory
nvidia-smi
# Look for "Memory-Usage" - should stay under 5GB
```

#### 2. DataLoader Optimization

**Current:** num_workers=0 (default)  
**Target:** 2-4 workers with pinned memory

**Implementation:**
```python
# In data_v030.py or train_v4.py
dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    num_workers=4,      # Parallel data loading
    pin_memory=True,    # Faster CPU‚ÜíGPU transfer
    persistent_workers=True  # Keep workers alive between epochs
)
```

**Expected gain:** 1.5-2x if I/O was bottleneck

#### 3. Mixed Precision Training (AMP)

**Current:** fp32 (default)  
**Target:** fp16/bf16 automatic mixed precision

**Implementation:**
```python
# In train_v4.py training loop
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for step in range(max_steps):
    # Forward pass with autocast
    with autocast(device_type='cuda', dtype=torch.float16):
        logits = model(x)
        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))
    
    # Backward with scaling
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

**Expected gain:** 2x (RTX 4050 has good fp16 support)  
**Risk:** Minimal quality impact at this scale

#### 4. torch.compile (PyTorch 2.0+)

**Current:** No compilation  
**Target:** JIT-compile model for kernel fusion

**Implementation:**
```python
# In train_v4.py, after model creation
import torch

model = create_hybrid_5m(vocab_size=tokenizer.vocab_size)
model = torch.compile(model)  # Compile before .to(device)
model = model.to(device)
```

**Expected gain:** 1.5-2x for small dims like 256  
**Benefit:** Fuses operations in parallel RWKV+Mamba blocks

**Note:** First few steps will be slow (compilation time), then speedup kicks in

---

### Advanced Optimizations (Experimental)

#### 5. Persistent Mode for GPU

**Setup:**
```bash
# Enable persistent mode (survives reboots)
sudo nvidia-smi -pm 1

# Set performance mode (max clocks)
sudo nvidia-smi -lgc 2100  # Lock GPU clock to max
```

**When to use:** Before long training runs for consistent performance

#### 6. Quantization (Inference Only)

**For generation/inference speedup:**
```python
import torch.quantization as quant

# After training, quantize for inference
model_int8 = quant.quantize_dynamic(
    model,
    {torch.nn.Linear},  # Quantize linear layers
    dtype=torch.qint8
)
```

**Expected gain:** 1.5-2x inference speed  
**Trade-off:** Slight quality degradation (test on validation set)

---

## Profiling & Bottleneck Analysis

### torch.profiler (Find Slow Operations)

**Setup:**
```python
import torch.profiler as profiler

# Wrap training loop section
with profiler.profile(
    activities=[
        profiler.ProfilerActivity.CPU,
        profiler.ProfilerActivity.CUDA,
    ],
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    for _ in range(10):  # Profile 10 steps
        loss = training_step(model, batch)
        loss.backward()
        optimizer.step()

# Export for viewing
prof.export_chrome_trace("trace.json")
```

**Analysis:**
1. Open Chrome browser
2. Navigate to `chrome://tracing`
3. Load `trace.json`
4. Look for:
   - Long operations (red bars)
   - GPU idle time (gaps)
   - Memory spikes

**Common bottlenecks:**
- Mamba2 selective state update
- RWKV6 channel mixing
- Data loading (if CPU time dominates)

### nsys Profile (CUDA Kernel Level)

**For detailed CUDA analysis:**
```bash
# Profile training script
nsys profile \
    --trace=cuda,nvtx \
    --output=profile_$(date +%Y%m%d_%H%M%S).nsys-rep \
    python train_v4.py --max_steps=100

# View in Nsight Systems GUI (if available)
# Or analyze report file
```

**What to look for:**
- Kernel launch overhead
- Memory transfer bottlenecks
- Synchronization points

### Quick Profiling During Training

**Add to training loop:**
```python
import time

# Before training starts
step_times = []
gpu_utils = []

for step in range(max_steps):
    step_start = time.time()
    
    # Training step
    loss = forward_backward(batch)
    
    step_times.append(time.time() - step_start)
    
    if step % 100 == 0:
        avg_time = np.mean(step_times[-100:])
        tokens_per_sec = (batch_size * seq_len) / avg_time
        print(f"Step {step}: {tokens_per_sec:.0f} tok/s, {avg_time*1000:.1f}ms/step")
```

---

## Controlled Experiments

### Variable Isolation Methodology

**Goal:** Understand which optimizations provide real speedup

**Process:**
1. **Baseline run:** Current configuration, no optimizations
2. **Single variable change:** Add ONE optimization
3. **Measure impact:** Compare throughput, GPU util, loss convergence
4. **Document:** Record results before moving to next variable

**Variables to test:**

| Variable | Baseline | Test Value | Expected Gain |
|----------|----------|------------|---------------|
| Batch size | 8 | 16 | 1.5-2x |
| num_workers | 0 | 4 | 1.5-2x |
| Mixed precision | fp32 | fp16 | 2x |
| torch.compile | Off | On | 1.5-2x |
| Gradient accumulation | 1 | 2 | Neutral (same effective batch) |

**Experiment template:**
```bash
# 1. Baseline (100 steps)
python train_v4.py --max_steps=100 --config=baseline

# 2. Test optimization
python train_v4.py --max_steps=100 --config=baseline --batch_size=16

# 3. Compare logs
# - Throughput (tok/s)
# - GPU utilization %
# - VRAM usage
# - Loss convergence (should be similar)
```

### Metric Tracking

**Log these for each experiment:**
```python
experiment_log = {
    'name': 'batch_size_16',
    'date': '2026-01-09',
    'config': {'batch_size': 16, 'num_workers': 4},
    'metrics': {
        'throughput_tok_s': 65000,
        'gpu_util_pct': 87,
        'vram_mb': 1200,
        'final_loss': 1.35,
        'time_per_step_ms': 32.5
    },
    'vs_baseline': {
        'speedup': 1.97,  # 65000 / 33000
        'loss_delta': -0.02  # Better
    }
}
```

---

## Analysis Metrics

### Performance Metrics

**Primary:**
- **Throughput (tokens/sec):** Higher is better, aim for 5-10x baseline
- **GPU Utilization (%):** Target 80-90%+ during training
- **VRAM Usage (MB):** Should stay well under 6GB limit
- **Training Loss:** Should converge similarly to baseline (validates optimization doesn't break training)

**Secondary:**
- **Time per step (ms):** Lower is better
- **Power consumption (W):** Lower at same performance = more efficient
- **Temperature (¬∞C):** Lower = better thermal management

### Quality Metrics (Must Not Degrade)

**Validation checks:**
- **Final validation loss:** Should be within ¬±0.05 of baseline
- **Gradient ratio (RWKV/Mamba):** Should remain in similar range
- **Perplexity:** Should not increase significantly
- **Sample quality:** Generated text should remain coherent

**Red flags:**
- Val loss >10% higher than baseline = optimization broke something
- Loss curve erratic = numerical instability (e.g., bad fp16 scaling)
- NaN/Inf losses = immediate stop and debug

### Diagnostic Metrics

**From V4_DIAGNOSTICS.md, prioritize:**
1. **Component gradient ratio:** Track RWKV/Mamba balance
2. **Activation variance:** Ensure no component collapse
3. **State entropy:** Monitor state dynamics (if accessible)

**New for V4.5:**
4. **SM occupancy:** From nvtop, aim for high values
5. **Memory bandwidth utilization:** From nvidia-smi/nvtop
6. **Kernel execution time:** From torch.profiler

### Long-Context Retrieval Testing

**Needle-in-a-Haystack (NIAH) Test:** Validates long-context memory retention

**Purpose:** Reveals "context rot" where accuracy drops beyond a certain length (e.g., models failing at 10K+ tokens despite claiming 128K context).

**How it works:** Hide a key fact (the "needle") in filler content (the "haystack") and query it. Vary needle position (start, middle, end) and context length to spot degradation.

**Setup:**
```bash
# 1. Install the testing tool
pip install needlehaystack

# 2. Set API keys (if testing external models for comparison)
export NIAH_MODEL_API_KEY="your-api-key"

# 3. Run basic test (example: 2K tokens, needle at 50% depth)
needlehaystack.run_test \
  --provider openai \
  --model_name "gpt-3.5-turbo-0125" \
  --context_lengths "[2000]" \
  --document_depth_percents "[50]"

# 4. Multi-needle test (10 facts spaced evenly)
needlehaystack.run_test \
  --provider openai \
  --model_name "gpt-3.5-turbo-0125" \
  --context_lengths "[2000,5000,10000]" \
  --document_depth_percents "[10,25,50,75,90]" \
  --multi_needle True \
  --save_results True
```

**Needle example:**
- Hidden text: "The best pizza topping is mushrooms."
- Filler: Paul Graham essays or random technical text
- Query: "What is the best pizza topping?"

**Evaluation:**
- Accuracy is binary (correct retrieval or not)
- Run multiple trials (10-50 per config)
- Plot accuracy vs. length/depth (heatmaps)
- Use included Jupyter notebook for visualization

**Testing strategy:**
1. Start small: 1K-10K tokens, single needle
2. Scale up: 10K-32K tokens, vary depth
3. Multi-needle: Test multiple fact retrieval
4. Compare: Baseline model vs. optimized versions

**Custom model integration:**
```python
# For local models, extend LLMNeedleHaystackTester class
from needlehaystack import LLMNeedleHaystackTester

class CustomModelTester(LLMNeedleHaystackTester):
    def evaluate_model(self, context: str, question: str) -> str:
        # Your model inference code here
        output = your_model.generate(context + "\n" + question)
        return output
```

**Expected baselines:**
- Top models (Gemini 2.5 Pro): Handle up to 500K tokens effectively
- Most models: Degrade at 32K+ tokens
- Hybrid RWKV/Mamba: Test degradation at 4K, 8K, 16K, 32K

**Red flags:**
- Accuracy drops below 80% at claimed context length
- Middle positions perform worse than start/end (attention bias)
- Multi-needle accuracy <50% (memory interference)

**When to test:**
- After major architectural changes
- Before/after optimization passes
- When tuning context window size
- When comparing hybrid ratios (RWKV vs Mamba dominance)

---

### Advanced Long-Context Benchmarks (Post-NIAH)

**Note:** These are "later game" optimizations - run only after NIAH tests pass and model demonstrates basic long-context capability.

#### LongBench v2 (2025): Multitask Real-World Memory

**Purpose:** Holistic memory evaluation on diverse tasks (QA, summarization, code completion) over long documents. Goes beyond simple retrieval to test deep understanding.

**Key features:**
- Bilingual (English/Chinese)
- 20+ tasks including multi-document QA, long code completion, summarization
- Tests up to 200K tokens
- Focus on cross-document reasoning

**Setup:**
```bash
# Clone the repository
git clone https://github.com/THUDM/LongBench
cd LongBench

# Install dependencies
pip install -r requirements.txt

# Download datasets (automatic on first run)
python eval.py --model your_llm --length 100000
```

**Evaluation:**
```bash
# Run on specific tasks
python eval.py \
  --model hybrid_v4_8M \
  --length 100000 \
  --tasks "multidoc_qa,code_completion,summarization"

# Full benchmark suite
python eval.py --model hybrid_v4_8M --length 200000 --all_tasks
```

**Metrics:**
- **F1 score** for QA tasks
- **ROUGE** for summarization
- **Code accuracy** for completion tasks
- Reveals if memory holds for cross-doc reasoning

**Expected performance:**
- Target: F1 >0.6 at 100K tokens
- Watch for: Sharp drops beyond certain lengths
- Compare: GPT-4 (~0.75 at 100K), Claude (~0.72 at 100K)

#### InfiniteBench (2024): Ultra-Long Context Testing

**Purpose:** Tests "infinite-like" memory at >100K tokens with complex tasks (book QA, infinite math sequences, full novel summarization).

**Key features:**
- Average 100K+ tokens per example
- Tests sustained attention and hierarchical memory
- Ideal for models with RAG/memory hierarchies

**Setup:**
```bash
# Repository and code on arXiv
# Check https://arxiv.org/abs/2402.13718 for latest instructions

# Generate long inputs
python generate_infinitebench_data.py --task book_qa --length 150000

# Run evaluation
python eval_infinitebench.py --model hybrid_v4_8M --task book_qa
```

**Tasks:**
- **Book QA:** Answer questions about full novels
- **Math sequences:** Maintain state through infinite series
- **Code repository:** Navigate entire codebases
- **Multi-session dialogue:** Track conversation across hundreds of turns

**Why test this:**
- Probes whether hybrid RWKV/Mamba architecture maintains state at extreme lengths
- Validates theoretical advantages of recurrent state vs attention
- Identifies if memory degrades gracefully or catastrophically

**Expected challenges:**
- VRAM requirements: 80GB+ for 1M token contexts (use gradient checkpointing)
- Computation time: Hours per evaluation
- May require model quantization or memory-efficient attention

#### General Metrics Across Benchmarks

**Plot accuracy vs log(context length):**
```python
import numpy as np
import matplotlib.pyplot as plt

# Example: Finding "usable" length (80% accuracy threshold)
context_lengths = [1000, 2000, 4000, 8000, 16000, 32000, 64000]
accuracies = [0.95, 0.92, 0.88, 0.85, 0.78, 0.65, 0.45]

plt.plot(np.log10(context_lengths), accuracies)
plt.axhline(y=0.8, color='r', linestyle='--', label='80% threshold')
plt.xlabel('Log10(Context Length)')
plt.ylabel('Accuracy')
plt.title('Context Length vs Accuracy')
plt.legend()

# Interpolate usable length
usable_length = np.interp(0.8, accuracies[::-1], context_lengths[::-1])
print(f"Usable context length (80% accuracy): {usable_length:.0f} tokens")
```

**Additional benchmarks to consider:**
- **MRCR (Multi-Needle):** Variant of NIAH with complex retrieval patterns
- **Fiction.liveBench:** Narrative memory and story coherence
- **Vellum AI Leaderboard:** Compare against SOTA models
- **LLM Stats:** Track long-context performance rankings

#### Testing Best Practices

**Control variables:**
- Fix `temperature=0` for deterministic results
- Average 5-10 runs per configuration
- Use same random seed across comparisons

**Hardware considerations:**
- Long contexts need substantial VRAM (80GB+ for 1M tokens)
- Consider gradient checkpointing to reduce memory
- Use mixed precision (fp16/bf16) for efficiency
- Offload to CPU if necessary (will be slow)

**Comparison strategy:**
- Benchmark against GPT-3.5/4, Claude, Gemini for context
- Use LongBench/InfiniteBench leaderboards for reference
- Document RWKV vs Mamba component contributions separately

**Limitations to note:**
- Synthetic tests may not capture all real-world drift
- Combine with custom data relevant to your use case
- Consider domain-specific benchmarks (e.g., code, legal, medical)

**Tools for evaluation:**
- **Open models:** Use Hugging Face Transformers
- **APIs:** Monitor token costs (can be expensive at 100K+)
- **Custom data:** Use Snorkel SWiM for domain-specific evaluation

#### Recommended Testing Progression

**Phase 1: Basic (Start here)**
1. NIAH test at 1K-16K tokens
2. Validate degradation points
3. Fix obvious issues

**Phase 2: Intermediate**
4. LongBench at 50K-100K tokens
5. Multi-task evaluation (QA, summarization, code)
6. Identify task-specific weaknesses

**Phase 3: Advanced (Only if Phase 1-2 pass)**
7. InfiniteBench at 100K-200K tokens
8. Ultra-long context stress testing
9. Memory hierarchy optimization

**Gating criteria:**
- Must pass NIAH (>80% at 16K) before LongBench
- Must pass LongBench (F1 >0.6 at 100K) before InfiniteBench
- Each phase requires 1-2 weeks of optimization work

**Start with NIAH for quick insights** - it's free, fast, and highly revealing. These advanced benchmarks quantify exactly how much your model truly "remembers" in production scenarios.

---

## Implementation Checklist

### Phase 1: Monitoring Setup (30 min)

- [ ] Install nvtop: `sudo apt install nvtop`
- [ ] Install powerstat: `sudo apt install powerstat`
- [ ] Test nvidia-smi logging: `nvidia-smi -l 1 > test.log` (Ctrl+C after 10s)
- [ ] Create logs directory: `mkdir -p logs/monitoring`
- [ ] Document baseline: Run current train_v4.py for 100 steps, save metrics

### Phase 2: Quick Win Optimizations (2-3 hours)

**Batch Size Experiment:**
- [ ] Test batch_size=12: Monitor VRAM with nvidia-smi
- [ ] Test batch_size=16: Monitor VRAM with nvidia-smi
- [ ] Test batch_size=24: Probably too large, but verify
- [ ] Choose optimal batch size (highest that fits in VRAM)
- [ ] Document: throughput improvement, VRAM usage

**DataLoader Optimization:**
- [ ] Add num_workers=2 to train_v4.py
- [ ] Test and measure throughput
- [ ] Try num_workers=4
- [ ] Add pin_memory=True
- [ ] Document: speedup from parallel data loading

**Mixed Precision:**
- [ ] Implement AMP in train_v4.py (see code example above)
- [ ] Run 100-step test, verify no NaN/Inf
- [ ] Run 1000-step test, compare final loss to baseline
- [ ] Document: speedup, loss convergence validation

**torch.compile:**
- [ ] Add torch.compile(model) to train_v4.py
- [ ] Run 100-step test (expect slow first steps)
- [ ] Measure steady-state throughput after compilation
- [ ] Document: speedup after warmup

### Phase 3: Profiling & Analysis (1-2 hours)

- [ ] Run torch.profiler on 10 training steps
- [ ] Export trace.json and analyze in chrome://tracing
- [ ] Identify top 3 slowest operations
- [ ] Check for GPU idle time (gaps in trace)
- [ ] Document findings: bottlenecks, optimization opportunities

**GPU Utilization Analysis:**
- [ ] Run nvtop during training
- [ ] Record GPU util % during forward pass
- [ ] Record GPU util % during backward pass
- [ ] Record GPU util % during optimizer step
- [ ] Target: 80-90%+ during forward/backward

### Phase 4: Controlled Experiments (3-4 hours)

**Run comparison matrix:**

| Config | Batch | Workers | AMP | Compile | Tok/s | VRAM | Loss |
|--------|-------|---------|-----|---------|-------|------|------|
| Baseline | 8 | 0 | No | No | 33K | 422MB | 1.37 |
| +Batch | 16 | 0 | No | No | ? | ? | ? |
| +Workers | 16 | 4 | No | No | ? | ? | ? |
| +AMP | 16 | 4 | Yes | No | ? | ? | ? |
| +Compile | 16 | 4 | Yes | Yes | ? | ? | ? |

- [ ] Run each configuration for 1000 steps
- [ ] Log all metrics
- [ ] Compare final losses (should all be within ¬±0.05)
- [ ] Choose best configuration for production training

### Phase 5: Documentation & Integration (1 hour)

- [ ] Update V4_BUILD_LOG.md with optimization results
- [ ] Update train_v4.py with best configuration as default
- [ ] Create optimization_results.md with comparison table
- [ ] Update V4_STRATEGY.md next task with findings
- [ ] Commit and push changes

---

## Expected Outcomes

**Conservative estimate (3x improvement):**
- Baseline: 33K tok/s
- With batch_size=16 + num_workers=4 + AMP: ~99K tok/s
- VRAM usage: <2GB
- Loss convergence: Similar to baseline

**Optimistic estimate (8x improvement):**
- Baseline: 33K tok/s
- All optimizations + torch.compile: ~264K tok/s
- GPU utilization: 85-90%
- Training time: 5000 steps in ~5 minutes (vs ~40 minutes baseline)

**Realistic target (5x improvement):**
- Combined optimizations: ~165K tok/s
- GPU utilization: 75-85%
- VRAM: 1.5-2GB
- Quality: No degradation

---

## Troubleshooting

### Common Issues

**VRAM Out of Memory:**
- Reduce batch_size
- Disable torch.compile (uses extra memory for compilation)
- Check for memory leaks (torch.cuda.empty_cache() between runs)

**NaN/Inf Losses with AMP:**
- Check GradScaler is properly used
- Try bf16 instead of fp16: `dtype=torch.bfloat16`
- Reduce learning rate by 10%

**No Speedup from Optimizations:**
- Check GPU utilization (nvidia-smi) - if already 95%+, at hardware limit
- Profile with torch.profiler to find actual bottleneck
- Verify optimizations are actually enabled (print statements)

**Slower with torch.compile:**
- First 5-10 steps are slow (compilation overhead)
- Measure steady-state throughput after step 20
- If still slower, may not benefit small models at this scale

---

## Next Steps After V4.5

Once optimization and analysis are complete:

1. **Scale up:** Apply optimizations to larger model (8M params)
2. **Longer training:** Run 50K+ steps with best configuration
3. **Hyperparameter tuning:** Address gradient ratio imbalance (from baseline metrics)
4. **Architecture experiments:** Test fusion mechanisms from V4_TESTING.md
5. **Production setup:** Prepare for extended training runs

**See V4_STRATEGY.md for task roadmap.**

---

*Document created: 2026-01-09*  
*Next review: After Phase 4 experiments complete*
