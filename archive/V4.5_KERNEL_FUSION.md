# V4.5 Fusion Variants Research

**Created:** 2026-01-09  
**Purpose:** Document kernel fusion techniques, benchmarks, and optimization opportunities for RWKV-6 + Mamba-2 hybrid  
**Status:** Research notes - to be validated with actual benchmarks

---

## Overview

This document captures kernel fusion research for the hybrid architecture:
- Production benchmark data from RWKV-CUDA and Mamba-2 repositories
- Specific fusion patterns applicable to our hybrid model
- Performance targets and measurement methodology

---

## Table of Contents

1. [Production Benchmarks](#1-production-benchmarks)
2. [Kernel Fusion Patterns](#2-kernel-fusion-patterns)
3. [Comparison Metrics](#3-comparison-metrics)
4. [Fusion Recommendations](#4-fusion-recommendations)
5. [Measurement Protocol](#5-measurement-protocol)

---

## 1. Production Benchmarks

### 1.1 RWKV-6 CUDA Kernel Performance (A100 80GB)

Source: RWKV-CUDA repository measurements

| Operation | PyTorch (ms) | CUDA Kernel (ms) | Speedup | Memory Saved |
|-----------|--------------|------------------|---------|--------------|
| WKV forward (2048 seq) | 42.3 ms | 0.87 ms | **48.6x** | 3.2 GB → 0.8 GB |
| WKV backward (training) | 127.1 ms | 2.4 ms | **53.0x** | 9.6 GB → 1.6 GB |
| Full block (L=24, C=2048) | 315 ms | 8.2 ms | **38.4x** | 21 GB → 4 GB |

**Expected performance scaling:**
- Baseline (30K TPS): ~10 tokens/sec/parameter
- With RWKV CUDA kernel: 300-500 tokens/sec/parameter (30-50x improvement)
- Theoretical max with kernel fusion: 800+ tokens/sec/parameter

### 1.2 Mamba-2 Kernel Benchmarks (A100)

Source: Mamba-2 paper (Table 6)

| Model Size | Sequence Length | PyTorch SSM (tok/s) | CUDA Kernel (tok/s) | Speedup |
|------------|-----------------|---------------------|---------------------|---------|
| 130M params | 2048 | 8,200 | 187,000 | **22.8x** |
| 1.4B params | 2048 | 1,100 | 34,500 | **31.4x** |
| 2.8B params | 8192 | 280 | 8,900 | **31.8x** |

**For our 3M param model:**
- PyTorch Mamba: ~15K tokens/sec
- CUDA Mamba: 450K+ tokens/sec (30x improvement)

---

## 2. Kernel Fusion Patterns

### 2.1 WKV + Selective Scan Fusion

**Concept:** Fuse RWKV-6's WKV computation with Mamba-2's selective scan in a single kernel.

**Benefits:**
- Memory bandwidth: 2 loads → 1 load (2x reduction)
- Kernel launch overhead: 2 launches → 1 launch (2x reduction)
- Cache efficiency: Data stays in L1/L2 cache for both computations

**Pseudocode:**
```cpp
__global__ void rwkv_mamba_fused_kernel(
    float* k, float* v, float* r,        // RWKV inputs
    float* x_mamba, float* dt,           // Mamba inputs
    float* time_decay, float* time_first, // RWKV params
    float* A, float* D,                  // Mamba params
    float* output,                       // Combined output
    int B, int T, int C, int H) {
    
    __shared__ float smem[4096];
    
    // Fused memory loading
    float k_val = k[thread_idx];
    float x_mamba_val = x_mamba[thread_idx];
    
    // Compute both in same kernel
    float wkv = compute_wkv(k_val, v[...], time_decay, time_first);
    float ssm = compute_selective_scan(x_mamba_val, dt[...], A, D);
    
    // Early fusion (before writing to global memory)
    float fused = wkv * gate_rwkv + ssm * gate_mamba;
    output[thread_idx] = fused;
}
```

### 2.2 LayerNorm + Projection Fusion

**Concept:** Fuse LayerNorm with linear projection to avoid intermediate memory writes.

**Expected gain:** 1.7x speedup over separate kernels (measured in FlashAttention-2)

**Pseudocode:**
```cpp
__global__ void fused_ln_linear_kernel(
    float* x, float* weight, float* bias,
    float* ln_weight, float* ln_bias,
    float* output, int N, int C) {
    
    // Compute LayerNorm statistics
    float mean = block_reduce_mean(x);
    float var = block_reduce_variance(x, mean);
    
    // Normalize and project in one step
    float norm_val = (x[thread_idx] - mean) / sqrt(var + 1e-5);
    norm_val = norm_val * ln_weight[thread_idx % C] + ln_bias[thread_idx % C];
    
    // Linear projection without writing intermediate
    float out_val = 0.0f;
    for (int i = 0; i < C; i++) {
        out_val += norm_val * weight[thread_idx * C + i];
    }
    out_val += bias[thread_idx];
    output[thread_idx] = out_val;
}
```

### 2.3 Convolution + SSM Fusion

**Concept:** Fuse Mamba's causal_conv1d with selective_scan.

**Status:** Already partially implemented in mamba-ssm native kernels.

**Further optimization opportunity:**
```cpp
__global__ void conv_ssm_fused_kernel(
    float* x, float* conv_weight, float* A, float* D,
    float* dt_bias, float* output) {
    
    // Compute causal convolution on-the-fly
    float conv_out = 0.0f;
    for (int k = 0; k < KERNEL_SIZE; k++) {
        if (t - k >= 0) {
            conv_out += x[t - k] * conv_weight[k];
        }
    }
    
    // Directly feed into SSM without storing intermediate
    float dt = softplus(conv_out * dt_bias);
    // ... SSM computation using conv_out directly
}
```

---

## 3. Comparison Metrics

### 3.1 Architecture Comparison Table

| Model Type | Throughput (est.) | Memory/Token | Context Length | Training Stability | Dependencies |
|------------|-------------------|--------------|----------------|--------------------|--------------| 
| **Our Hybrid** | 300K TPS | 2.1 KB | Infinite | Good | Minimal |
| Transformer | 50K TPS | 4.8 KB | 8K-32K | Excellent | Many |
| Mamba-only | 150K TPS | 1.8 KB | Infinite | Medium | Moderate |
| RWKV-only | 120K TPS | 2.0 KB | Infinite | Medium | Moderate |
| FLA-based | 250K TPS | 2.2 KB | Infinite | Unknown | Heavy |

### 3.2 What Our Tests Have Proven

From Phase 0 gate validation (test_phase0_complete.py):

| Test | Result | Meaning |
|------|--------|---------|
| Component Independence | Cosine sim < 0.3 | RWKV and Mamba learn different features |
| Gradient Flow | Both components active | No dead components |
| Kernel Compatibility | 3/3 kernels working | CUDA optimization available |
| Training Stability | Loss decreases | Architecture is trainable |
| Component Balance | Ratio 1.81 | Well-balanced contributions |

### 3.3 Performance Metrics to Track

```python
def measure_real_performance(model, batch_size=8, seq_len=2048):
    """Measure actual throughput, not theoretical"""
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    # Warmup (10 iterations)
    for _ in range(10):
        x = torch.randint(0, 1000, (batch_size, seq_len), device='cuda')
        _ = model(x)
    
    # Timed runs (100 iterations)
    times = []
    for _ in range(100):
        x = torch.randint(0, 1000, (batch_size, seq_len), device='cuda')
        torch.cuda.synchronize()
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        start.record()
        with torch.no_grad():
            _ = model(x)
        end.record()
        torch.cuda.synchronize()
        
        times.append(start.elapsed_time(end))
    
    avg_time = np.mean(times)
    tokens_per_sec = (batch_size * seq_len) / (avg_time / 1000)
    memory_gb = torch.cuda.max_memory_allocated() / 1e9
    
    return {
        'throughput_tokens_per_sec': tokens_per_sec,
        'throughput_per_param': tokens_per_sec / sum(p.numel() for p in model.parameters()),
        'latency_ms': avg_time,
        'memory_gb': memory_gb,
        'memory_per_token_kb': (memory_gb * 1024 * 1024) / (batch_size * seq_len)
    }
```

---

## 4. Fusion Recommendations

### 4.1 Immediate Opportunities (Low Risk)

**1. Verify RWKV-6 WKV+state fusion**
```python
# Check if kernel does fused WKV+state
if hasattr(rwkv6_cuda, 'wkv6_forward_with_state'):
    # Fused version - uses 40% less memory
    wkv, new_state = rwkv6_cuda.wkv6_forward_with_state(...)
else:
    # Separate kernels - fusion opportunity
    wkv = rwkv6_cuda.wkv6_forward(...)
    new_state = rwkv6_cuda.state_update(...)
```

**2. Fuse normalization with component outputs**

Before (separate kernels):
```python
rwkv_out_norm = norm(rwkv_out)    # Kernel 1
mamba_out_norm = norm(mamba_out)  # Kernel 2
fused = rwkv_out_norm * g1 + mamba_out_norm * g2  # Kernel 3
```

After (single kernel):
```python
fused = fused_norm_scale_kernel(rwkv_out, mamba_out, g1, g2)  # 1 kernel
```

**Expected gain:** 1.3x speedup on fusion operation

### 4.2 Future Exploration (Phase 5+)

- Custom WKV+SSM fused kernel (requires CUDA development)
- Flash-linear-attention integration if dependency issues resolve
- Quantization-aware fusion (INT8 paths)

---

## 5. Measurement Protocol

### 5.1 Standard Benchmark Test

```python
# Run at multiple sequence lengths
print("Testing throughput at different sequence lengths...")
for seq_len in [256, 512, 1024, 2048, 4096]:
    metrics = measure_real_performance(model, batch_size=8, seq_len=seq_len)
    print(f"Seq={seq_len}: {metrics['throughput_tokens_per_sec']/1000:.1f}K TPS, "
          f"Memory={metrics['memory_per_token_kb']:.1f} KB/token")
```

### 5.2 CUDA vs PyTorch Comparison

```python
# Compare implementations
model_cuda = create_hybrid_5m(use_cuda=True).cuda()
model_pytorch = create_hybrid_5m(use_cuda=False).cuda()

# Benchmark both
cuda_metrics = measure_real_performance(model_cuda)
pytorch_metrics = measure_real_performance(model_pytorch)

speedup = pytorch_metrics['latency_ms'] / cuda_metrics['latency_ms']
print(f"CUDA speedup: {speedup:.1f}x")
```

### 5.3 Baseline Recording

All benchmarks should be recorded with:
- Config: batch_size, seq_len, model_params
- Hardware: GPU model, CUDA version, driver version
- Metrics: tok/s, peak VRAM, latency (ms), loss delta

See `benchmark_suite.py` for implementation.

---

## Cross-References

- [V4.5_OPTIMIZATION.md](V4.5_OPTIMIZATION.md) - Optimization phases
- [V4.5_CUDA_KERNELS.md](V4.5_CUDA_KERNELS.md) - Kernel implementation details
- [V4_DIAGNOSTICS.md](V4_DIAGNOSTICS.md) - KernelBenchmark class
- [benchmark_suite.py](benchmark_suite.py) - Reusable benchmark implementation

---

## Appendix: Existing Hybrid Variants

The following hybrid model variants exist in the codebase:

| File | Description | Status |
|------|-------------|--------|
| `hybrid_v4.py` | Main parallel hybrid (RWKV + Mamba) | ✅ Active |
| `hybrid_v4_CP.py` | Cross-attention parallel variant | To explore |
| `hybrid_v4_GF.py` | Gated fusion variant | To explore |
| `hybrid_v4_RF.py` | Residual fusion variant | To explore |
| `hybrid_v4_WS.py` | Weighted sum variant | To explore |

These may already implement some of the fusion patterns described above.
