# V4.5 CUDA Integration ‚Äî How We Solved It

**Created:** 2026-01-09  
**Status:** ‚úÖ COMPLETE (Phase 0 ‚Äî 2026-01-09)  
**Purpose:** Historical record of CUDA kernel integration for RWKV-6 + Mamba-2 hybrid

---

## Executive Summary

**The Problem:** Native Python implementations of RWKV-6 and Mamba-2 were 30-50x slower than CUDA kernels.

**The Solution:**
- Mamba-2: Used `mamba-ssm` package with native CUDA kernels (pre-installed)
- RWKV-6: Created `rwkv6_cuda_wrapper.py` with JIT compilation from RWKV-CUDA source

**Result:** Production-speed training at ~35K tokens/sec on RTX 4050.

**Key Files:**
- `rwkv6_prototype.py` ‚Äî RWKV-6 implementation with CUDA kernel support
- `rwkv6_cuda_wrapper.py` ‚Äî JIT compilation wrapper for RWKV-CUDA kernels  
- `cuda_backends.py` ‚Äî Hybrid layer integration (imports both RWKV-6 and Mamba-2)

---

## What Was Tried (Historical)

### Option A: FLA Library ‚ùå REJECTED
- Flash-Linear-Attention library provides RWKV-6 and Mamba
- Problems: Version conflicts, incomplete RWKV-6 support, heavy dependencies
- Decision: Do not use FLA

### Option B: Python Wrappers ‚ö†Ô∏è FALLBACK ONLY
- Created simplified Python implementations in `archive/cuda_backends.py`
- Worked but ~50x slower than CUDA kernels
- Kept as fallback for debugging/testing

### Option C: CUDA Integration ‚úÖ FINAL SOLUTION
- Mamba-2: `mamba-ssm` package with native kernels
- RWKV-6: JIT compilation from RWKV-CUDA source

---

## Implementation Details

### Mamba-2 (Easy)

```python
from mamba_ssm import Mamba2
# Pre-installed with CUDA kernels, no extra work needed
```

### RWKV-6 (Required Custom Work)

Created `rwkv6_cuda_wrapper.py`:
```python
# JIT compiles RWKV-CUDA kernels on first use
# Handles head_size variants (32, 64, 128)
# Returns wkv6 function for forward pass
```

Created `rwkv6_prototype.py`:
```python
# Full RWKV-6 layer implementation
# Uses CUDA kernel via wrapper
# Matches BlinkDL/RWKV-LM specification
```

---

## Validation Results (Phase 0 Complete)

| Gate | Test | Result | Status |
|------|------|--------|--------|
| G0 | Kernel availability | causal-conv1d, selective_scan, wkv6 | ‚úÖ 3/3 |
| G1 | Forward pass | No NaN, correct shapes | ‚úÖ PASS |
| G2 | Init entropy | 5.46 (warn range) | ‚ö†Ô∏è WARN |
| G3 | Mini training | Loss 5.59‚Üí5.57, grad 1.08 | ‚úÖ PASS |
| G4 | Component balance | Ratio 1.81 | ‚úÖ PASS |

---

## Reference: CUDA Kernel Documentation

### RWKV-6 Kernel Specifics

**Kernel Source Location:**
```bash
# Clone the specific kernel implementations
git clone https://github.com/BlinkDL/RWKV-CUDA
cd RWKV-CUDA/rwkv6  # RWKV-6 specific kernels
```

**Core Kernel Files You Need:**
1. **`rwkv6_fused_forward.cu`** - Main WKV computation kernel
   - Implements parallel associative scan for WKV recurrence
   - Uses `__half2` operations for FP16 optimization
   - Expects memory layout: `[B, H, T, S]` (batch, heads, time, state)

2. **`rwkv6_fused_backward.cu`** - Gradient kernel (for training)
   - Computes gradients for backpropagation
   - Required for training, not just inference
   - Matches forward kernel's memory layout

3. **`bindings.cpp`** - PyTorch C++ bindings
   - Exposes CUDA kernels to PyTorch
   - Handles tensor conversion and device management
   - PYBIND11 interface

**PyTorch Extension Setup:**

Complete `setup.py` for building RWKV-6 CUDA extension:

```python
# setup.py for RWKV-6 CUDA extension
from setuptools import setup, Extension
from torch.utils import cpp_extension
import os

# Check CUDA availability
CUDA_HOME = os.getenv('CUDA_HOME', '/usr/local/cuda')

setup(
    name='rwkv6_cuda',
    ext_modules=[
        cpp_extension.CUDAExtension(
            'rwkv6_cuda',
            sources=[
                'src/rwkv6_fused_forward.cpp',
                'src/rwkv6_fused_forward_kernel.cu',
                'src/rwkv6_fused_backward.cpp', 
                'src/rwkv6_fused_backward_kernel.cu',
                'src/bindings.cpp'
            ],
            include_dirs=[
                f'{CUDA_HOME}/include',
                'include/',
                cpp_extension.include_paths()
            ],
            library_dirs=[f'{CUDA_HOME}/lib64'],
            libraries=['cudart'],
            extra_compile_args={
                'cxx': ['-O3', '-std=c++17', '-fopenmp'],
                'nvcc': [
                    '-O3', 
                    '-std=c++17',
                    '-U__CUDA_NO_HALF_OPERATORS__',
                    '-U__CUDA_NO_HALF_CONVERSIONS__',
                    '-U__CUDA_NO_BFLOAT16_CONVERSIONS__',
                    '--use_fast_math',
                    '--ptxas-options=-v',
                    '--gpu-architecture=sm_89',  # RTX 4050 (adjust for your GPU)
                    '-lineinfo'
                ]
            }
        )
    ],
    cmdclass={'build_ext': cpp_extension.BuildExtension}
)
```

**Build Command:**
```bash
cd /home/m_tes/groundthink/RWKV-CUDA/rwkv6
python setup.py build_ext --inplace
# Or use pip for editable install:
pip install -e .
```

**Key Configuration Notes:**
- `--gpu-architecture=sm_89` for RTX 4050 (sm_80 for A100, sm_86 for RTX 3090)
- `-U__CUDA_NO_HALF_OPERATORS__` enables FP16 operations
- `--use_fast_math` trades precision for speed (acceptable for neural nets)
- `-lineinfo` enables profiling with `nsys`
- OpenMP (`-fopenmp`) for CPU-side parallelization

**Compilation Requirements:**
```bash
# For RWKV-CUDA compilation
nvcc --version  # Need CUDA 11.8+
pip install ninja

# Clone and build RWKV-CUDA
git clone https://github.com/BlinkDL/RWKV-CUDA
cd RWKV-CUDA
python setup.py build_ext --inplace
```

---

### Kernel Compilation Troubleshooting

**Common CUDA Compilation Fixes:**

If you encounter compilation issues, try these solutions in order:

```bash
# 1. Ensure proper CUDA version
nvcc --version  # Should match PyTorch CUDA version
python -c "import torch; print(f'PyTorch CUDA: {torch.version.cuda}')"

# 2. Set correct compute capability for your GPU
export TORCH_CUDA_ARCH_LIST="8.9"  # For RTX 4050
# Other common architectures:
#   "8.0" for A100
#   "7.5" for RTX 2080 Ti
#   "8.6" for RTX 3090
#   "9.0" for H100

# 3. Use ninja for faster builds
pip install ninja

# 4. Compile with verbose output
python setup.py build_ext --inplace --verbose

# 5. If memory errors during compilation
export MAX_JOBS=4  # Limit parallel jobs (reduce if still OOM)
export CUDA_VISIBLE_DEVICES=0  # Use single GPU

# 6. Clean rebuild if previous attempt failed
python setup.py clean
rm -rf build/ *.egg-info/
python setup.py build_ext --inplace
```

**GPU Compute Capability Reference:**

| GPU Model | Compute Capability | TORCH_CUDA_ARCH_LIST |
|-----------|-------------------|----------------------|
| RTX 4050 | SM 8.9 | "8.9" |
| RTX 4090 | SM 8.9 | "8.9" |
| RTX 3090 | SM 8.6 | "8.6" |
| RTX 3080 | SM 8.6 | "8.6" |
| A100 | SM 8.0 | "8.0" |
| V100 | SM 7.0 | "7.0" |
| RTX 2080 Ti | SM 7.5 | "7.5" |
| H100 | SM 9.0 | "9.0" |

**Verify GPU Architecture:**
```bash
nvidia-smi --query-gpu=compute_cap --format=csv,noheader
# Returns: 8.9 (for RTX 4050)
```

**Common Error Messages and Solutions:**

1. **"nvcc fatal: Unsupported gpu architecture"**
   - Solution: Set correct `TORCH_CUDA_ARCH_LIST` or update `--gpu-architecture` in setup.py

2. **"undefined reference to `cudaFree`"**
   - Solution: Add `-lcudart` to linker flags, check `CUDA_HOME` points to correct CUDA installation

3. **"virtual memory exhausted: Cannot allocate memory"**
   - Solution: Reduce `MAX_JOBS`, close other programs, or add swap space

4. **"torch/extension.h: No such file or directory"**
   - Solution: Reinstall PyTorch: `pip install --force-reinstall torch`

5. **"PTX .version 7.0 does not support .target sm_89"**
   - Solution: Upgrade CUDA toolkit to 11.8+ or PTX JIT will handle it

6. **"Command '['which', 'g++-12']' returned non-zero exit status 1"**
   - PyTorch looking for specific compiler version not installed
   - Solution: Set compiler environment variables before importing torch:
   ```bash
   export CXX=/usr/bin/g++
   export CC=/usr/bin/gcc
   ```
   - Or in Python (must be before `import torch`):
   ```python
   import os
   os.environ['CXX'] = '/usr/bin/g++'
   os.environ['CC'] = '/usr/bin/gcc'
   import torch  # Now import torch
   ```

**Memory Layout:**
- Input tensors MUST be contiguous
- Preferred format: `[batch, n_heads, seq_len, head_size]`
- Use `.contiguous()` before kernel calls if needed
- State tensor shape depends on kernel implementation

---

### Mamba-2 Kernel Specifics

**Official Implementation:**
- Already available via `pip install mamba-ssm`
- Production-ready selective scan kernels
- Includes optimized causal conv1d
- ~100x faster than PyTorch sequential

**Installation:**
```bash
cd /home/m_tes/groundthink
source .venv/bin/activate
pip install mamba-ssm --no-cache-dir
pip install causal-conv1d --no-cache-dir  # Usually installed as dependency
```

**Kernel Optimization:**

Since you have `causal-conv1d` compiled, you can use explicit kernel calls for maximum performance:

```python
from mamba_ssm import Mamba2 as OfficialMamba2
from mamba_ssm.ops.selective_scan_interface import selective_scan_fn
import causal_conv1d_cuda

class OptimizedMamba2(nn.Module):
    """Mamba-2 with explicit kernel calls and memory optimization"""
    
    def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4, expand: int = 2):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(expand * d_model)
        
        # Projections
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=True)
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=True)
        
        # Conv layer
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            bias=True
        )
        
        # SSM parameters
        self.A = nn.Parameter(torch.randn(self.d_inner, d_state))
        self.D = nn.Parameter(torch.ones(self.d_inner))
        self.dt_bias = nn.Parameter(torch.randn(self.d_inner))
        
        # Initialize as per Mamba-2 paper
        nn.init.normal_(self.A, mean=0.0, std=0.02)
        nn.init.normal_(self.dt_bias, mean=0.0, std=0.02)
        
        # Ensure conv weight is initialized properly
        nn.init.kaiming_normal_(self.conv1d.weight, nonlinearity='linear')
        
        # Memory-efficient training settings
        self.use_checkpoint = True  # Gradient checkpointing for long sequences
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Optimized forward with explicit kernel calls
        
        Args:
            x: [batch, seq_len, d_model]
        
        Returns:
            [batch, seq_len, d_model]
        """
        B, T, C = x.shape
        
        # In projection
        xz = self.in_proj(x)  # [B, T, 2*d_inner]
        x, z = xz.chunk(2, dim=-1)
        
        # Conv using compiled causal-conv1d CUDA kernel
        x = x.transpose(1, 2)  # [B, d_inner, T]
        
        if hasattr(causal_conv1d_cuda, 'causal_conv1d_fwd'):
            # Use direct CUDA kernel call
            x = causal_conv1d_cuda.causal_conv1d_fwd(
                x, 
                self.conv1d.weight,
                self.conv1d.bias,
                True  # causal=True
            )
        else:
            # Fallback to PyTorch conv
            x = self.conv1d(x)
        
        x = x[:, :, :T]  # Remove extra padding
        x = x.transpose(1, 2)  # [B, T, d_inner]
        
        # Apply SiLU to z
        z = F.silu(z)
        
        # Selective SSM using official kernel
        if self.use_checkpoint and self.training and T > 1024:
            # Use gradient checkpointing for long sequences
            from torch.utils.checkpoint import checkpoint
            
            y = checkpoint(
                selective_scan_fn,
                x,  # u
                self._get_dt(z),  # delta
                self.A,
                self.D.unsqueeze(-1),
                z,  # B
                None,  # C
                self.d_state,
                True,  # delta_softplus
                True   # selective_scan
            )
        else:
            # Direct kernel call
            y = selective_scan_fn(
                x,  # u
                self._get_dt(z),  # delta
                self.A,
                self.D.unsqueeze(-1),
                z,  # B
                None,  # C
                self.d_state,
                True,  # delta_softplus
                True   # selective_scan
            )
        
        # Output projection
        out = self.out_proj(y * z)
        return out
    
    def _get_dt(self, z: torch.Tensor) -> torch.Tensor:
        """Compute delta parameter"""
        dt = z @ self.dt_bias
        dt = F.softplus(dt + 3.0)  # Shift for stability
        return dt
```

**Key Optimizations:**
1. **Explicit CUDA Kernel Calls:**
   - Direct `causal_conv1d_cuda.causal_conv1d_fwd()` for convolution
   - `selective_scan_fn()` for SSM computation
   - Bypasses Python overhead of wrapper layers

2. **Gradient Checkpointing:**
   - Enabled for sequences > 1024 tokens
   - Trades compute for memory (2x slower, 50% less VRAM)
   - Critical for RTX 4050's 6GB limit

3. **Memory Layout:**
   - Proper transpositions for conv1d (channel-first)
   - Contiguous tensors for kernel calls
   - Efficient parameter initialization

4. **Stability Improvements:**
   - `softplus(dt + 3.0)` prevents numerical issues
   - Proper initialization scales from Mamba-2 paper
   - SiLU activation for gating

**Usage:**
```python
# Simple wrapper for hybrid compatibility
class Mamba2_CUDA(nn.Module):
    """Thin wrapper around optimized Mamba-2"""
    
    def __init__(self, hidden_size, **kwargs):
        super().__init__()
        self.mamba = OptimizedMamba2(
            d_model=hidden_size,
            d_state=kwargs.get('d_state', 16),
            d_conv=kwargs.get('d_conv', 4),
            expand=kwargs.get('expand', 2),
        )
    
    def forward(self, x):
        return self.mamba(x)
```

---

### Fallback Strategy

**Production Hybrid Block with CPU/GPU Switching:**
```python
class HybridBlock_Production(nn.Module):
    """Hybrid block with CUDA kernels and CPU fallback"""
    
    def __init__(self, hidden_size):
        super().__init__()
        # CUDA implementations
        self.rwkv_cuda = RWKV6Attention_CUDA(hidden_size)
        self.mamba_cuda = Mamba2_CUDA(hidden_size)
        
        # PyTorch fallbacks
        self.rwkv_pytorch = RWKV6Attention_Prototype(hidden_size)
        self.mamba_pytorch = Mamba2_Prototype(hidden_size)
        
        # Fusion
        self.rwkv_gain = nn.Parameter(torch.ones(hidden_size) * 0.7)
        self.mamba_gain = nn.Parameter(torch.ones(hidden_size) * 0.3)
        
    def forward(self, x):
        if x.is_cuda:
            # Use CUDA path
            try:
                rwkv_out = self.rwkv_cuda(x)
                mamba_out = self.mamba_cuda(x)
            except RuntimeError as e:
                print(f"CUDA kernel failed: {e}, falling back to PyTorch")
                rwkv_out = self.rwkv_pytorch(x)
                mamba_out = self.mamba_pytorch(x)
        else:
            # CPU fallback to PyTorch
            rwkv_out = self.rwkv_pytorch(x)
            mamba_out = self.mamba_pytorch(x)
        
        # Learned fusion
        return rwkv_out * self.rwkv_gain + mamba_out * self.mamba_gain
```

**Benefits:**
- ‚úÖ Graceful degradation if CUDA fails
- ‚úÖ CPU compatibility for debugging
- ‚úÖ No code changes needed in training loop
- ‚úÖ Automatic device detection

---

### üéØ Recommended Next Steps

**Phase 1 - Immediate (2-3 hours):**
1. Deploy PyTorch prototypes to validate hybrid architecture math
2. Test forward pass and gates (G1-G2)
3. Run 100-step training test
4. Document baseline performance

**Phase 2 - Parallel Track (Week 2):**
1. Start compiling/integrating RWKV-CUDA kernels
2. Test mamba-ssm official implementation
3. Build benchmark suite with `torch.cuda.Event()`
4. Create performance comparison table

**Phase 3 - Optimization (Week 3):**
1. Profile with `nsys` to find bottlenecks
2. Optimize memory layouts
3. Add gradient checkpointing for long sequences
4. Test mixed precision training

**Success Criteria:**
- ‚úÖ Prototypes establish correctness (no NaN, stable gradients)
- ‚úÖ CUDA kernels achieve >10x speedup
- ‚úÖ GPU utilization >80%
- ‚úÖ Training completes without OOM on RTX 4050

**Kernel Compatibility Check:**
Before deployment, verify kernels are available using the compatibility check function in [V4_TESTING.md - G0: Kernel Compatibility Check](V4_TESTING.md#kernel-compatibility-check-g0---prerequisites).

---

## Context

**Dual-Path Strategy:**
- **Path B (Current)**: PyTorch prototypes for validation (~5-10K tok/sec)
- **Path A (Future)**: CUDA/Triton kernels for production (~100K+ tok/sec)

**When to Pursue:**
- After prototype wrappers validate hybrid fusion mechanism
- After architecture proves effective in training
- When performance becomes the bottleneck (>10K training steps)

---

## Performance Gap Analysis

### PyTorch Sequential vs CUDA Kernels

**RWKV-6 WKV Computation:**
- PyTorch sequential: O(B*T) with Python loops
- CUDA parallel scan: O(log T) with parallel reduction
- **Expected speedup:** 10-50x for long sequences

**Mamba-2 Selective Scan:**
- PyTorch sequential: O(T) with Python loops  
- Hardware-aware parallel scan: O(log T) with associative operators
- **Expected speedup:** 10-100x for long sequences

**Overall Impact:**
- Prototype: ~5-10K tokens/sec
- CUDA kernels: ~100-300K tokens/sec
- **Target speedup:** 10-30x

---

## RWKV-6 CUDA Kernel Requirements

### WKV Kernel (Core Recurrence)

**Mathematical Operation:**
```
For each timestep t:
  state_t = exp(-w) * state_{t-1} + exp(k_t) * v_t
  output_t = exp(r_t) * state_t
```

**Kernel Specifications:**
- **Algorithm:** Parallel associative scan
- **Data layout:** [batch, n_heads, seq_len, head_size]
- **Memory pattern:** Fused multiply-add with exponentials
- **Precision:** FP32 or mixed precision FP16/FP32
- **Occupancy target:** >75% SM utilization

**Implementation Options:**

1. **Triton Kernel** (Recommended for prototyping)
   - Python-like syntax
   - Auto-tuning for different shapes
   - Good debugging experience
   - Performance: 80-90% of hand-written CUDA

2. **CUDA C++ Kernel** (Production)
   - Maximum performance
   - Fine-grained memory control
   - Harder to debug and maintain
   - Performance: 100% optimized

3. **Use Existing Implementation**
   - FLA library: `fla/ops/rwkv6/recurrent_fuse.py`
   - RWKV-LM official: `cuda/wkv6_cuda.cu`
   - Trade-off: Dependency vs development time

**References:**
- RWKV official CUDA kernels: https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v5/cuda
- Triton tutorials: https://triton-lang.org/main/getting-started/tutorials/index.html

---

## Mamba-2 CUDA Kernel Requirements

### Selective Scan Kernel (Core SSM)

**Mathematical Operation:**
```
For each timestep t:
  dt_t = softplus(x_t @ dt_proj)
  A_discrete = exp(A * dt_t)
  B_t = x_t @ B_proj
  
  state_t = A_discrete * state_{t-1} + B_t * x_t
  output_t = C @ state_t + D * x_t
```

**Kernel Specifications:**
- **Algorithm:** Hardware-aware parallel scan
- **Data layout:** [batch, d_inner, seq_len]
- **Memory pattern:** Fused discretization + scan
- **Precision:** Mixed FP16/FP32 for stability
- **Occupancy target:** >80% SM utilization

**Additional Kernels Needed:**

1. **Causal Conv1d Kernel**
   - Depthwise convolution with causal masking
   - Kernel size typically 3-4
   - Already implemented: `causal-conv1d` package

2. **Selective State Update Kernel**
   - Fused A/B/C matrix operations
   - Input-dependent discretization
   - Already implemented: `mamba-ssm` package

**Implementation Options:**

1. **Use mamba-ssm Package** (Recommended)
   - Install: `pip install mamba-ssm`
   - Mature, tested implementation
   - Includes all necessary kernels
   - Trade-off: Dependency + installation complexity

2. **Use causal-conv1d + Custom Scan**
   - Install: `pip install causal-conv1d`
   - Implement selective scan in Triton
   - Moderate development effort

3. **Full Custom Implementation**
   - Implement all kernels from scratch
   - Maximum control and learning
   - High development cost (weeks)

**References:**
- Mamba official repo: https://github.com/state-spaces/mamba
- Causal conv1d: https://github.com/Dao-AILab/causal-conv1d
- Parallel scan algorithms: https://en.wikipedia.org/wiki/Prefix_sum#Parallel_algorithms

---

## Installation Challenges (Linux Native)

### Current Environment
- OS: Ubuntu (native Linux)
- CUDA: 12.4
- GPU: RTX 4050 (sm_89, 6GB VRAM)
- Python: 3.x in .venv

### Package Installation History

**Previous Attempts (from context):**
- ‚ùå Windows: causal-conv1d and mamba-ssm failed (MSVC incompatibility)
- ‚úÖ Linux migration: Environment should support CUDA packages now

**Known Requirements:**
```bash
# CUDA toolkit and compiler
nvcc --version  # Should show CUDA 12.4

# Build tools
sudo apt install build-essential

# Python dev headers
sudo apt install python3-dev

# Ninja build system (faster compilation)
pip install ninja
```

### Installation Commands (To Test)

**Option 1: Install mamba-ssm (includes selective scan kernels)**
```bash
cd /home/m_tes/groundthink
source .venv/bin/activate

# Install dependencies first
pip install packaging ninja

# Try installing mamba-ssm
pip install mamba-ssm --no-cache-dir

# Verify installation
python -c "from mamba_ssm import Mamba; print('mamba-ssm OK')"
```

**Option 2: Install causal-conv1d only**
```bash
# Lighter weight, just for conv kernels
pip install causal-conv1d --no-cache-dir

# Verify
python -c "from causal_conv1d import causal_conv1d_fn; print('causal-conv1d OK')"
```

**Potential Issues:**
- GCC version compatibility (need 7.x - 11.x for CUDA 12.4)
- Architecture flag: `-gencode arch=compute_89,code=sm_89` for RTX 4050
- C++11 ABI compatibility: May need `-D_GLIBCXX_USE_CXX11_ABI=1`

---

## PRODUCTION PATH (A): CUDA Kernel Integration Blueprint

**Target:** 30K+ tokens/sec with native CUDA kernels

---

### 1. RWKV-6 WKV Kernel Integration

#### Step 1: Get Official Kernels

```bash
git clone https://github.com/BlinkDL/RWKV-CUDA
cd RWKV-CUDA
# Check for RWKV-6 compatible kernels - likely in rwkv6/ directory
```

#### Step 2: PyTorch C++ Extension Wrapper

```cpp
// wkv6_cuda.cpp - Template for integrating RWKV-6 CUDA kernel
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

torch::Tensor wkv6_forward_cuda(
    torch::Tensor k, torch::Tensor v,
    torch::Tensor time_decay, torch::Tensor time_first) {
    
    // This is where you'd call the actual CUDA kernel
    // From RWKV-CUDA: rwkv6_fused_forward_kernel<<<blocks, threads>>>(
    //     k.data_ptr<float>(), v.data_ptr<float>(), ...);
    
    auto output = torch::zeros_like(v);
    // kernel launch here
    
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("wkv6_forward", &wkv6_forward_cuda, "RWKV-6 WKV forward (CUDA)");
}
```

#### Step 3: Hybrid Wrapper Class

```python
class RWKV6Attention_CUDA(nn.Module):
    """Production RWKV-6 with CUDA kernels"""
    
    def __init__(self, hidden_size):
        super().__init__()
        # Same parameters as prototype
        
        # Load CUDA extension
        try:
            import wkv6_cuda
            self.wkv6_func = wkv6_cuda.wkv6_forward
        except ImportError:
            print("CUDA kernel not found, falling back to PyTorch")
            self.wkv6_func = None
    
    def forward(self, x, state=None):
        if self.wkv6_func and x.is_cuda:
            # Use CUDA kernel
            B, T, C = x.shape
            k, v, r = self._project(x)  # Get projections
            
            # Reshape for CUDA kernel (depends on kernel interface)
            wkv = self.wkv6_func(k, v, self.time_decay, self.time_first)
            # ... rest of forward pass
        else:
            # Fallback to PyTorch
            return super().forward(x, state)
```

---

### 2. Mamba-2 Kernel Utilization

**‚úÖ STATUS: COMPLETE (2026-01-09)**

mamba-ssm 2.2.6 is installed and verified working with CUDA kernels:
- `selective_scan_fn` - Core SSM computation
- `causal_conv1d_cuda` - Optimized convolution
- `Mamba2` - Full implementation with ~100x speedup

**Verified working:**
```bash
python -c "from mamba_ssm import Mamba2; import torch; \
  m=Mamba2(d_model=256).cuda(); x=torch.randn(2,64,256).cuda(); \
  print(m(x).shape)"  # torch.Size([2, 64, 256])
```

**Wrapper for hybrid_v4.py:**
```python
from mamba_ssm import Mamba2 as OfficialMamba2

class Mamba2_CUDA(nn.Module):
    """Thin wrapper around official Mamba-2 with CUDA kernels"""
    
    def __init__(self, hidden_size, **kwargs):
        super().__init__()
        self.mamba = OfficialMamba2(
            d_model=hidden_size,
            d_state=kwargs.get('d_state', 16),
            d_conv=kwargs.get('d_conv', 4),
            expand=kwargs.get('expand', 2),
        )
    
    def forward(self, x):
        return self.mamba(x)  # Already uses CUDA kernels
```

**No further Mamba-2 work needed.** Focus remaining effort on RWKV-6 kernel integration.

---

## üî¨ Validation Protocol

### Phase 1: RWKV-6 Prototype + Testing (Current)

**Status: IN PROGRESS**

**Mamba-2:** ‚úÖ COMPLETE - using official mamba-ssm CUDA kernels
**RWKV-6:** Deploy prototype, then build CUDA wrapper

**Deliverables:**
- ‚úÖ `mamba-ssm` installed and verified working
- ‚¨ú `rwkv6_prototype.py` deployed and tested
- ‚¨ú Forward pass test passed (G1 gate)
- ‚¨ú Init entropy test (G2 gate)
- ‚¨ú Hybrid block integration complete

**Timeline:** 2-3 hours

---

### Phase 2: RWKV-6 CUDA Integration (Next)

**Track A - RWKV-CUDA Integration:**
- ‚úÖ Clone RWKV-CUDA repository (done: `/home/m_tes/groundthink/RWKV-CUDA/wkv6/`)
- ‚¨ú Study kernel interface (wkv6_op.cpp, wkv6_cuda_v1.cu)
- ‚¨ú Create PyTorch C++ extension wrapper
- ‚¨ú Write RWKV6Attention_CUDA class with fallback
- ‚¨ú Test on small inputs
- ‚¨ú Benchmark vs prototype

**Track B - Mamba-2:** ‚úÖ COMPLETE (no further work needed)

**Track C - Benchmarking Suite:**
- ‚¨ú Create performance test script
- ‚¨ú Measure tokens/sec at different seq_len
- ‚¨ú Measure GPU utilization
- ‚¨ú Compare RWKV-6 prototype vs CUDA

**Timeline:** 1-2 hours after Phase 1

---

### Phase 3: Performance Benchmarking

**Comprehensive kernel validation with benchmarking suite.**

**Benchmark Implementation:** See [V4_DIAGNOSTICS.md - Section III: Performance Benchmarking Suite](V4_DIAGNOSTICS.md#iii-performance-benchmarking-suite) for complete `KernelBenchmark` class and usage examples.

**Compare throughput across implementations:**

| Implementation | Seq Len | Tokens/Sec | GPU Util | VRAM | Notes |
|----------------|---------|------------|----------|------|-------|
| PyTorch Prototype | 256 | ~5-10K | 30-40% | 500MB | Baseline |
| PyTorch Prototype | 1024 | ~2-5K | 20-30% | 800MB | Sequential bottleneck |
| Mamba-2 Official | 256 | ~100-200K | 80-90% | 600MB | CUDA kernels |
| Mamba-2 Official | 1024 | ~80-150K | 85-95% | 1.2GB | Scales well |
| RWKV-6 Custom CUDA | 256 | TBD | TBD | TBD | After integration |
| RWKV-6 Custom CUDA | 1024 | TBD | TBD | TBD | Target: 50x speedup |
| Full CUDA Hybrid | 256 | **Target: 100-300K** | **85%+** | **<1GB** | **Production goal** |

**Success Criteria:**
- ‚úÖ Prototype establishes correctness baseline
- ‚úÖ Mamba-2 official kernel achieves >50K tok/sec
- ‚úÖ RWKV-6 CUDA kernel achieves >30K tok/sec
- ‚úÖ Combined hybrid achieves >100K tok/sec
- ‚úÖ GPU utilization >80% during compute

---

## ‚ö° Performance Target Analysis

### Current Baseline
- **Achieved:** 33K tokens/sec at 3.8M params (from V4 first training run)
- **Metric:** ~8.7 tokens/sec/parameter
- **Implementation:** Unknown (likely Triton fallback or mixed)

### Performance Breakdown by Component

**RWKV-6 Analysis:**
- PyTorch sequential WKV: ~5-10K tokens/sec
- Expected with CUDA kernel: ~250-500K tokens/sec
- **Speedup potential:** 50x

**Mamba-2 Analysis:**
- PyTorch sequential SSM: ~3-8K tokens/sec
- Official mamba-ssm kernels: ~300-800K tokens/sec
- **Speedup potential:** 100x

**Hybrid Model Combined:**
- Both components run in parallel
- Throughput limited by slower component
- With CUDA on both: ~200-400K tokens/sec
- **Target:** 100-300 tokens/sec/parameter (10-30x improvement)

### Bottleneck Identification

**Current bottleneck (prototypes):**
1. Sequential Python loops in `_wkv_sequential` and `_selective_scan`
2. Poor GPU utilization (30-40% expected)
3. Memory bandwidth underutilized

**After CUDA kernels:**
1. Memory bandwidth becomes bottleneck
2. GPU utilization should be 80-90%
3. May need to optimize data layout

### RTX 4050 Hardware Limits

**Specifications:**
- 6GB VRAM (constraint for large batches/sequences)
- 115W TDP (thermal limit for sustained compute)
- SM count: 20 (vs 128 for A100)
- Memory bandwidth: ~192 GB/s (vs 1.5 TB/s for A100)

**Expected Performance at Scale:**
- 3.8M model @ batch=8, seq=256: ~150-250K tok/sec (achievable)
- 8M model @ batch=8, seq=256: ~100-150K tok/sec (achievable)
- 8M model @ batch=16, seq=1024: ~30-50K tok/sec (VRAM limited)

**Optimization Strategy:**
- Prioritize memory efficiency over raw compute
- Use gradient checkpointing for longer sequences
- Keep batch size modest (8-16)
- Focus on 256-512 token sequences for training

---

## Implementation Details

### Memory Layout Requirements

**RWKV-6 WKV Kernel:**
- Input tensors must be contiguous in memory
- Preferred layout: `[batch, n_heads, seq_len, head_size]`
- May need `.contiguous()` before kernel call
- State tensor format depends on kernel implementation

**Mamba-2 Selective Scan:**
- Official kernel expects: `[batch, seq_len, d_model]`
- Internal reshaping to `[batch, d_inner, seq_len]`
- Conv1d requires channel-first format
- Automatic conversion in official implementation

### CUDA Kernel Error Handling

```python
def safe_cuda_call(func, *args, fallback=None, **kwargs):
    """Wrapper for CUDA kernel calls with fallback"""
    try:
        if not args[0].is_cuda:
            return fallback(*args, **kwargs) if fallback else None
        return func(*args, **kwargs)
    except RuntimeError as e:
        if "CUDA" in str(e):
            print(f"CUDA kernel failed: {e}, using fallback")
            return fallback(*args, **kwargs) if fallback else None
        raise
```

### Gradient Checkpointing for Long Sequences

```python
from torch.utils.checkpoint import checkpoint

class ParallelHybridBlock(nn.Module):
    def forward(self, x, use_checkpoint=False):
        if use_checkpoint and self.training:
            return checkpoint(self._forward_impl, x)
        return self._forward_impl(x)
```

### Mixed Precision Considerations

- RWKV-6 time_decay must stay in FP32 (numerical stability)
- Mamba-2 discretization benefits from FP32
- Activations can use FP16
- Consider using `torch.cuda.amp` for automatic mixed precision

---

## Triton Kernel Development Path

### Why Triton?
- Python-like syntax (easier than CUDA C++)
- Auto-tuning for optimal tile sizes
- Good performance (80-90% of hand-tuned CUDA)
- Faster iteration cycle

### RWKV-6 WKV in Triton (Outline)

**Parallel Scan Algorithm:**
```python
import triton
import triton.language as tl

@triton.jit
def wkv_fwd_kernel(
    k_ptr, v_ptr, w_ptr, u_ptr,  # Input pointers
    out_ptr,                      # Output pointer
    B, H, T, S,                   # Dimensions
    BLOCK_SIZE: tl.constexpr,     # Tuning parameter
):
    # Get program IDs
    batch_idx = tl.program_id(0)
    head_idx = tl.program_id(1)
    
    # Parallel associative scan over sequence
    # TODO: Implement parallel reduction
    pass
```

**Development Steps:**
1. Implement naive sequential version in Triton
2. Add parallel reduction (log T complexity)
3. Optimize memory access patterns
4. Tune block sizes for RTX 4050
5. Benchmark against PyTorch baseline

### Mamba-2 Selective Scan in Triton (Outline)

**Selective Scan Algorithm:**
```python
@triton.jit
def selective_scan_fwd_kernel(
    x_ptr, z_ptr, dt_ptr, A_ptr,  # Input pointers
    out_ptr, state_ptr,            # Output pointers
    B, D, T, N,                    # Dimensions
    BLOCK_SIZE: tl.constexpr,
):
    # Discretization + parallel scan
    # TODO: Implement fused ops
    pass
```

**Development Steps:**
1. Start with conv1d kernel (simpler)
2. Implement sequential scan with discretization
3. Add parallelization (associative scan)
4. Fuse operations to reduce memory bandwidth
5. Profile and optimize

---

## Integration Strategy

### Phase 1: Validate with Prototypes (Current)
- Use PyTorch sequential implementations
- Test hybrid fusion mechanism
- Gather architecture validation data
- Timeline: Days

### Phase 2: Hybrid Approach (Intermediate)
- Keep RWKV-6 prototype (simpler)
- Install mamba-ssm for Mamba-2 (bigger bottleneck)
- Get partial speedup with minimal work
- Timeline: Hours

### Phase 3: Triton Kernels (Advanced)
- Implement WKV in Triton
- Implement selective scan in Triton
- Full control, good performance
- Timeline: 1-2 weeks

### Phase 4: Production CUDA (If Needed)
- Hand-optimized CUDA C++ kernels
- Maximum performance
- Only if Triton insufficient
- Timeline: Weeks

---

## Decision Points

**When to Move to CUDA Kernels:**
1. ‚úÖ Prototype validates hybrid fusion works
2. ‚úÖ Training shows promising results (loss decreasing)
3. ‚úÖ Performance becomes bottleneck (>10K steps needed)
4. ‚ùå Still in architecture exploration phase

**Which Path to Choose:**
- **Fast validation needed:** Stick with prototypes
- **Partial speedup acceptable:** Install mamba-ssm only
- **Learning + control important:** Develop Triton kernels
- **Maximum performance needed:** Hand-tune CUDA C++

**Recommended Next Steps:**
1. Complete Task 6.6: Deploy prototype wrappers
2. Run training experiments with prototypes
3. Measure actual performance bottleneck
4. Decide: mamba-ssm package vs custom Triton vs full CUDA

---

## Resources

### Papers
- RWKV: "RWKV: Reinventing RNNs for the Transformer Era"
- Mamba-2: "Transformers are SSMs: Generalized Models and Efficient Algorithms"
- Parallel Scan: "Parallel Prefix Sum (Scan) with CUDA" (Harris et al.)

### Code References
- RWKV official: https://github.com/BlinkDL/RWKV-LM
- Mamba official: https://github.com/state-spaces/mamba
- FLA library: https://github.com/sustcsonglin/flash-linear-attention
- Triton tutorials: https://triton-lang.org/main/getting-started/tutorials/

### Triton Examples
- Matrix multiplication: `triton/python/tutorials/03-matrix-multiplication.py`
- Fused softmax: `triton/python/tutorials/02-fused-softmax.py`
- Custom kernels: `triton/python/tutorials/06-fused-attention.py`

---

## Status

**Current Path:** PyTorch Prototypes (Path B)
- ‚úÖ RWKV-6 prototype documented
- ‚úÖ Mamba-2 prototype documented
- ‚¨ú Deployment pending (Task 6.6)

**Future Path:** CUDA Kernels (Path A)
- ‚¨ú Pending prototype validation
- ‚¨ú Performance profiling needed
- ‚¨ú Decision: package vs Triton vs CUDA

**Next Review:** After 10K training steps with prototypes
