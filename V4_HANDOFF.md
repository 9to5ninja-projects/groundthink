# V4 Agent Handoff Document

**Purpose:** Continuity snapshot (version & task status only)  
**Current Version:** 4.11-Alpha (Phase 4.0 â€” BPE Re-Validation)  
**Updated:** 2026-01-10 (End of Day)  
**Last Agent Action:** Task 40 complete, strategic reframe â€” BPE is correct baseline, not a fix. Created Phase 4.0.  
**Repository:** https://github.com/9to5ninja-projects/groundthink  
**Git Status:** Modified (V4_STRATEGY.md, V4_HANDOFF.md updated)

---

## ğŸš¨ CRITICAL REFRAME: Read This First

**Previous Understanding (INCORRECT):**
> "BPE tokenization fixes component balance. Use BPE and the problem is solved."

**Corrected Understanding:**
> "Char-level tokenization was a shortcut for quick sanity checks. BPE is the CORRECT BASELINE we should have used from the start. All Phase 3.6-3.8 fusion comparisons were done on char-level and are NOT verified for production. The component balance problem is NOT solved."

**Why This Matters:**
- Task 40 completed with R/M ratio 0.21 â€” at the lower bound of acceptable
- Activation variance ratio: 71x (RWKV var=8.58, Mamba var=0.12) â€” **severe imbalance**
- BPE improved R/M 2x vs char-level (0.21 vs 0.08-0.11) but did NOT fix the problem
- All fusion variant rankings (GF-MH > GF > CP > HGF > HY) are char-level data â€” unverified

---

## ğŸ“‹ SESSION SUMMARY (Jan 10 End of Day)

**What was accomplished:**
1. âœ… Task 40 completed â€” 5000 steps, BPE tokenization, GF-MH model
2. âœ… Strategic reframe â€” Recognized char-level was shortcut, BPE is correct baseline
3. âœ… Created Phase 4.0 â€” BPE Re-Validation phase with 7 tasks
4. âœ… Updated V4_STRATEGY.md â€” Marked Phase 3.6-3.8 as CHAR-LEVEL ONLY

**Key Finding:**
BPE did NOT fix component balance as hypothesized. R/M improved from 0.08-0.11 to 0.21, but activation variance (71x) shows Mamba is still severely underutilized.

---

## ğŸ“Š TASK 40 FINAL RESULTS

**Status:** âœ… COMPLETE  
**Log:** `logs/task40_bpe_run.log`  
**Checkpoints:** `checkpoints/ckpt_GF-MH_step5000.pt`, `checkpoints/ckpt_GF-MH_final.pt`

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| R/M Ratio | 0.21 | 0.20-0.46 | âš ï¸ At lower bound |
| Activation Variance Ratio | 71x | <10x ideal | âŒ Severe imbalance |
| Train Loss | 4.92 | Decreasing | âœ… |
| Val Loss | 6.22 | â€” | Higher than char-level |
| Throughput | ~31K tok/s | â€” | âœ… Stable |

**Interpretation:**
- Gradient ratio (R/M 0.21) barely meets threshold
- But activation variance (71x) shows Mamba output is 71x weaker than RWKV
- This is NOT a healthy hybrid â€” RWKV still dominates

---

## ğŸ¯ CURRENT PHASE: 4.0 â€” BPE Re-Validation

**Objective:** Verify state space fundamentals and Tiny model graduation criteria with BPE before proceeding to diagnostics or scaling.

**Rationale:** All Phase 3.6-3.8 experiments used char-level tokenization. More critically, we never verified that the state machinery actually works. State monitoring should be our first priority.

**Key Learning from Task 40:**
- Activation variance ratio: 71x (RWKV var=8.58, Mamba var=0.12)
- Gradient ratio: 0.21 (at lower bound of acceptable)
- **Conclusion:** State machinery may not be functioning as designed

### Phase 4.0 Task List (Revised Priority Order)

| Order | # | Task | Status | Details |
|-------|---|------|--------|---------|
| **1** | 41a | Implement state extraction API | â¬œ **BLOCKER** | Add `return_states` to model forward() |
| **2** | 42 | Run S0-S4 state space tests | â¬œ TODO | Verify state machinery works |
| **3** | 41 | Create test_tiny_graduation.py | â¬œ TODO | Include S0-S4 + G1-G4 |
| 4 | 43 | Run Tiny overfit test (BPE) | â¬œ TODO | 10-100 samples, loss â†’ 0 |
| 5 | 44 | Run Tiny naive baseline (BPE) | â¬œ TODO | Val loss < random |
| 6 | 45 | Run G1-G4 gates (BPE) | â¬œ TODO | Re-validate with BPE |
| 7 | 46 | Checkpoint/resume test | â¬œ TODO | Save + reload works |
| 8 | 47 | Fusion variant re-ranking | â¬œ TODO | 1K steps each with BPE |
| 9 | 48 | Component balance investigation | â¬œ TODO | Why 71x variance? |

### State Space Tests (S0-S4) â€” NEW PRIORITY

| Test | Purpose | Pass Criteria |
|------|---------|---------------|
| S0 | State shapes exist | Correct dimensions |
| S1 | Initialization health | Norm 0.01-100, no NaN |
| S2 | State evolution | Different inputs â†’ different states |
| S3 | State determinism | Same input â†’ same state |
| S4 | Component contribution | Variance ratio <100x |

**See [CANARY_TESTS.md](CANARY_TESTS.md#s0-s4-state-space-fundamentals-35m-only--required-first) for implementations.**

### Tiny Graduation Criteria (per SCALING_MILESTONES.md)

| Test | Criteria | Status |
|------|----------|--------|
| **S0-S4 state tests** | State machinery verified | â“ **API missing** |
| Overfit 10-100 samples | Loss â†’ near 0 | â“ Not tested |
| Val < naive baseline | Better than random | â“ Not tested |
| G1-G4 gates pass | Per V4_TESTING.md | â“ Not tested with BPE |
| Checkpoint/resume | Save + reload works | â“ Not tested with BPE |
| Component balance | Documented | âš ï¸ 71x variance (severe) |

**Gate:** Phase 4.0 PASS when S0-S4 pass AND all graduation criteria verified with BPE.

---

## âš ï¸ FOR NEXT AGENT

**Priority 1: Implement State Extraction API (Task 41a â€” BLOCKER)**

The model currently has no way to return internal states. Implement:

```python
# In models/hybrid_v4_GF.py
def forward(self, x, return_states=False):
    # ... existing forward logic ...
    
    if return_states:
        return output, {
            'rwkv_state': rwkv_hidden,    # Internal RWKV state
            'mamba_state': mamba_hidden,  # Internal Mamba state  
            'gate_values': gate_output    # Fusion gate values
        }
    return output
```

**Location:** [models/hybrid_v4_GF.py](models/hybrid_v4_GF.py)  
**Impact:** Blocks ALL state monitoring (S0-S4, State Tracing, diagnostics)

**Priority 2: Run S0-S4 State Space Tests (Task 42)**

Once state extraction works, verify state machinery:
```bash
python tests/test_tiny_graduation.py --test-states --tokenizer bpe
```

**Priority 3: Create test_tiny_graduation.py (Task 41)**

Combine all tests:
- S0-S4 state space fundamentals
- G1-G4 validation gates
- Overfit test
- Naive baseline test
- Checkpoint/resume test

**Priority 4: Investigate Component Balance (Task 48)**

The 71x activation variance ratio is concerning:
- RWKV var=8.58, Mamba var=0.12
- Is this architectural or fixable?
- Consider: gate_init, mamba_lr_mult, architectural changes

---

## ğŸš¨ REMAINING BLOCKERS

### Blocker 1: State Extraction API â€” CRITICAL (Task 41a)
- **Location:** [models/hybrid_v4_GF.py](models/hybrid_v4_GF.py)
- **Problem:** No `return_states=True` parameter to get internal states
- **Impact:** Cannot run S0-S4 state tests, State Tracing Module, or diagnostics
- **Fix:** Add return_states parameter that returns RWKV state, Mamba state, gate values
- **Status:** â¬œ **BLOCKER â€” FIX FIRST**

### Blocker 2: Hidden State Extraction â€” SUPERSEDED
- **Status:** Merged into Blocker 1 (same issue, different framing)

### Blocker 3: Component Balance (71x variance)
- **Problem:** Activation variance ratio 71x between RWKV and Mamba
- **Impact:** Mamba may not be contributing meaningfully to model output
- **Investigation:** Task 48 â€” need state monitoring to understand root cause

---

## ğŸ“ Current Status Summary

**Phase:** 4.0 BPE RE-VALIDATION  
**Last Action:** Task 40 complete, strategic reframe, added S0-S4 state tests  
**Next Action:** Task 41a â€” Implement state extraction API (BLOCKER)

**Phase 3.6-3.8 Status:** âš ï¸ CHAR-LEVEL ONLY â€” Results unverified for production

**Checkpoint Files:**
- `checkpoints/ckpt_GF-MH_step5000.pt` â€” Task 40 (BPE, 5K steps)
- `checkpoints/ckpt_GF-MH_final.pt` â€” Task 40 final

**Data Available:**
- `data/fineweb_5m.txt` â€” BPE training data (5M bytes)
- `data/shakespeare.txt` â€” Char-level reference only

---

## ğŸ“ Project Structure

```
groundthink/
â”œâ”€â”€ train_v4.py                  # Main training entry point
â”œâ”€â”€ models/                      # Model registry
â”‚   â”œâ”€â”€ __init__.py              # get_model('GF-MH'), list_models()
â”‚   â”œâ”€â”€ hybrid_v4*.py            # Variants (HY, GF, WS, RF, CP, etc.)
â”œâ”€â”€ data/                        # Data loading
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ tokenizer.py             # BPE via --tokenizer bpe
â”‚   â”œâ”€â”€ fineweb_5m.txt           # BPE training data
â”‚   â””â”€â”€ shakespeare.txt          # Char-level reference ONLY
â”œâ”€â”€ configs/                     # Training YAML configs
â”œâ”€â”€ checkpoints/                 # Model weights (gitignored)
â”œâ”€â”€ tests/                       # Test suite
â”‚   â””â”€â”€ test_tiny_graduation.py  # TODO: Create this
â”œâ”€â”€ logs/                        # Training logs
â”‚   â””â”€â”€ task40_bpe_run.log       # Task 40 complete log
â””â”€â”€ docs (*.md files)            # Strategy & reference
```

**Key Docs:**
- [V4_STRATEGY.md](V4_STRATEGY.md) â€” Task backlog (see Phase 4.0 for current tasks)
- [SCALING_MILESTONES.md](SCALING_MILESTONES.md) â€” Graduation criteria per model size
- [V4_TESTING.md](V4_TESTING.md) â€” G1-G4 gate definitions
- [VALIDATION_ROADMAP.md](VALIDATION_ROADMAP.md) â€” Week 1-3 plan (after Phase 4.0)

---

## ğŸ”‘ Critical Institutional Knowledge

### The Tokenization Lesson (Critical)

**What We Learned:**
- Char-level tokenization was used for quick iteration during Phases 3.6-3.8
- This was appropriate for infrastructure validation but NOT for architecture evaluation
- BPE is the correct baseline for production models
- All fusion variant rankings from Phase 3.6-3.7 are char-level data and need re-validation

**What BPE Actually Showed (Task 40):**
- R/M ratio improved: 0.08-0.11 (char) â†’ 0.21 (BPE) â€” **2x improvement**
- But activation variance: 71x â€” **still severely imbalanced**
- Conclusion: BPE helps but does NOT solve component balance

### Scaling Philosophy (Foundation)

Each parameter scale is an **experimental regime with distinct objectives**:
- **3.5M:** Sanity check â€” does training system work? â† **WE ARE HERE (Phase 4.0)**
- **8M:** Proof of concept â€” does architecture learn real patterns?
- **30M:** Scaling laws â€” do predictions hold?
- **125M:** MVP delivery â€” is this production-ready?

**Current Gate:** Phase 4.0 validates 3.5M criteria with BPE before proceeding.

### Component Balance Problem (Open)

| Metric | Char-Level | BPE | Target | Status |
|--------|------------|-----|--------|--------|
| R/M Gradient Ratio | 0.08-0.11 | 0.21 | 0.3-3.0 | âš ï¸ Improved but low |
| Activation Variance | ~100x | 71x | <10x | âŒ Still severe |

**Possible Causes:**
1. Architectural â€” RWKV inherently dominates in hybrid fusion
2. Hyperparameter â€” gate_init, mamba_lr_mult need tuning
3. Data â€” FineWeb sample may favor RWKV patterns
4. Expected â€” Maybe 71x is acceptable at 3.5M scale

**Investigation:** Task 47 in Phase 4.0

---

## âš ï¸ Known Issues & Decisions

**Issue 1: Phase 3.6-3.8 Data Validity**
- All experiments used char-level tokenization
- Fusion rankings (GF-MH > GF > CP > HGF > HY) are unverified
- **Action:** Re-validate with BPE in Task 46

**Issue 2: Component Balance**
- 71x activation variance is severe
- R/M 0.21 is at threshold boundary
- **Action:** Investigate in Task 47

**Issue 3: Tasks 37-38 Status**
- Previously marked "DEPRECATED â€” BPE fixes balance"
- Now marked "REQUIRES RE-EVALUATION" since BPE didn't fully fix it
- May need to revisit differential warmup or regularization

---

## ğŸ“Š Documentation Governance (Librarian Role)

**Recent Changes (This Session):**
1. V4_STRATEGY.md â€” Added Phase 4.0, marked 3.6-3.8 as CHAR-LEVEL ONLY
2. V4_HANDOFF.md â€” Complete rewrite with corrected framing
3. Task 40 status â€” Updated from RUNNING to COMPLETE with results

**Core Documents (Sacred Status):**
- SCALING_MILESTONES.md â€” Strategic foundation (verified still accurate)
- V4_STRATEGY.md â€” Master task source (updated with Phase 4.0)
- VALIDATION_ROADMAP.md â€” Execution timeline (deferred until Phase 4.0 complete)

---

## ğŸ¯ Phase 4.0 Gate Criteria

**PASS conditions (all required):**
- âœ… **S0-S4 state space tests pass** â€” state machinery verified
- âœ… Overfit test passes (loss â†’ near 0 on small sample)
- âœ… Naive baseline test passes (val loss < random)
- âœ… G1-G4 gates pass with BPE tokenization
- âœ… Checkpoint/resume works
- âœ… Component balance assessed and documented

**FAIL triggers:**
- âŒ Any S0-S4 state test fails â€” state machinery broken
- âŒ Cannot overfit small sample
- âŒ Val loss worse than random
- âŒ Any G1-G4 gate fails with BPE
- âŒ Component balance deemed unacceptable (decision in Task 48)

**Outcome:** 
- If PASS â†’ Proceed to Phase 3.9 diagnostics (with BPE baseline)
- If FAIL â†’ Debug architecture at 3.5M before any scaling

---

## ğŸš€ Quick Start for Next Agent

1. **Read the CRITICAL REFRAME section at the top** â€” understand the strategic shift
2. **Fix Blocker 1 (Task 41a)** â€” implement `return_states` in model forward()
3. **Run S0-S4 state tests (Task 42)** â€” verify state machinery works
4. **Create test_tiny_graduation.py (Task 41)** â€” combine all graduation tests
5. **Investigate 71x variance (Task 48)** â€” understand component balance issue

**Order of Operations:**
```
Task 41a â†’ Task 42 â†’ Task 41 â†’ Tasks 43-46 â†’ Task 47 â†’ Task 48
(API)      (S0-S4)   (Script)  (Grad tests)  (Fusion)  (Balance)
```

**Do NOT proceed to Phase 3.9 diagnostics until Phase 4.0 is complete.**

**Key Documents:**
- [CANARY_TESTS.md](CANARY_TESTS.md#s0-s4-state-space-fundamentals-35m-only--required-first) â€” S0-S4 state tests
- [SCALING_MILESTONES.md](SCALING_MILESTONES.md#35m-parameters-sanity-check--architecture-debug) â€” Tiny graduation criteria
- [STATEFUL_VALIDATION_GUIDE.md](STATEFUL_VALIDATION_GUIDE.md#part-0-state-space-fundamentals-35m--run-first) â€” State monitoring framework
