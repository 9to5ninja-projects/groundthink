# V4 Agent Handoff

**Version:** 5.0-Alpha | **Phase:** 0.5 PLANNING (Harmonization) | **Updated:** 2026-01-11

---

## Current Status

‚úÖ **Phase 4.0 Graduation PASSED** ‚Äî GF-MH passes all validation gates.
‚úÖ **Task 62 COMPLETE** ‚Äî GPT-2 baseline comparison on WikiText-103 with BPE.
‚úÖ **Tasks 55-60 COMPLETE** ‚Äî Diagnostic tooling suite built.
‚úÖ **Librarian Audit COMPLETE** ‚Äî Gap analysis between V4 and V0.5 performed.

| Test | Result |
|------|--------|
| S0-S4 State Tests | 5/5 PASS (variance ratio 108,583x) |
| Task 43 Overfit | PASS (loss 0.48 in 65 steps) |
| Task 44 Baseline | PASS (6.01 vs 9.68, 37.9% better) |
| G1-G4 Gates | G1‚úì G2‚úì G3‚è≠ G4‚ö† |
| Task 46 Checkpoint | PASS (21.5 MB, identical reload) |
| **Task 62 GPT-2** | **EQUIVALENT** (ratio 1.008) |
| Task 58 Ablation | FAIL (RWKV 99.9%, Mamba 0.1%) |
| Task 59 Evolution | PASS (state responds to input) |
| Task 60 Long-ctx | PASS (1.04x ratio) |

---

## Last Session (2026-01-11)

**GPU/Memory Strategy Discovery:**
- WSL crashes when tokenizing full 540MB WikiText-103 (BPE training + tokenization)
- **SOLUTION:** VS Code Colab extension (`google.colab`) provides FREE T4 GPU + 15GB RAM
- Bypasses all local memory constraints without renting cloud machines
- Notebook updated to auto-detect environment and use full dataset on Colab

**Recommended Workflow:**
1. Open `notebooks/task_0_0_1_wsl.ipynb` in VS Code
2. Select Kernel ‚Üí Connect to Google Colab
3. Run all cells with full dataset + GPU acceleration

**Memory Investigation Results:**
| Component | Memory | Notes |
|-----------|--------|-------|
| PyTorch import | ~350 MB | Fixed cost |
| ops/RWKV6Attention | +40 MB | No mamba loaded |
| 5.6M model + optimizer | +150 MB | Acceptable |
| BPE tokenization 540MB | CRASH | WSL limit hit |
| Colab environment | 15 GB | No limits |

**Librarian Audit & Harmonization:**
- Reconciled V4 diagnostic findings with V0.5 "Twin Debate" architecture.
- Identified critical "missed implementations" (GRUs, Qualia Fade, Semantic Weighting Sensors).
- Established new documentation strategy: Incremental conversion of research paper to tasks.
- Consolidated `ops/` package with core CUDA and prototype wrappers.

**Diagnostic Tooling Results:**
- **Mamba Paradox:** Confirmed Mamba gradients are 10x larger vs RWKV, but state contribution is <0.3%.
- **Attractor Zone:** Verified all gate initializations gravitate toward 10-30% R/M ratio.
- **BPE Efficacy:** Confirmed 16K BPE significantly improves component balance over char-level.

---

## Next Actions (V0.5 Strategy)

### Phase 0: Base Model Characterization (CURRENT PRIORITY)
| Priority | Task | Description | Status |
|----------|------|-------------|--------|
| **0.0.1** | Pure RWKV-6 Benchmark | 4M params, WikiText-103, BPE 16K | ‚úÖ READY |
| **0.0.2** | Pure Mamba-2 Benchmark | 4M params, WikiText-103, BPE 16K | ‚¨ú TODO |
| **0.0.3** | GPT-1 Baseline | 4M params for fair comparison | ‚¨ú TODO |
| **0.0.4** | Comparative Analysis | Document findings, inform fusion design | ‚¨ú TODO |

**Rationale:** Understand individual pathway behavior before implementing fusion.

**Task 0.0.1 Implementation Complete (2026-01-11):**
- ‚úÖ Model: models/rwkv6_pure.py (8 layers √ó 144 hidden, 4.46M params, tied embeddings)
- ‚úÖ Test Script: tests/task_0_0_1_rwkv6_benchmark.py (integrated training + metrics)
- ‚úÖ Config: configs/task_0_0_1.yaml (10K steps, WikiText-103 + BPE 16K)
- ‚úÖ Variance Tool: tools/variance_analysis.py (layer-wise output variance tracking)
- ‚úÖ NIAH Test: tests/test_niah_bpe.py (BPE needle-in-haystack at multiple depths)
- üìã Requirements inventory: /tmp/phase0_inventory.txt (tools verified, 5 new components created)

**‚ö†Ô∏è DEVIATIONS FROM ORIGINAL PLAN (2026-01-11):**

| Deviation | Original Plan | Actual | Rationale |
|-----------|--------------|--------|----------|
| Dataset size | Full WikiText-103 (540MB) | 50MB subset | Full corpus uses 5.3GB RAM just to load; Colab crashes during tokenization |
| Execution env | Local WSL | Google Colab (VS Code extension) | WSL ~2.5GB limit; Colab provides 15GB RAM + T4 GPU free |
| Tokenization | Single-pass | 10MB chunked | Memory-safe; prevents OOM during BPE encoding |
| mamba-ssm | Installed | Skipped on Colab | Build fails without CUDA toolkit; not needed for RWKV6-only baseline |
| RWKV6 CUDA kernel | wkv6_cuda from RWKV-CUDA/ | PyTorch prototype | CUDA kernel requires compilation; prototype is portable |
| WKV computation | CUDA-optimized parallel scan | Sequential Python loop | Prototype for validation only; ~100x slower but mathematically correct |

**‚ö†Ô∏è RWKV6 Prototype Notes (Critical for Future Sessions):**

**Available Classes:**
1. **`RWKV6Attention_Prototype`** - Full block (LN + WKV + squared ReLU FFN + residuals)
   - ‚ö†Ô∏è Squared ReLU can cause value explosion over many layers
   - Use if you want original RWKV-6 spec exactly
   
2. **`RWKV6TimeMix`** - Time-mixing only (RECOMMENDED)
   - No internal FFN/LN - wrap with your own GELU FFN
   - This is what Task 0.0.1 notebook uses
   - Stable across 8 layers

**Key Fixes (2026-01-11):**
- WKV normalization: now properly tracks state_num/state_den
- Value explosion: solved by using RWKV6TimeMix + GELU FFN
- Performance: ~0.5s/step on CPU vs ~0.01s/step with CUDA kernel (50x slower)

**Why Not CUDA Kernel?**
- Requires `ninja` + CUDA toolkit for JIT compilation
- Colab free tier has limited build environment
- Prototype is sufficient for baseline characterization (not training at scale)

**Why 50MB is Valid for Baseline:**
- ~5M tokens after BPE (same density as original plan)
- Sufficient for loss curve characterization and variance analysis
- Full 540MB can be revisited when infrastructure supports it
- Matches compute budget constraints documented in V4_BUILD_LOG.md

**Task 0.0.1 Status (2026-01-11): üü¢ PRELIMINARY COMPLETE**

| Finding | Value | Note |
|---------|-------|------|
| Characterization | **AMPLIFIER** | Variance grows ~1.27x per layer |
| Variance range | 1.0 ‚Üí 5.4 std | 5.4x total amplification |
| Learning | 125 ‚Üí 35 loss | 72% reduction (50 steps) |
| Logits | [-57, +134] | Exploding, softmax saturates |

**Key Insight:** RWKV-6 alone amplifies variance through layers. Does NOT stabilize.
This informs fusion design: if Mamba-2 is a STABILIZER, they may complement each other.

**Outputs:**
- ‚úÖ `logs/dataset_meta.json` - Dataset config
- ‚úÖ `logs/rwkv6_variance.json` - Layer variance data  
- ‚úÖ `logs/rwkv6_baseline_findings.json` - Full findings

**Next Steps:**
1. ‚¨ú Extended RWKV-6 run (500-1000 steps) for convergence metrics
2. ‚¨ú Task 0.0.2: Mamba-2 characterization (is it STABILIZER or AMPLIFIER?)
3. ‚¨ú Task 0.0.3: GPT-1 baseline for fair comparison
4. ‚¨ú Task 0.0.4: Comparative analysis ‚Üí inform fusion design

See [BASE_MODEL_CHARACTERIZATION.md](BASE_MODEL_CHARACTERIZATION.md) for detailed plan.

### Phase A: Documentation Cleanup (Sonnet) ‚úÖ COMPLETE
| Priority | Task | Description | Status |
|----------|------|-------------|--------|
| **A1** | Trim V4_STRATEGY.md | Archive completed task details, keep only summary tables. Target: <500 lines. | ‚úÖ COMPLETE (206 lines) |
| **A2** | Consolidate V4 Docs | Merge redundant findings into OBSERVATION_SYNTHESIS.md. | ‚úÖ COMPLETE (archived 2 docs) |
| **A3** | Finalize V0.5_ROADMAP.md | Ensure all Section 1 tasks have clear acceptance criteria. | ‚úÖ COMPLETE (6 tasks defined) |
| **A4** | Update DOCUMENTATION_MAP.md | Reflect new file structure (V0.5 docs, archived V4 details). | ‚úÖ COMPLETE (navigation updated) |

### Phase B: Implementation (Sonnet) - READY TO START
| Priority | Task | Description |
|----------|------|-------------|
| **B1** | GRU Arbiter (Task 0.1) | Replace `nn.Linear` gate with `nn.GRUCell` in `ops/`. |
| **B2** | Mamba Residual (Task 0.2) | Add `h = x + mamba(x)` skip connection. |
| **B3** | Debate Loss (Task 0.3) | Implement cosine similarity penalty in `tools/`. |
| **B4** | Pilot Run (Task 0.4) | 5K steps, verify Mamba contribution > 5%. |

**‚úÖ Phase A Gate: PASSED** - Documentation clean, ready for implementation.

---

**CRITICAL:** All V5 benchmarks must use:
- WikiText-103 data (`data/wikitext103/train.txt`)
- BPE tokenizer (`data/tokenizer_wikitext.json`, vocab=16K)
- Same data/tokenizer for both GPT-2 and GF-MH

See [V5_GATING.md](V5_GATING.md) for thresholds and criteria.

---

## Quick Start

```bash
source .venv/bin/activate

# Run all graduation tests
python tests/test_tiny_graduation.py --states --gates --overfit --baseline --checkpoint

# Train with state monitoring
python train_v4.py --model GF-MH --tokenizer bpe --log-states

# Check model registry
python -c "from models import list_models; print(list_models())"
```

---

## Key Files

| File | Purpose |
|------|---------|
| [V4_STRATEGY.md](V4_STRATEGY.md) | Master task backlog (Phases 4.0-5.0) |
| [CHANGELOG.md](CHANGELOG.md) | Version history |
| [STATEFUL_VALIDATION_GUIDE.md](STATEFUL_VALIDATION_GUIDE.md) | Test harness documentation |
| [CANARY_TESTS.md](CANARY_TESTS.md) | S0-S4 and G1-G4 definitions |
| [tests/test_tiny_graduation.py](tests/test_tiny_graduation.py) | Unified test harness |
| [tests/test_diagnostics.py](tests/test_diagnostics.py) | D1-D4 diagnostic analysis |
| [tests/test_ablation.py](tests/test_ablation.py) | Component ablation (Task 58) |
| [tests/test_long_context.py](tests/test_long_context.py) | 64-512 degradation (Task 60) |
| [tools/thresholds.py](tools/thresholds.py) | Unified thresholds (Task 56) |
| [tools/information_flow_tracer.py](tools/information_flow_tracer.py) | MI tracing (Task 55) |
| [tools/state_metrics.py](tools/state_metrics.py) | State health tracking |
| [tools/gradient_coupling.py](tools/gradient_coupling.py) | Gradient flow analysis |

---

## Known Issues

| Issue | Status | Notes |
|-------|--------|-------|
| G4 Gradient Imbalance | ‚ö†Ô∏è WARN | Mamba grads 10x larger than RWKV |
| S4 State Variance | ‚ö†Ô∏è WARN | 66K-124K ratio (architecture-dependent) |
| D1 State Divergence | ‚ö†Ô∏è WARN | RWKV norm grows 2.5x over 512 tokens |
| D3 Component Balance | ‚ö†Ô∏è WARN | Mamba only 0.2% contribution by state norm |
| Gate Attractor | ‚ÑπÔ∏è INFO | All gates converge to 0.06-0.27 zone |

**Finding (Observation 14):** Optimizer finds loss-minimizing attractor regardless of init.
- GF-XM (0.03 init): 66K S4 ratio, 1.81 val loss
- GF-MH (0.30 init): 88K S4 ratio, ~1.58 val loss ‚Üê still best
- GF-XR (0.97 init): 124K S4 ratio, 1.96 val loss

---

## Git Status

```
Latest: c6c2f59
Branch: main
Status: Clean (pending doc sync)
```

---

*For detailed task definitions, see [V4_STRATEGY.md](V4_STRATEGY.md)*  
*For version history, see [CHANGELOG.md](CHANGELOG.md)*
