#!/usr/bin/env python3
import os
import time
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.checkpoint import checkpoint
from transformers import AutoTokenizer
from datasets import load_dataset, interleave_datasets
import bitsandbytes as bnb
from torch.utils.tensorboard import SummaryWriter

# Try to import Triton kernel for 10x speedup
try:
    # Try importing assuming script is in root and groundthink is a package
    from groundthink.ops.selective_scan_triton import selective_scan_triton_forward
    USE_TRITON = True
    print("‚úÖ Triton Kernel available (Package). Acceleration ON.")
except ImportError:
    try:
        # Try relative import if script is running from inside groundthink folder
        from ops.selective_scan_triton import selective_scan_triton_forward
        USE_TRITON = True
        print("‚úÖ Triton Kernel available (Local). Acceleration ON.")
    except ImportError as e:
        USE_TRITON = False
        print(f"‚ö†Ô∏è Triton not found: {e}. Falling back to slow Python loop.")

# ==========================================
# 0. A100 OPTIMIZATIONS
# ==========================================
torch.set_float32_matmul_precision('high') # Enable TF32
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# ==========================================
# 1. CONFIGURATION (125M V2 Profile)
# ==========================================
class Config:
    vocab_size = 50257      # GPT2 Standard
    d_model = 768           # 125M Scale (Standard Small)
    n_layer = 12            # 125M Scale (Standard Small)
    head_size = 64          
    
    # A100 Specifics for Small Model
    # Can go MUCH faster with larger batches
    micro_batch_size = 64    # increased for small model
    grad_accum_steps = 4     # Total batch ~256
    
    max_seq_len = 2048      
    learning_rate = 6e-4    # Higher LR for smaller model (GPT-3 small used 6e-4)
    
    total_steps = 5000      # More steps for smaller model convergence
    
    project_name = "groundthink_v2_125M"
    dtype = torch.bfloat16  # NATIVE BF16

config = Config()

# ==========================================
# 2. MODEL ARCHITECTURE (V2: SelectiveWKV)
# ==========================================
class SelectiveWKV_V2(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dim = config.d_model
        self.n_head = config.d_model // config.head_size
        self.head_size = config.head_size
        
        self.x_proj = nn.Linear(self.dim, self.dim, bias=False)
        self.w_proj = nn.Linear(self.dim, self.dim)
        self.k_proj = nn.Linear(self.dim, self.dim, bias=False)
        self.v_proj = nn.Linear(self.dim, self.dim, bias=False)
        self.r_proj = nn.Linear(self.dim, self.dim, bias=False)
        self.out_proj = nn.Linear(self.dim, self.dim, bias=False)
        self.ln_x = nn.LayerNorm(self.dim)
        
        # Initialize Geometric Decay (CRITICAL FOR LONG CONTEXT)
        with torch.no_grad():
            n_heads = self.n_head
            head_size = self.head_size
            r_min = 0.9
            r_max = 0.999
            r_schedule = torch.linspace(r_min, r_max, n_heads)
            w_target = 1.0 - r_schedule
            bias_schedule = torch.logit(w_target)
            bias_full = bias_schedule.unsqueeze(-1).expand(n_heads, head_size).reshape(-1)
            self.w_proj.bias.copy_(bias_full)
            self.w_proj.weight.data.normal_(0, 0.01)

    def forward(self, x, state=None):
        B, T, C = x.size()
        x = self.ln_x(x)
        
        # Projections
        w_raw = self.w_proj(self.x_proj(x))
        w = torch.sigmoid(w_raw) 
        k = self.k_proj(x)
        v = self.v_proj(x)
        r = torch.sigmoid(self.r_proj(x))
        
        # Shape for Triton: [B, H, T, D]
        # Current shape: [B, T, C]
        
        k = k.view(B, T, self.n_head, self.head_size).permute(0, 2, 1, 3) # [B, H, T, D]
        v = v.view(B, T, self.n_head, self.head_size).permute(0, 2, 1, 3)
        w = w.view(B, T, self.n_head, self.head_size).permute(0, 2, 1, 3) 
        
        if USE_TRITON:
            # Native Triton Op: O(N)
            # Forward pass only for now
            y = selective_scan_triton_forward(k, v, w)
        else:
            # Slow Reference (Recurrent)
            # NOTE: Highly inefficient in python loop, only for debugging
            y = torch.zeros_like(v)
            if state is None:
                state = torch.zeros(B, self.n_head, self.head_size, self.head_size, device=x.device, dtype=x.dtype)
            
            # Simple chunkwise or sequential?
            # Sequential for correctness in fallback
            # (Omitting full slow recurrence implemention for brevity in V2 script as we target A100)
            pass 

        # Reshape back: [B, H, T, D] -> [B, T, H, D] -> [B, T, C]
        y = y.permute(0, 2, 1, 3).reshape(B, T, C)
        
        # Gate (Receptance)
        y = y * r
        
        return self.out_proj(y), None

class GroundThinkBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln1 = nn.LayerNorm(config.d_model)
        self.mixer = SelectiveWKV_V2(config) # V2 Hybrid
        self.ln2 = nn.LayerNorm(config.d_model)
        self.mlp = nn.Sequential(
            nn.Linear(config.d_model, 4 * config.d_model),
            nn.GELU(),
            nn.Linear(4 * config.d_model, config.d_model)
        )

    def forward(self, x):
        mixer_out, _ = self.mixer(self.ln1(x))
        x = x + mixer_out
        x = x + self.mlp(self.ln2(x))
        return x

class GroundThink125M(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)
        self.blocks = nn.ModuleList([GroundThinkBlock(config) for _ in range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.d_model)
        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)
        self.token_emb.weight = self.head.weight # Tie weights

    def configure_optimizers(self, learning_rate, weight_decay=0.01):
        # Optimization from Design Note 4: Hybrid Tax Mitigation
        # STRATEGY: 
        # 1. "Mamba Dynamics" (Decay/Time-Mixing) -> High LR (1e-3) to encourage long-term state usage
        # 2. "RWKV Projections" (Linear/Gates) -> Standard LR (6e-4) for stability
        # 3. "No Decay" -> Biases, LayerNorms
        
        mamba_params = [] # w_proj (The core recurrence dynamics)
        rwkv_params = []  # Projections, MLP, Embeddings
        nodecay_params = []
        
        for name, p in self.named_parameters():
            if not p.requires_grad:
                continue
                
            # Filter No-Decay first
            if p.dim() < 2 or "ln" in name or "bias" in name:
                nodecay_params.append(p)
                continue
                
            # Split Hybrid Components
            # w_proj is the parameter governing "Forget/Decay rates" -> The Memory Mechanic
            if "w_proj" in name:
                mamba_params.append(p)
            else:
                rwkv_params.append(p)

        # Mamba LR Boost: 1.5x - 2.0x base LR (Conservative start vs 1e-3 blind jump)
        # Base LR is 6e-4. 
        # Mamba LR -> ~1e-3
        
        optim_groups = [
            {'params': rwkv_params,  'weight_decay': weight_decay, 'lr': learning_rate},
            {'params': mamba_params, 'weight_decay': weight_decay, 'lr': learning_rate * 1.8}, 
            {'params': nodecay_params, 'weight_decay': 0.0,        'lr': learning_rate}
        ]
        
        print(f"üîß Optimizer Split: RWKV {len(rwkv_params)} tensors | Mamba {len(mamba_params)} tensors | NoDecay {len(nodecay_params)} tensors")
        
        optimizer = bnb.optim.Adam8bit(optim_groups, betas=(0.9, 0.95))
        return optimizer

    def forward(self, idx, targets=None):
        B, T = idx.size()
        x = self.token_emb(idx)
        
        # Gradient Checkpointing (Selective recommended in notes)
        # For 125M we probably don't need it on A100, but good for scaling
        for block in self.blocks:
            # x = checkpoint(block, x, use_reentrant=False) 
            x = block(x) # Checkpointing disabled for 125M speed
            
        x = self.ln_f(x)
        
        if targets is not None:
            logits = self.head(x)
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = targets[..., 1:].contiguous()
            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            return logits, loss
        
        return self.head(x), None

# ==========================================
# 3. DATASET V2 (Deterministic Load)
# ==========================================
from datasets import load_from_disk

def get_dataloaders(config):
    # Try looking in local folder (A100 standard path)
    load_path = "data_v2" 
    
    if not os.path.exists(load_path):
        # Fallback to absolute path if provided or just check sibling dir
        load_path = os.path.join(os.path.dirname(__file__), "data_v2")
    
    if os.path.exists(load_path):
        print(f"üìÇ Loading Pre-Processed V2 Dataset from {load_path}...")
        try:
            dataset = load_from_disk(load_path)
            print(f"‚úÖ Loaded {len(dataset)} samples.")
        except Exception as e:
            print(f"‚ùå Failed to load disk dataset: {e}")
            raise
    else:
        print(f"‚ùå V2 Dataset not found at {load_path}.")
        print("   Run 'python prepare_v2_dataset.py' first to generate the artifact.")
        raise FileNotFoundError("Missing V2 Dataset Artifact")
    
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

    def collate_fn(batch):
        texts = [item['text'] for item in batch]
        encoded = tokenizer(
            texts, padding=True, truncation=True, 
            max_length=config.max_seq_len, return_tensors="pt"
        )
        
        input_ids = encoded['input_ids']
        labels = input_ids.clone()
        
        # Mask padding tokens in loss
        labels[encoded['attention_mask'] == 0] = -100
        return input_ids, labels

    train_loader = DataLoader(dataset, batch_size=config.micro_batch_size, shuffle=True, collate_fn=collate_fn)
    return train_loader, tokenizer
    
# ==========================================
# 4. TRAINING LOOP
# ==========================================
# (Standard loop from previous file to be appended)
