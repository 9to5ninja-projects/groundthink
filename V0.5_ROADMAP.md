# GroundThink V0.5 Roadmap: The Twin Debate
**Baseline:** `groundthink_architecture_research.md` (3.4K lines)
**Status:** Phase 0 (Base Model Characterization) â†’ Phase 1 (Implementation)
**Governance:** Librarian Tier 1 (Task Tracking) + Documentation-First Strategy

**âš ï¸ PREREQUISITE:** [BASE_MODEL_CHARACTERIZATION.md](BASE_MODEL_CHARACTERIZATION.md) â€” Benchmark pure RWKV-6 and Mamba-2 before fusion design.

---

## 0. Phase 0: Base Model Characterization (CURRENT)

**Objective:** Understand individual pathway behavior before implementing fusion.

**Tasks:** See [BASE_MODEL_CHARACTERIZATION.md](BASE_MODEL_CHARACTERIZATION.md)
- âœ… 0.0.1: Pure RWKV-6 benchmark (4M) â€” **AMPLIFIER** (5.5x total, 1.28x/layer)
- âœ… 0.0.1.a: BlinkDL init ablation â€” **CONFIRMED** (fixes saturation)
- âœ… 0.0.2: Pure Mamba-2 benchmark (4M) â€” **AMPLIFIER** at model level (2.0x), DAMPER at layer level
- âœ… 0.0.2.a: Mamba-2 init ablation â€” **CONFIRMED** (max_prob 0.86â†’0.05, entropy 0.5â†’7.0)
- âœ… 0.0.3: GPT-1 baseline (4M) â€” **AMPLIFIER (782x)** extreme amplification
- â¬œ 0.0.4: Comparative analysis
- â¬œ 0.0.5: Scale test (8M, optional)

**Key Finding (Revised 2026-01-12):** 
- All three full models are AMPLIFIERS: GPT-1 (782x) >> RWKV-6 (5.5x) > Mamba-2 (2.0x)
- At raw SSM layer level: RWKV-6 (amplifier) vs Mamba-2 (damper) â€” complementary!
- SSMs amplify far less aggressively than attention (142x less than GPT-1)
- BlinkDL init is critical and architecture-agnostic
- Fusion architecture should consider layer-level fusion to preserve complementary behavior

**Gate:** Phase 0 complete â†’ Findings inform Phase 1 fusion design.

---

## 1. Core Objective (Section 1 Mapping)
Transition from "Naive Hybrid" (where Mamba is a loose hinge) to **"Twin Debate"** (where RWKV-6 and Mamba-2 actively compete and are reconciled by an Arbiter).

**Note:** Specific fusion architecture decisions (GRU vs stateless, debate loss necessity) informed by Phase 0 findings.

### Implementation Checklist (Section 1: Architecture Core)
- [ ] **Stateful Arbiter (Î±-Gate):** Replace static `nn.Linear` gate with a GRU cell. The Arbiter must have memory of *what it has already heard* from the twins.
- [ ] **Forced Mamba Contribution:** Implement a direct residual path `x + Mamba(x)` before fusion to solve the "Loose Hinge" paradox (Massive gradients, near-zero state contribution).
- [ ] **Pathway Normalization:** Audit LayerNorm placement. Each twin should be normalized *before* fusion to equalize signal magnitude.
- [ ] **Debate Loss ($L_{debate}$):** Implement Twin Divergence Loss ($L_{diversity}$) to enforce unique feature extraction by each pathway.

## 2. Phase V0.5: Architectural Refinement (4M - 8M)

### 2.1 The Arbiter (Î±-Gating)
- **Goal:** Move from static or simple gating to a learnable, stateful Î±-fusion mechanism.
- **Formula:** `output = Î± Â· RWKV + (1 - Î±) Â· Mamba` (where Î± is GRU-conditioned).
- **Tasks:**
    - [ ] Implement `ArbiterGRU` module in `ops/`.
    - [ ] Add `residual_mamba=True` flag to `ParallelHybridBlock`.
    - [ ] Verify Î± distribution across layers using `information_flow_tracer.py`.

### 2.2 The Twin Debate (Loss Functions)
- **Goal:** Encourage pathway specialization.
- **New Losses:**
    - `L_task`: Standard Cross-Entropy.
    - `L_diversity`: Penalize pathways for high cosine similarity (forced disagreement).
    - `L_arbiter`: Reward the arbiter for choosing the path with lower individual loss (context-aware trust).
- **Tasks:**
    - [ ] Implement `TwinDebateLoss` module in `tools/`.
    - [ ] Run "Debate Baseline" on 4M model to verify Mamba state contribution > 5%.

### 2.3 Qualia Preservation & Spatial Awareness
- **Goal:** Internalize "magic" (coherence) through importance weighting.
- **Tasks:**
    - [ ] Implement Three-Phase Fade (Initial Focus -> Distributed Wisdom -> Internalized Coherence).
    - [ ] Add Segment Embeddings (INPUT/THINK/OUTPUT) to the tokenizer/embedding layer.

## 3. Detailed Task List (V0.5)

| # | Task | Priority | Status | Complexity | Description |
|---|------|----------|--------|------------|-------------|
| 0.1 | **Arbiter Upgrade (GRU)** | ðŸ”´ CRITICAL | â¬œ TODO | M | Replace linear gate with stateful GRU cell. |
| 0.2 | **Mamba Residual Path** | ðŸ”´ CRITICAL | â¬œ TODO | S | Add `h_mamba = x + mamba(x)` before gated blend. |
| 0.3 | **Twin Debate Loss** | ðŸ”´ CRITICAL | â¬œ TODO | L | Diversity + Arbiter loss components implemented. |
| 0.4 | **4M "Debate" Pilot** | ðŸŸ¡ HIGH | â¬œ TODO | M | Run 5K steps. Target: Mamba state > 5%. |
| 0.5 | **Segment Embeddings** | ðŸŸ¡ HIGH | â¬œ TODO | M | Explicit tokens/embeddings for thought blocks. |
| 0.6 | **Three-Phase Fade** | ðŸŸ¢ LOW | â¬œ TODO | L | Importance weighting schedule and loss scaling. |

---

## 5. Task Acceptance Criteria

### Task 0.1: Arbiter Upgrade (GRU)
**Done when:**
- [ ] `ops/arbiter_gru.py` module created with `ArbiterGRU` class
- [ ] GRU cell replaces `nn.Linear` in fusion gate computation
- [ ] Hidden state initialized and carried across sequence positions
- [ ] Forward pass returns Î± weights shaped `[batch, seq_len, 1]`
- [ ] Unit test passes: Î± varies based on sequence history (not static)

### Task 0.2: Mamba Residual Path
**Done when:**
- [ ] `ParallelHybridBlock` has `residual_mamba` flag (default=True)
- [ ] Mamba output added to input: `h_mamba = x + mamba(x)`
- [ ] Fusion operates on `h_rwkv` and `h_mamba` (both with residuals)
- [ ] Forward pass test: Mamba contribution > 0 even when Î±=0
- [ ] Ablation test: Zeroing Mamba affects output (no longer loose hinge)

### Task 0.3: Twin Debate Loss
**Done when:**
- [ ] `tools/debate_loss.py` module created with `TwinDebateLoss` class
- [ ] `L_diversity` computes cosine similarity penalty between pathway outputs
- [ ] `L_arbiter` rewards Î± for selecting lower-loss pathway
- [ ] Combined loss: `L_total = L_task + Î»_div * L_diversity + Î»_arb * L_arbiter`
- [ ] Training script accepts `--debate-loss` flag with lambda hyperparameters

### Task 0.4: 4M "Debate" Pilot
**Done when:**
- [ ] Model trained for 5K steps with debate loss enabled
- [ ] Mamba state contribution measured via ablation: > 5% (target)
- [ ] Î± distribution logged: mean, std, min, max per layer
- [ ] Loss curves saved: L_task, L_diversity, L_arbiter over time
- [ ] Results documented in pilot report comparing vs baseline GF-MH

### Task 0.5: Segment Embeddings
**Done when:**
- [ ] Special tokens added to tokenizer: `<INPUT>`, `<THINK>`, `<OUTPUT>`
- [ ] Segment embedding layer added to model (3 embeddings, same dim as token embeddings)
- [ ] Embeddings summed with token embeddings before first layer
- [ ] Training data preprocessed with segment markers
- [ ] Forward pass test: Different segments produce different embeddings

### Task 0.6: Three-Phase Fade
**Done when:**
- [ ] `tools/importance_scheduler.py` module created
- [ ] Phase 1 (0-33% steps): High weights on control tokens/segments
- [ ] Phase 2 (33-66% steps): Linear fade of importance weights
- [ ] Phase 3 (66-100% steps): Uniform weights (internalized)
- [ ] Training script integrates scheduler with loss scaling
- [ ] Checkpoint comparison: Model maintains coherence without explicit weights

## 4. Graduation Criteria (V0.5 -> V1.0)
- **Mamba State Contribution:** > 10% (measured by ablation/diagnostics).
- **Arbiter Confidence:** > 0.7 on "Critique" vs "Narrative" shifts.
- **8M WikiText Parity:** Beats GPT-2 by > 5% in loss.

---
*Derived mapping of Section 1 (CORE ARCHITECTURE OVERVIEW) from [groundthink_architecture_research.md](groundthink_architecture_research.md).*
