{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a57259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "‚úÖ Dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies (Colab / Cloud)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    packages = [\n",
    "        \"torch>=2.0.0\",\n",
    "        \"transformers>=4.30.0\",\n",
    "        \"datasets>=2.12.0\",\n",
    "        \"bitsandbytes>=0.41.0\", \n",
    "        \"accelerate>=0.20.0\"\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages)\n",
    "        print(\"‚úÖ Dependencies installed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error installing packages: {e}\")\n",
    "\n",
    "# Run install if in Colab/Cloud\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    install_packages()\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ÑπÔ∏è Local environment detected. Skipping auto-install (ensure requirements are met).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1231d7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Detected Colab Environment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Handle environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Detected Colab Environment\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è Not running in Colab - Google Drive features disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afffe948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    vocab_size = 50257   # GPT2 Tokenizer standard - Future proof for web data\n",
    "    d_model = 2048       # 1B Scale hidden dim\n",
    "    n_layer = 18         # Depth\n",
    "    head_size = 64       # Tensor Core friendly\n",
    "    grad_accum_steps = 64 # INCREASED: To maintain batch size with lower micro-batch\n",
    "    micro_batch_size = 1  # REDUCED: Critical for 1B on T4 to prevent OOM\n",
    "    learning_rate = 4e-4\n",
    "    max_seq_len = 512     # TinyStories context\n",
    "    project_name = \"groundthink_1B\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88faa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. THE SELECTIVE-WKV BLOCK (CHUNKING ENABLED)\n",
    "# ==========================================\n",
    "class SelectiveWKV_1B(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim = config.d_model\n",
    "        self.n_head = config.d_model // config.head_size\n",
    "        self.head_size = config.head_size\n",
    "        \n",
    "        # Projections for Selective Gates (Mamba style)\n",
    "        self.x_proj = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.w_proj = nn.Linear(self.dim, self.dim) # Selective Decay\n",
    "        \n",
    "        # RWKV-style Key/Value/Receptance\n",
    "        self.k_proj = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.r_proj = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.out_proj = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.ln_x = nn.LayerNorm(self.dim)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        B, T, C = x.size()\n",
    "        x = self.ln_x(x)\n",
    "        \n",
    "        # Selection logic\n",
    "        w = torch.sigmoid(self.w_proj(self.x_proj(x))) \n",
    "        \n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        r = torch.sigmoid(self.r_proj(x))\n",
    "        \n",
    "        # Reshape for head-wise matrix updates\n",
    "        k = k.view(B, T, self.n_head, self.head_size, 1)\n",
    "        v = v.view(B, T, self.n_head, 1, self.head_size)\n",
    "        w = w.view(B, T, self.n_head, self.head_size, 1) \n",
    "        \n",
    "        # Initial State: [B, n_head, head_size, head_size]\n",
    "        if state is None:\n",
    "            state = torch.zeros(B, self.n_head, self.head_size, self.head_size, device=x.device)\n",
    "        \n",
    "        # CHUNKED RECURRENCE (For Training Efficiency/Stability)\n",
    "        chunk_size = 64 # Good balance for T4 GPU\n",
    "        num_chunks = (T + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start = i * chunk_size\n",
    "            end = min(start + chunk_size, T)\n",
    "            \n",
    "            # Slice chunk\n",
    "            k_chunk = k[:, start:end]\n",
    "            v_chunk = v[:, start:end]\n",
    "            w_chunk = w[:, start:end]\n",
    "            r_chunk = r[:, start:end]\n",
    "            \n",
    "            chunk_out = []\n",
    "            \n",
    "            # Inner loop (Scan within chunk)\n",
    "            for t in range(end - start):\n",
    "                # Matrix update: S = (1-w)*S + (k @ v^T)\n",
    "                kv = k_chunk[:, t] @ v_chunk[:, t]\n",
    "                state = (1 - w_chunk[:, t]) * state + kv\n",
    "                \n",
    "                # Read: r @ state\n",
    "                ctx = r_chunk[:, t].view(B, self.n_head, 1, self.head_size) @ state\n",
    "                chunk_out.append(ctx.view(B, C))\n",
    "            \n",
    "            outputs.append(torch.stack(chunk_out, dim=1))\n",
    "            \n",
    "        return self.out_proj(torch.cat(outputs, dim=1)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f75fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundThinkBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.mixer = SelectiveWKV_1B(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.d_model, 4 * config.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.d_model, config.d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mixer_out, _ = self.mixer(self.ln1(x))\n",
    "        x = x + mixer_out\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GroundThink1B(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.blocks = nn.ModuleList([GroundThinkBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.token_emb.weight = self.head.weight\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        x = self.token_emb(idx)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5886b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. DATA & TOKENIZATION (TinyStories)\n",
    "# ==========================================\n",
    "def get_dataloaders(config):\n",
    "    print(\"üìö Loading TinyStories dataset...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # Custom collate for streaming dataset\n",
    "    def collate_fn(batch):\n",
    "        texts = [item['text'] for item in batch]\n",
    "        encoded = tokenizer(\n",
    "            texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=config.max_seq_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoded['input_ids']\n",
    "        labels = input_ids.clone()\n",
    "        return input_ids, labels\n",
    "\n",
    "    return DataLoader(dataset, batch_size=config.micro_batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eafdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. TRAINING LOOP (Pure FP16 - No Scaler)\n",
    "# ==========================================\n",
    "import gc\n",
    "\n",
    "def learning_rate_schedule(step, warmup_steps=1000, max_lr=4e-4):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step / warmup_steps)\n",
    "    return max_lr \n",
    "\n",
    "def train_step(model, optimizer, dataloader, config, start_run_step=0):\n",
    "    print(\"üî• Starting Training (Pure FP16 Mode)...\")\n",
    "    \n",
    "    # Aggressive memory cleanup before start\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NOTE: GradScaler is REMOVED. \n",
    "    # FP16 Weights (required for memory) + GradScaler (requires FP32 grads) = Incompatible.\n",
    "    # We will run in pure FP16 accumulation.\n",
    "    \n",
    "    running_loss = 0\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Setup checkpoints\n",
    "    save_dir = f\"checkpoints/{config.project_name}\"\n",
    "    if IN_COLAB:\n",
    "        save_dir = f\"/content/drive/MyDrive/{config.project_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(save_dir, \"latest.pt\")\n",
    "    \n",
    "    # Attempt to load checkpoint (Scaler state ignored)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        pass # Optimizer state load handles the chaos\n",
    "\n",
    "    for step, (x, y) in enumerate(dataloader, start=start_run_step):\n",
    "        # Update LR with warmup\n",
    "        lr = learning_rate_schedule(step, max_lr=config.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        \n",
    "        # Forward Pass\n",
    "        try:\n",
    "            # Autocast still useful for ops that demand FP32 (like Softmax) internally\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                _, loss = model(x, y)\n",
    "                loss = loss / config.grad_accum_steps\n",
    "            \n",
    "            # Backward (No Scaler)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Detach to save memory\n",
    "            running_loss += loss.item() * config.grad_accum_steps\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"‚ö†Ô∏è OOM at step {step}. Attempting recovery...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "            \n",
    "        # Step (Gradient Accumulation)\n",
    "        if (step + 1) % config.grad_accum_steps == 0:\n",
    "            \n",
    "            # Clip Grads (Standard)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Optimizer Step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Logging\n",
    "            if (step + 1) % 100 == 0:\n",
    "                dt = time.time() - t0\n",
    "                print(f\"Step {step+1} | Loss: {running_loss/config.grad_accum_steps:.4f} | LR: {lr:.2e} | Time: {dt:.2f}s\")\n",
    "                running_loss = 0\n",
    "                t0 = time.time()\n",
    "        \n",
    "        # Checkpoint (Resilient)\n",
    "        if (step + 1) % 500 == 0:\n",
    "            print(f\"üíæ Saving checkpoint at step {step+1}...\")\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model': model.state_dict(),\n",
    "                'opt': optimizer.state_dict(),\n",
    "                # No scaler state\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            # Periodic cleanup\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe0696d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2251075360.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Setup Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mIN_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 2. Initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute Training\n",
    "\n",
    "# 0. MEMORY RECOVERY & CHECK\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Aggressive cleanup\n",
    "keys_to_clean = ['model', 'optimizer', 'dataloader', 'scaler', 'x', 'y', 'loss']\n",
    "for key in keys_to_clean:\n",
    "    if key in globals():\n",
    "        del globals()[key]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# VRAM Status Check\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "free = t - r\n",
    "\n",
    "print(f\"üìä VRAM Status: {free/1024**3:.2f}GB Free | {a/1024**3:.2f}GB Allocated\")\n",
    "\n",
    "# LOCK: Prevent execution if Zombie Process exists\n",
    "if free < 6 * 1024**3:\n",
    "     print(\"‚ö†Ô∏è WARNING: Less than 6GB Free. Trying anyway with FP16 optimization...\")\n",
    "\n",
    "# 1. Setup Drive\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    \n",
    "# 2. Initialize\n",
    "# MVP FALLBACK OPTION: Un-comment these lines if 1B fails repeatedly\n",
    "# config = Config()\n",
    "# config.d_model = 768  # 150M Scale (Approximation)\n",
    "# config.n_layer = 12\n",
    "# config.project_name = \"groundthink_150M_MVP\"\n",
    "\n",
    "config = Config()\n",
    "print(f\"üöÄ Initializing GroundThink ({config.d_model} dim, {config.n_layer} layers)\")\n",
    "\n",
    "# OPTIMIZATION: Robust FP16 Initialization\n",
    "print(\"üìâ Converting model to FP16 (Half Precision)...\")\n",
    "try:\n",
    "    model = GroundThink1B(config)\n",
    "    model.to(dtype=torch.float16)  # Convert ALL weights to FP16\n",
    "    \n",
    "    # STABILITY FIX: Keep LayerNorms in FP32\n",
    "    # Pure FP16 LayerNorms are notoriously unstable (exploding gradients)\n",
    "    print(\"üõ°Ô∏è Restoring LayerNorms to FP32 for stability...\")\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            module.float()\n",
    "\n",
    "    model = model.cuda() # Move to GPU\n",
    "    print(f\"‚úÖ Model loaded on GPU. VRAM: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model Init Failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Optimizer (8-bit)\n",
    "# 8-bit Adam works natively with FP16 params\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# 4. Resume logic\n",
    "start_step = 0\n",
    "save_dir = f\"/content/drive/MyDrive/{config.project_name}\" if IN_COLAB else f\"checkpoints/{config.project_name}\"\n",
    "checkpoint_path = os.path.join(save_dir, \"latest.pt\")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"üîÑ Resuming from {checkpoint_path}\")\n",
    "    ckpt = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['opt'])\n",
    "    start_step = ckpt['step']\n",
    "else:\n",
    "    print(\"üÜï Starting fresh run\")\n",
    "\n",
    "# 5. Run\n",
    "dataloader = get_dataloaders(config)\n",
    "train_step(model, optimizer, dataloader, config, start_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
