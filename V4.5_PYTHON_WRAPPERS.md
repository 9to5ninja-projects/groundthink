# V4.5 Python Wrappers - Backup Implementation Plan

**Created:** 2026-01-09  
**Status:** Documentation  
**Purpose:** Document simplified Python wrappers as fallback if full RWKV-6/Mamba-2 implementation takes too long

---

## Context

**Current Situation:**
- FLA library not installed (and we decided not to use it)
- `archive/fla_replacements.py` contains simplified Python implementations
- These wrappers were created as placeholders
- Model `hybrid_v4.py` expects to import `RWKV6Attention` and `Mamba2`

**The Question:**
Are these wrappers "good enough" for research, or do we need full spec-compliant implementations?

---

## What the Wrappers Are

### Location
`archive/fla_replacements.py` (114 lines)

### Components
1. **RWKV6Attention** (lines 9-62) - Simplified recurrent attention
2. **Mamba2** (lines 65-113) - Simplified state-space model

---

## RWKV6Attention Wrapper Analysis

**File:** `archive/fla_replacements.py` lines 9-62

### What It Claims to Do
```python
class RWKV6Attention(nn.Module):
    """Replacement for fla.layers.rwkv6.RWKV6Attention"""
```

### Architecture
```
Input (B, T, C)
  ↓
LayerNorm → Time Mixing:
  ├→ receptance (r)
  ├→ key (k)  
  ├→ value (v)
  └→ gate (g)
  
WKV = softmax(k) * v
out = output(r * wkv) * g
  ↓
Add residual
  ↓
LayerNorm → Channel Mixing:
  ├→ channel_key
  ├→ relu²
  ├→ channel_value
  └→ channel_receptance (sigmoid)
  
Return (output, None, None)
```

### Key Parameters
- `d_model` or `hidden_size`: Dimension size
- `num_heads`: Number of attention heads (default 8)
- `chunk_size`: Processing chunk size (default 64)

### Components Present
- ✅ Time decay parameters (`time_decay`, `time_first`)
- ✅ R/K/V/G projections
- ✅ Channel mixing (FFN-like)
- ✅ Dual LayerNorms
- ✅ Returns tuple format: `(output, None, None)`

### What's Simplified
- ⚠️ **WKV computation**: Uses `softmax(k) * v` instead of proper recurrent WKV kernel
  - Real RWKV-6: WKV = weighted key-value with exponential decay over time
  - This version: Just attention-style softmax
- ⚠️ **No explicit state**: Doesn't maintain recurrent hidden state
- ⚠️ **Time decay not applied**: Parameters exist but aren't used in forward pass
- ⚠️ **Chunk processing not implemented**: Has `chunk_size` param but processes full sequence

### Critical Missing Feature
**Recurrent state update**. Real RWKV maintains:
```
state_t = decay * state_{t-1} + k_t * v_t
output_t = r_t * state_t
```

This wrapper just does standard attention.

---

## Mamba2 Wrapper Analysis

**File:** `archive/fla_replacements.py` lines 65-113

### What It Claims to Do
```python
class Mamba2(nn.Module):
    """Replacement for fla.layers.mamba2.Mamba2"""
```

### Smart Fallback
```python
try:
    from mamba_ssm import Mamba as RealMamba
    self.use_real_mamba = True
except ImportError:
    self.use_real_mamba = False
    # Use simplified version
```

**Attempts to use real mamba-ssm if available, falls back to simplified.**

### Simplified Architecture (when mamba-ssm not available)
```
Input (B, T, C)
  ↓
in_proj → split into (x, gate)
  ↓
Conv1d (groups=d_inner, kernel=d_conv)
  ↓
SiLU activation
  ↓
* D parameter (direct path)
  ↓
* SiLU(gate)
  ↓
out_proj
  ↓
Return output
```

### Key Parameters
- `d_model` or `hidden_size`: Input dimension
- `d_state`: SSM state dimension (default 16)
- `d_conv`: Conv kernel size (default 4)
- `expand`: Expansion factor (default 2)

### Components Present
- ✅ Input/output projections
- ✅ Depthwise Conv1d
- ✅ Gating mechanism
- ✅ Direct path (D parameter)
- ✅ Proper expansion to `d_inner`

### What's Simplified
- ⚠️ **No SSM**: Missing the core selective state-space model
  - Real Mamba-2: A, B, C matrices with selective mechanism
  - This version: Just conv + gating
- ⚠️ **No state management**: Doesn't track SSM hidden state
- ⚠️ **No selective scan**: Core Mamba feature missing

### Critical Missing Feature
**Selective SSM computation**. Real Mamba does:
```
A, B, C = discretize(input-dependent params)
state_t = A * state_{t-1} + B * x_t  
output_t = C * state_t
```

This wrapper just does conv + gating.

---

## Performance Implications

### RWKV6Attention Wrapper
- **Speed**: Fast (pure PyTorch, no custom kernels)
- **Memory**: O(T²) attention (vs O(T) for real RWKV)
- **Accuracy**: Unknown - not tested against real RWKV-6

### Mamba2 Wrapper
- **Speed**: Fast if using real mamba-ssm, slow otherwise
- **Memory**: Linear in sequence length (good)
- **Accuracy**: Simplified version missing core SSM logic

---

## Use Cases for These Wrappers

### When Simplified Wrappers Are OK
1. **Initial architecture testing**: Verify model shape, gradient flow
2. **Fusion mechanism testing**: Test rwkv_gain/mamba_gain learning
3. **Short sequences (<512 tokens)**: Less dependent on long-range memory
4. **Baseline comparison**: "What if we used simple attention instead?"

### When Simplified Wrappers Are NOT OK
1. **Long-context claims**: Can't validate 2K+ token memory without real recurrence
2. **Performance benchmarks**: Not comparable to papers
3. **Production models**: Missing core architectural features
4. **Research publication**: Would be dishonest to call this "RWKV-6 + Mamba-2"

---

## Backup Plan Options

### Option 1: Use Simplified Wrappers (Fast Path)
**Time:** 1 hour  
**Steps:**
1. Move `archive/fla_replacements.py` to root
2. Update imports in `hybrid_v4.py`
3. Test forward pass (G1 gate)
4. Document clearly: "Using simplified attention-based approximations"
5. Proceed with fusion mechanism research

**Pros:**
- Fast, can continue work immediately
- Tests fusion mechanism design
- Validates training pipeline

**Cons:**
- Not real RWKV-6 or Mamba-2
- Can't claim long-context capabilities
- May waste training time on wrong architecture

### Option 2: Fix Critical Issues Only (Medium Path)
**Time:** 3-5 days  
**Steps:**
1. Research minimal WKV implementation (RWKV-6)
2. Add proper recurrent state to RWKV wrapper
3. Install mamba-ssm for real Mamba (if compatible)
4. Test both components independently
5. Integrate and validate

**Pros:**
- More correct than simplified
- Still avoids FLA dependency
- Focused scope

**Cons:**
- Requires understanding WKV kernel math
- May still miss subtle details
- mamba-ssm may have same install issues as FLA

### Option 3: Full Spec-Compliant Implementation (Slow Path)
**Time:** 1-2 weeks  
**Steps:**
1. Research RWKV-6 paper + code thoroughly
2. Research Mamba-2 paper + code thoroughly
3. Implement from scratch with tests
4. Validate against reference implementations
5. Document everything

**Pros:**
- Correct implementation
- Can publish/share confidently
- Learn architectures deeply

**Cons:**
- Long timeline
- May require CUDA kernel knowledge
- Could still have bugs

### Option 4: Reconsider FLA (Pragmatic Path)
**Time:** 2-3 hours  
**Steps:**
1. `pip install fla-linear-attn`
2. Change imports: `from fla.layers.rwkv6 import RWKV6Attention`
3. Test and validate
4. Continue with original plan

**Pros:**
- Battle-tested implementations
- Fast, reliable
- Focus on fusion mechanism (the actual research)

**Cons:**
- Dependency we wanted to avoid
- "Giving up" on custom implementation

---

## Recommendation

**For immediate progress:** Option 1 (simplified wrappers)
- Document clearly what they are
- Use for fusion mechanism testing
- Parallel track: research proper implementations

**For production model:** Option 2 or 4
- Option 2 if you want to learn/own the implementation
- Option 4 if you want to focus on hybrid fusion research

---

## Next Steps

1. **Decision point**: Choose option based on timeline and goals
2. **If Option 1**: Create Task 6.6b "Deploy Simplified Wrappers"
3. **If Option 2-3**: Continue with Tasks 6.6-6.12 as planned
4. **If Option 4**: Create Task 6.6c "Install and Configure FLA"

---

## Option 2 Implementation: PyTorch Prototype Wrappers

### RWKV-6 PyTorch Prototype (Time-Mixing Focus)

**Purpose:** Minimum viable RWKV-6 implementation that captures mathematical essence for hybrid block validation. Slow but correct for prototyping.

**Key Features:**
- ✅ Proper `_wkv_sequential()` - Implements recurrent state update
- ✅ Time decay parameters used in computation (not just placeholders)
- ✅ Squared ReLU for channel mixing (RWKV spec detail)
- ✅ Returns tuple format: `(output, None, None)`
- ⚠️ Sequential loop = O(T) per batch, not optimized (for validation only)

**File:** `rwkv6_prototype.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class RWKV6Attention_Prototype(nn.Module):
    """PyTorch-only RWKV-6 time-mixing block for prototype validation"""
    
    def __init__(self, hidden_size, n_head=4, head_size=64):
        super().__init__()
        self.hidden_size = hidden_size
        self.n_head = n_head
        self.head_size = head_size
        
        # Time-mixing parameters (simplified from RWKV-v6 demo)
        self.time_decay = nn.Parameter(torch.ones(n_head, head_size))
        self.time_first = nn.Parameter(torch.zeros(n_head, head_size))
        
        # Linear projections (no bias as per RWKV spec)
        self.key = nn.Linear(hidden_size, hidden_size, bias=False)
        self.value = nn.Linear(hidden_size, hidden_size, bias=False)
        self.receptance = nn.Linear(hidden_size, hidden_size, bias=False)
        self.output = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # Channel-mixing (FFN-like)
        self.ffn_key = nn.Linear(hidden_size, hidden_size * 4, bias=False)
        self.ffn_value = nn.Linear(hidden_size * 4, hidden_size, bias=False)
        
        # Layer normalization (RWKV uses PreLN)
        self.ln1 = nn.LayerNorm(hidden_size)
        self.ln2 = nn.LayerNorm(hidden_size)
        
    def forward(self, x, state=None):
        B, T, C = x.shape
        H, S = self.n_head, self.head_size
        
        # Time-mixing block
        x_ln = self.ln1(x)
        
        # Project to key, value, receptance
        k = self.key(x_ln).view(B, T, H, S).transpose(1, 2)  # [B,H,T,S]
        v = self.value(x_ln).view(B, T, H, S).transpose(1, 2)
        r = self.receptance(x_ln).view(B, T, H, S).transpose(1, 2)
        
        # RWKV recurrence computation (pure PyTorch, non-optimized)
        wkv = self._wkv_sequential(k, v)  # Custom WKV in PyTorch
        
        # Apply receptance and output projection
        rwkv = torch.sigmoid(r) * wkv
        rwkv = rwkv.transpose(1, 2).contiguous().view(B, T, C)
        output1 = self.output(rwkv)
        
        # Residual connection
        x = x + output1
        
        # Channel-mixing block
        x_ln = self.ln2(x)
        ffn_out = self.ffn_value(F.relu(self.ffn_key(x_ln))**2)  # Squared ReLU
        x = x + ffn_out
        
        return x, None, None  # Match your required tuple format
    
    def _wkv_sequential(self, k, v):
        """Sequential WKV computation - correct but slow (for validation only)"""
        B, H, T, S = k.shape
        wkv = torch.zeros_like(v)
        
        # Apply time decay and first
        time_decay = torch.exp(-torch.exp(self.time_decay.float()))
        time_first = self.time_first.float()
        
        for b in range(B):
            for t in range(T):
                if t == 0:
                    wkv[b, :, t] = torch.exp(time_first) * v[b, :, t]
                else:
                    wkv[b, :, t] = torch.exp(k[b, :, t]) * v[b, :, t] + \
                                  time_decay * wkv[b, :, t-1]
        
        return wkv
```

**Performance Notes:**
- Sequential loops make this O(B*T) instead of parallelized
- Suitable for validation with seq_len < 512
- For production: needs Triton kernel or parallel scan implementation

**Integration with hybrid_v4.py:**
- Constructor signature needs alignment: `__init__(self, hidden_size, ...)`
- Add parameter mapping: `d_model` → `hidden_size`, `num_heads` → `n_head`
- State parameter optional (not used in current hybrid design)

---

### Mamba-2 PyTorch Prototype

**Purpose:** Minimum viable Mamba-2 implementation with selective SSM for hybrid block validation. Slow but correct for prototyping.

**Key Features:**
- ✅ Selective SSM with A, D matrices and dt_bias
- ✅ Discretization with exponential A (stability)
- ✅ Conv1d depthwise processing
- ✅ Gating mechanism with z (selectivity)
- ✅ Returns tensor directly (not tuple)
- ⚠️ Sequential scan = O(T) not parallelized (for validation only)

**File:** `mamba2_prototype.py`

```python
class Mamba2_Prototype(nn.Module):
    """PyTorch-only Mamba-2 selective SSM for prototype validation"""
    
    def __init__(self, hidden_size, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.hidden_size = hidden_size
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(expand * hidden_size)
        
        # Selective SSM parameters
        self.A = nn.Parameter(torch.randn(self.d_inner, d_state))
        self.D = nn.Parameter(torch.ones(self.d_inner))
        self.dt_bias = nn.Parameter(torch.zeros(self.d_inner))
        
        # Projections
        self.in_proj = nn.Linear(hidden_size, self.d_inner * 2, bias=False)
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv-1,
            bias=False
        )
        self.out_proj = nn.Linear(self.d_inner, hidden_size, bias=False)
        
        # Initialize A properly (important for stability)
        nn.init.normal_(self.A, mean=0.0, std=0.02)
        
    def forward(self, x):
        B, T, C = x.shape
        
        # Selective projection
        x_proj = self.in_proj(x)  # [B,T,2*d_inner]
        x, z = x_proj.chunk(2, dim=-1)
        
        # 1D convolution (simplified, non-causal optimized)
        x = x.transpose(1, 2)
        x = self.conv1d(x)[:, :, :T]
        x = x.transpose(1, 2)
        
        # Selective SSM computation (sequential, non-optimized)
        y = self._selective_scan(x, z)
        
        # Output projection
        out = self.out_proj(y)
        return out
    
    def _selective_scan(self, x, z):
        """Simplified selective scan - correct but slow"""
        B, T, D = x.shape
        N = self.d_state
        
        # Discretization (simplified)
        A = -torch.exp(self.A.float())  # Ensure A is negative
        dt = torch.nn.functional.softplus(x @ self.dt_bias)
        
        # Sequential scan
        h = torch.zeros(B, D, N, device=x.device)
        outputs = []
        
        for t in range(T):
            # Selective part: mix based on z
            current_x = x[:, t] * torch.sigmoid(z[:, t]).unsqueeze(-1)
            
            # SSM update
            h = h * torch.exp(A * dt[:, t].unsqueeze(-1)) + \
                current_x.unsqueeze(-1) * dt[:, t].unsqueeze(-1)
            
            # Output
            y_t = (h @ self.D.unsqueeze(-1)).squeeze(-1)
            outputs.append(y_t)
        
        return torch.stack(outputs, dim=1)
```

**Performance Notes:**
- Sequential scan makes this O(T) instead of parallel scan
- Suitable for validation with seq_len < 512
- For production: needs parallel scan (associative scan) or hardware-aware implementation

**Integration with hybrid_v4.py:**
- Constructor signature needs alignment: `__init__(self, hidden_size, ...)`
- Add parameter mapping: `d_model` → `hidden_size`
- Calculate `num_heads` from existing formula: `(expand * hidden_size) // head_dim`
- Returns tensor directly (no tuple unpacking needed)

---

## Integration Plan: Prototype Wrappers → hybrid_v4.py

### Step 1: Create Component Files

Create two new files in workspace root:
- `rwkv6_prototype.py` - Contains `RWKV6Attention_Prototype`
- `mamba2_prototype.py` - Contains `Mamba2_Prototype`

### Step 2: Update hybrid_v4.py Imports

**Current (broken):**
```python
from fla_replacements import RWKV6Attention
from fla_replacements import Mamba2
```

**Replace with:**
```python
from rwkv6_prototype import RWKV6Attention_Prototype as RWKV6Attention
from mamba2_prototype import Mamba2_Prototype as Mamba2
```

### Step 3: Parameter Mapping

**ParallelHybridBlock constructor currently passes:**
- RWKV6: `hidden_size`, `num_heads`, `layer_idx`
- Mamba2: `hidden_size`, `num_heads`, `head_dim`, `expand`, `layer_idx`

**Prototype constructors expect:**
- RWKV6Attention_Prototype: `hidden_size`, `n_head`, `head_size`
- Mamba2_Prototype: `hidden_size`, `d_state`, `d_conv`, `expand`

**Wrapper classes needed:**
```python
# In rwkv6_prototype.py
class RWKV6Attention:
    """Adapter for hybrid_v4.py compatibility"""
    def __init__(self, hidden_size, num_heads=4, layer_idx=0, **kwargs):
        head_size = hidden_size // num_heads
        self.impl = RWKV6Attention_Prototype(hidden_size, num_heads, head_size)
    
    def forward(self, x, **kwargs):
        return self.impl(x)

# In mamba2_prototype.py  
class Mamba2:
    """Adapter for hybrid_v4.py compatibility"""
    def __init__(self, hidden_size, num_heads=4, head_dim=64, expand=2, layer_idx=0, **kwargs):
        # num_heads ignored (not used in SSM)
        self.impl = Mamba2_Prototype(hidden_size, d_state=16, d_conv=4, expand=expand)
    
    def forward(self, x, **kwargs):
        return self.impl(x)
```

### Step 4: Test Integration

**Test sequence:**
1. Import test: `python -c "import hybrid_v4"`
2. Instantiate model: `model = hybrid_v4.HybridModel()`
3. Forward pass: `out = model(torch.randint(0, 100, (2, 64)))`
4. Gate G1: Check for NaN, verify shapes
5. Gate G2: Check init entropy

### Step 5: Performance Baseline

**Expected performance with prototypes:**
- Throughput: ~5-10K tokens/sec (down from 33K)
- Reason: Sequential scans instead of parallel kernels
- Acceptable for: Architecture validation, fusion mechanism testing
- Not acceptable for: Training runs >1000 steps, production models

---

## Next Tasks (Updated)

**Task 6.6: Deploy Prototype Wrappers** (M complexity, ~2-3 hours)
- Create `rwkv6_prototype.py` and `mamba2_prototype.py`
- Add adapter wrapper classes for API compatibility
- Update `hybrid_v4.py` imports
- Test forward pass (G1 gate)
- Test initialization (G2 gate)
- Document baseline performance
- **Status:** Ready to implement

**Task 6.5: Test Monitoring Tools** (unblocked after 6.6)
- Run 100-step training with prototypes
- Verify monitoring tools capture metrics
- Document performance characteristics

---

## References

- Current wrappers: `archive/fla_replacements.py`
- Model architecture: `hybrid_v4.py` ParallelHybridBlock
- V3 lessons: [V4_HANDOFF.md](V4_HANDOFF.md) - "Don't make up components"
